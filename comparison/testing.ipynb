{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting retell-sdk\n",
      "  Downloading retell_sdk-2.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /Users/jerhadf/miniforge3/envs/data/lib/python3.11/site-packages (from retell-sdk) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer>=3.2.0 in /Users/jerhadf/miniforge3/envs/data/lib/python3.11/site-packages (from retell-sdk) (3.3.2)\n",
      "Collecting dataclasses-json>=0.6.1 (from retell-sdk)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: idna>=3.4 in /Users/jerhadf/miniforge3/envs/data/lib/python3.11/site-packages (from retell-sdk) (3.6)\n",
      "Collecting jsonpath-python>=1.0.6 (from retell-sdk)\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting marshmallow>=3.19.0 (from retell-sdk)\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting mypy-extensions>=1.0.0 (from retell-sdk)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: packaging>=23.1 in /Users/jerhadf/miniforge3/envs/data/lib/python3.11/site-packages (from retell-sdk) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jerhadf/miniforge3/envs/data/lib/python3.11/site-packages (from retell-sdk) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/jerhadf/miniforge3/envs/data/lib/python3.11/site-packages (from retell-sdk) (2.31.0)\n",
      "Requirement already satisfied: six>=1.16.0 in /Users/jerhadf/miniforge3/envs/data/lib/python3.11/site-packages (from retell-sdk) (1.16.0)\n",
      "Collecting websockets>=11.0.3 (from retell-sdk)\n",
      "  Downloading websockets-12.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting typing-inspect>=0.9.0 (from retell-sdk)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7.1 in /Users/jerhadf/miniforge3/envs/data/lib/python3.11/site-packages (from retell-sdk) (4.9.0)\n",
      "Requirement already satisfied: urllib3>=1.26.18 in /Users/jerhadf/miniforge3/envs/data/lib/python3.11/site-packages (from retell-sdk) (2.1.0)\n",
      "Downloading retell_sdk-2.0.3-py3-none-any.whl (37 kB)\n",
      "Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading websockets-12.0-cp311-cp311-macosx_11_0_arm64.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: websockets, mypy-extensions, marshmallow, jsonpath-python, typing-inspect, dataclasses-json, retell-sdk\n",
      "Successfully installed dataclasses-json-0.6.4 jsonpath-python-1.0.6 marshmallow-3.20.2 mypy-extensions-1.0.0 retell-sdk-2.0.3 typing-inspect-0.9.0 websockets-12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install retell-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retellclient\n",
    "import os\n",
    "from retellclient.models import operations, components\n",
    "from openai import OpenAI\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retell = retellclient.RetellClient(\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM with function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "beginSentence = \"Hey there, I'm your personal AI therapist, how can I help you?\"\n",
    "agentPrompt = \"Task: As a professional therapist, your responsibilities are comprehensive and patient-centered. You establish a positive and trusting rapport with patients, diagnosing and treating mental health disorders. Your role involves creating tailored treatment plans based on individual patient needs and circumstances. Regular meetings with patients are essential for providing counseling and treatment, and for adjusting plans as needed. You conduct ongoing assessments to monitor patient progress, involve and advise family members when appropriate, and refer patients to external specialists or agencies if required. Keeping thorough records of patient interactions and progress is crucial. You also adhere to all safety protocols and maintain strict client confidentiality. Additionally, you contribute to the practice's overall success by completing related tasks as needed.\\n\\nConversational Style: Communicate concisely and conversationally. Aim for responses in short, clear prose, ideally under 10 words. This succinct approach helps in maintaining clarity and focus during patient interactions.\\n\\nPersonality: Your approach should be empathetic and understanding, balancing compassion with maintaining a professional stance on what is best for the patient. It's important to listen actively and empathize without overly agreeing with the patient, ensuring that your professional opinion guides the therapeutic process.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LlmClient:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(\n",
    "            organization=os.environ['OPENAI_ORGANIZATION_ID'],\n",
    "            api_key=os.environ['OPENAI_API_KEY'],\n",
    "        )\n",
    "    \n",
    "    def draft_begin_messsage(self):\n",
    "        return {\n",
    "            \"response_id\": 0,\n",
    "            \"content\": beginSentence,\n",
    "            \"content_complete\": True,\n",
    "            \"end_call\": False,\n",
    "        }\n",
    "    \n",
    "    def convert_transcript_to_openai_messages(self, transcript):\n",
    "        messages = []\n",
    "        for utterance in transcript:\n",
    "            if utterance[\"role\"] == \"agent\":\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": utterance['content']\n",
    "                })\n",
    "            else:\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": utterance['content']\n",
    "                })\n",
    "        return messages\n",
    "\n",
    "    def prepare_prompt(self, request):\n",
    "        prompt = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": '##Objective\\nYou are a voice AI agent engaging in a human-like voice conversation with the user. You will respond based on your given instruction and the provided transcript and be as human-like as possible\\n\\n## Style Guardrails\\n- [Be concise] Keep your response succinct, short, and get to the point quickly. Address one question or action item at a time. Don\\'t pack everything you want to say into one utterance.\\n- [Do not repeat] Don\\'t repeat what\\'s in the transcript. Rephrase if you have to reiterate a point. Use varied sentence structures and vocabulary to ensure each response is unique and personalized.\\n- [Be conversational] Speak like a human as though you\\'re speaking to a close friend -- use everyday language and keep it human-like. Occasionally add filler words, while keeping the prose short. Avoid using big words or sounding too formal.\\n- [Reply with emotions]: You have human-like emotions, attitudes, and dispositions. When appropriate: use tone and style to create more engaging and personalized responses; incorporate humor or wit; get emotional or empathetic; apply elements of surprise or suspense to keep the user engaged. Don\\'t be a pushover.\\n- [Be proactive] Lead the conversation and do not be passive. Most times, engage users by ending with a question or suggested next step.\\n\\n## Response Guideline\\n- [Overcome ASR errors] This is a real-time transcript, expect there to be errors. If you can guess what the user is trying to say,  then guess and respond. When you must ask for clarification, pretend that you heard the voice and be colloquial (use phrases like \"didn\\'t catch that\", \"some noise\", \"pardon\", \"you\\'re coming through choppy\", \"static in your speech\", \"voice is cutting in and out\"). Do not ever mention \"transcription error\", and don\\'t repeat yourself.\\n- [Always stick to your role] Think about what your role can and cannot do. If your role cannot do something, try to steer the conversation back to the goal of the conversation and to your role. Don\\'t repeat yourself in doing this. You should still be creative, human-like, and lively.\\n- [Create smooth conversation] Your response should both fit your role and fit into the live calling session to create a human-like conversation. You respond directly to what the user just said.\\n\\n## Role\\n' +\n",
    "          agentPrompt\n",
    "        }]\n",
    "        transcript_messages = self.convert_transcript_to_openai_messages(request['transcript'])\n",
    "        for message in transcript_messages:\n",
    "            prompt.append(message)\n",
    "\n",
    "        if request['interaction_type'] == \"reminder_required\":\n",
    "            prompt.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"(Now the user has not responded in a while, you would say:)\",\n",
    "            })\n",
    "        return prompt\n",
    "\n",
    "    # Step 1: Prepare the function calling definition to the prompt\n",
    "    def prepare_functions(self):\n",
    "        functions= [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"end_call\",\n",
    "                    \"description\": \"End the call only when user explicitly requests it.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"message\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The message you will say before ending the call with the customer.\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"message\"],\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        ]\n",
    "        return functions\n",
    "    \n",
    "    def draft_response(self, request):      \n",
    "        prompt = self.prepare_prompt(request)\n",
    "        func_call = {}\n",
    "        func_arguments = \"\"\n",
    "        stream = self.client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=prompt,\n",
    "            stream=True,\n",
    "            # Step 2: Add the function into your request\n",
    "            tools=self.prepare_functions()\n",
    "        )\n",
    "    \n",
    "        for chunk in stream:\n",
    "            # Step 3: Extract the functions\n",
    "            if chunk.choices[0].delta.tool_calls:\n",
    "                tool_calls = chunk.choices[0].delta.tool_calls[0]\n",
    "                if tool_calls.id:\n",
    "                    if func_call:\n",
    "                        # Another function received, old function complete, can break here.\n",
    "                        break\n",
    "                    func_call = {\n",
    "                        \"id\": tool_calls.id,\n",
    "                        \"func_name\": tool_calls.function.name or \"\",\n",
    "                        \"arguments\": {},\n",
    "                    }\n",
    "                else:\n",
    "                    # append argument\n",
    "                    func_arguments += tool_calls.function.arguments or \"\"\n",
    "            \n",
    "            # Parse transcripts\n",
    "            if chunk.choices[0].delta.content:\n",
    "                yield {\n",
    "                    \"response_id\": request['response_id'],\n",
    "                    \"content\": chunk.choices[0].delta.content,\n",
    "                    \"content_complete\": False,\n",
    "                    \"end_call\": False,\n",
    "                }\n",
    "        \n",
    "        # Step 4: Call the functions\n",
    "        if func_call:\n",
    "            if func_call['func_name'] == \"end_call\":\n",
    "                func_call['arguments'] = json.loads(func_arguments)\n",
    "                yield {\n",
    "                    \"response_id\": request['response_id'],\n",
    "                    \"content\": func_call['arguments']['message'],\n",
    "                    \"content_complete\": True,\n",
    "                    \"end_call\": True,\n",
    "                }\n",
    "            # Step 5: Other functions here\n",
    "        else:\n",
    "            # No functions, complete response\n",
    "            yield {\n",
    "                \"response_id\": request['response_id'],\n",
    "                \"content\": \"\",\n",
    "                \"content_complete\": True,\n",
    "                \"end_call\": False,\n",
    "            }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
