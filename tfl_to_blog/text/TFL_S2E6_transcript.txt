Hello world, what is up? Welcome back to the Feelings Lab. I'm your host Matt Forte, and for today's episode, we're talking about empathy and digital health. Admittedly, I may be a bit biased, but really enjoying season two so far. We've taken a couple of pretty big swings, and I've been learning a ton of fascinating stuff. One of our goals has been to take these complex concepts with massive implications for our future, such as compassion in robotics, and explore them through hyper-specific use cases we feel could be emblematic of a larger shift, kind of like the robot companion Moxie from that same episode. It's just the way my brain works. In order to wrap my head around these big ideas and take in the entire field of view, sometimes it helps me to dial in my focus to one real precise point. And I'm pulling the curtain back a bit tonight, not just to self-congratulate and talk about what a good job I think we're all doing, although, kudos team, but because I'm really excited to dig into today's topic by way of discussing the phenomenal work our guest has been doing in pursuit of developing better diagnostic and therapeutic solutions for children living with behavioral health conditions. When it comes to AI, something we keep talking about is scalability and how AI is enabling one to do the job of many. And you'll totally hear a bit of that today as we get into some of the amazing new diagnostic tools that Cognoa is bringing to the market. But for me, the stuff I'm really excited about and the really cool things tend to happen after diagnosis, the new tools, techniques, and treatments personalized and designed to meet the needs of underserved populations and narrowing further and hopefully one day eliminating the proverbial cracks so many people slip through when navigating the healthcare system. So as per usual, I got a ton of questions. What are some of these new treatments? How do we know who would benefit most from the techniques? And beyond that, how do we raise awareness that these options are even available? Another popular refrain on this show, AI is only as smart as the examples we give it. So where are we getting the data from? Should there be privacy concerns surrounding the information? I don't know.
I never know. For these and so many other questions, I do what I would hope any person in a position of ignorance knows well enough to do, and I defer to the experts. And it just so happens that today, as far as experts go, we have ourselves an embarrassment of richest people. Joining me as always, CEO and chief scientist at Hume AI, co-host and dear friend, Dr. Alan Cowan is here. Alan, good to see you. How you doing, buddy? Doing great. Good to see you too, Matt. Very nice. Back once again, professor of psychology at UC Berkeley and faculty director of the Greater Good Science Center, season one co-host and the cherry on top of the season two Sunday, the wonderful Dacher Keltner is here. Dacher, great to have you back. How you doing, bud? I'm doing well. Good to see you, Matt. Fantastic, man. Pleasure as always. I'm starting to wonder why you threw me off as co-host for this episode, but for the season. No, I'm teasing you. Dacher, you can dictate your title. I will put whatever you want in the lower third. As for today's guest and third co-host, he's associate professor of pediatrics, psychiatry and biomedical data sciences at Stanford Medical School. He has pioneered the use of machine learning and artificial intelligence for fast, quantitative and mobile detection of neurodevelopmental disorders in children, as well as the use of machine learning systems on wearable devices, such as Google Glass, for real-time ex-clinical therapy. His groundbreaking work has garnered numerous awards and accolades, including a spot in the top 10 of the world's top 30 autism researchers. He's the co-founder of Cognoa, a company dedicated to creating an unparalleled standard of care and pediatric behavioral health. Please welcome to the show. Great pleasure to introduce the great Dennis Wallace here. Dennis, my God, what a treat. Thank you so much for making the time, sir. How are you doing? I'm doing great. Thanks for having me. And I'm now a full professor. So, you know, I got to update my bio at some point. Congratulations, sir. Happy to hear it, man. Very, very awesome. All right, guys. Well, no time to waste. Let's get to it. I'm going to try and break this conversation tonight down into like different sections, diagnosis, treatment and augmentation. So let's start with diagnosis, especially on the heels of the June 2021 FDA authorization of Cognoa's Canvas DX autism diagnosis aid.
I feel like I got to start with you, man. Uh, first of all, congrats. That's a big deal to get that approval. And a second for our listeners and yeah, huge deal. Uh, and I don't mean to gloss over that by any stretch that is massive. And so very excited. Um, but my big question, uh, kind of set the stage for our listeners and those unaware. I know this is something I've heard you speak about, uh, a bunch of times before, but for those that aren't familiar, uh, before we get into how Canvas DX works, just briefly take us back a notch and talk a little bit about, uh, some of the pain points in the diagnostic process you and your team identified and, and why you were driven to create this tool in the first place. Yeah. I mean, the biggest issue has been the long waiting list and the, the average age of diagnosis in the United States is like stubbornly hovering at five. It hasn't moved much in, uh, in a long time, decades. And a lot of that has to do with the way we do it today. The center of care is slow, cumbersome. It's in clinic. It's one to one. You know, it's not scalable. It's not repeatable. There's all sorts of issues with quantification and subjectivity. Uh, you know, all of which ultimately can be fixed through canvas. Yes. Right. So it's faster. It's nimble. It's mobile. Moreover, it can be delivered in pediatricians offices, as opposed to in specialty care clinics, which is where these bottlenecks are occurring. So like, that's the main reason is to break open that bottleneck and get these kids diagnosed as early as possible and make it possible to happen. This phrase of ex-clinical is actually conceptualized in canvas DX to a certain extent, because a lot of the data that comes are coming from the parents who are using an app on phone that delivers information to Kaknoa that's operated on by an AI engine and then delivered to the clinician who can make a decision on the spot in real time. So, you know, ultimately we think it'll fix a lot of these issues with the average age diagnosis to start to move that from where it is now, which is like close to five to much lower, closer to two. Wow. Got it. So the question then becomes, right? How do you democratize the process, streamline it, scale it without sacrificing the integrity of the results? And that's what we're looking to do with canvas DX here. Eliminate those barriers. Totally makes sense. I want to get more into unpacking some of the stuff you said and the specifics of how it does that. But before we do, I've read before that this has been a very personal journey for you, a member of your family is autistic.
I'm curious, when did you start to first connect the dots and see how machine learning and AI could break down some of these walls and shake this whole thing up and solve some of these problems? My background, I'm a computational geneticist. It's sort of an interesting story that I can get into if you want to know more details. My thesis was nowhere near where I am now, which is essentially I worked on the population genetics and evolutionary trajectory of a paleotropical endemic moss that preferentially grows on coconut trees and tropical islands in South Pacific. For lots of reasons you can imagine, a good choice ultimately. But that led me to learn a lot about machine learning and computational genetics, computational biology. And meanwhile, my now my wife, Abby, I've known her family for a long time. Her sister, Becky, has a severe form of autism, which I've been familiar with since high school. We spent a lot of time together, ultimately convinced Abby to marry me. But I've known a lot about Becky and the pros and cons of autism and her family since I was 17, 16 years old. So I've always wanted to harness some of those skills to something important to those computational skills that I developed working with moss. And when I was recruited to Harvard Medical School to start my faculty life, I was able to do that. I was able to start to pivot my attention towards this problem. And I thought genetics would be the answer, right? Because it's very genetic. It's very heritable. But we're still working, we're still, we still are scratching our heads as to how the genetics play a role and how we can utilize genetics for clinical decision support and for, you know, for targeting therapy and things like that. That's still years from now, away from where we are. So I started again, knowing that going into that, I realized, well, you know, let's look at how the diagnosis is occurring. How are we arriving at these decisions in the first place? Who are the who are the children that were diagnosed with autism? So I trained myself on the traditional practices, the standard care, by shadowing clinicians and sitting behind one way, you know, mirrors that like are part of the observation.
It's part of the observation rooms where kids are getting assessed for an autism risk and ultimately diagnosed. And it was there that I realized that while the process is very hands on and the people doing it are wonderful, that it's extraordinarily slow. Yeah, it's also quite artificial. These are scary clinical rooms they've never seen before with a stranger they've never met before. Ultimately, depending on the level of shyness or anxiety the child is experiencing, you start to have subjectivity shifts occur, which can really influence the actual diagnosis. And at one point, this is a cool part of the story, which I found kind of baffling. The person scoring the child, they score after they do the observation, didn't like all the numbers and they were leading in the wrong direction, ultimately, in their opinion. She just changed them and made sure the child was diagnosed. So that was the moment in time where I just said like, okay, this can't be good. It's not going to help us with genetics. And it's certainly not going to help these children, you know, get the right, get to where they need to go. There has to be some, there has to be a way to infuse digital thinking and quantification into this. So that's the journey. It's motivated by my personal story. And by just this, this whole, this whole arena is crying out for AI solutions. God, it had to make that FDA approval all the more of an, like not just a professional victory, but an emotional one as well, like to reach that point after so many years. That's so amazing, man. This may be a pretty big question. We are a podcast about emotions. We talk in great length about the universal human experience, how we all process emotions. But I think it's worth exploring. And I'd love to hear, especially from the big brains I got on this show with me tonight. What, what do we know about how emotional behavior differs for individuals with autism? And again, I'm asking a very large question, but just, just to kind of get into the specifics. Alan, Dakar, what do we know? Well, I'll weigh in. I don't think we know a whole lot, frankly. I mean, we know some, we know social difficulties. We, there's a prevailing hypothesis.
I'd be curious to hear what Dennis thinks about it, of kind of empathy deficits in autistic kids. A lot of it is, no offense to the measure, done with this thing called eyes of the mind, I think, which I don't think is a valid measure, to be quite frank. And Dennis's work is revolutionary because, you know, I too have autism in my wife's side of the family, pretty profound and widely distributed. And the parents know something's up, right? And they know it early. And we know from one of the truisms of child developmental psychopathology is the earlier you get to it, it's just transformative, right? And they sense it in their tone of voice, they sense it in eye contact, there are differences in patterns of gaze in autistic toddlers. And so why aren't we gathering those data to start the diagnosis early? And that's why Dennis's work is revolutionary. So we know a bit, we know, you know, patterns of gaze are off, that are different, that they don't read eye movements quite as in a similar fashion as neurotypicals. Do you know more, Alan or Dennis, or what are you guys thinking about? Well, I mean, now going back, yeah, sure. I did a project once for a startup that was looking into behavioral assays for developmental disorders, and just looked through the literature. And what struck me is that there were all these tests people were conducting in the 60s and 70s. And the effect sizes were just through the roof, but it was like a sample of 10 people. And then when the samples get larger, suddenly the effect sizes get smaller. And that led me to believe that we actually don't know as much as we thought we knew. And it led me to believe we need more data. That's pretty much where I saw the reading the minds of the eyes task. And there were all these other tasks, different kinds of games and cards and identifying emotions or engaging in different kinds of social interaction or behavioral interviews. And I would say that the findings weren't satisfying to me. And what seems to be the case is that you kind of, maybe as Dennis was alluding to earlier, you know it when you see it.
You can observe a child's behavior and tell if they have autism. And it seems like that seems is a more reliable assessment than any of these supposed tests. And so how do we move forward from there? Yeah. Well, that's the really interesting and exciting thing to me about Dennis's work, because now we've just both said, well, it's one of those, you know, when you see it things and something that is to use a word we've used a million times on this podcast. So ineffable, how do you then train machines to help you identify it? It's typically a very human thing when we go, I don't know how to define it, but I know it when I see it. So just talk about closing some of those gaps and jumping over some of those hurdles. Dennis, how do you then employ the use of machine learning and AI to help us identify and diagnose this thing that very smart individuals sit here and go, well, it's hard to say, but we don't know enough, but we know when we see it. Yeah. And that's a great point. In fact, clinicians who do routine diagnosis will say to me, they can hear which kids have autism as they walk from their office through the waiting room to the clinical evaluation room. And I believe it, you know, and there is this, this, there's this thing called the clinical global, the clinical global impression that's used by clinicians to make, you know, to make diagnoses, they can literally pull a trigger on a CGI diagnostic basically, which is cool. And it obviously, for me, I think, you know, it harkens back to like Malcolm Gladwell's blink test. There's some, these guys possess this empirical intuition and have the ability to blink and tell which kids have autism. What I always wanted to do is figure out how to quantify that and provide it to non-specialists. It's like, ultimately when you think about AI in medicine, in a lot of cases, it's going to be scaling specialists, it's going to be scaling into the intuitive specialist behaviors and abilities to the community settings, to disperse settings, to lower socioeconomic settings, which is so, so important, right? To increase diversity and inclusion and everything else, which isn't happening at all today in the autism world, both for diagnostics and therapeutics, huge disparities there. So to answer your question, though, I think the one thing I just want to be clear for the audience is that AI is not anywhere near where it needs to be.
We have just figured out how to minimize and optimize the amount of human input and observations that are required. And those are essentially vectors that get put together into the equation essentially. And then those vectors combined enable the machine learning system to run to produce scores. But the cool thing also about this is that as we receive those measures, in particular, we receive those measures from independent observers operating in parallel, iterator reliability becomes possible, something that's not possible today. And we're getting labels essentially generated on time sequence video data that become obviously powerful potentials for future AI model development. You indicated earlier that AI is only as smart as the data we give it. And right now we don't have a lot of data on these kinds of individuals. But the more we can succeed with a system such as Canvas DX, operationally, in the future, there's going to be an opportunity to build smarter models that will obviate the need increasingly, although probably never entirely for human input in this equation. And that's good because scaling continues to be a thing we have to worry about. And of course, then you're going to have issues of security and privacy and AI performance drift and all these things we should be talking about and we're very mindful of. But I just wanted to at least say that for now, just to give you an idea of how things work. Yeah, for sure. My understanding based on what I read, and you keep me honest here, is when it comes to Canvas DX, there's three forms of data input. There's a questionnaire that the family fills out, a questionnaire the primary care physician will fill out, and then there's the questionnaire completed by a video analyst who reviews, I think it's two videos of the child recorded by the family.
Right out of the gate, just to go back to something you mentioned earlier, like the earlier observations, they're in these environments where you're not really sure what kind of a reading you're getting. It's a scary room. It's a doctor. It's a... Right now, we've... Okay, huge jump right there. They're at home. This is where they're in their most comfortable, most natural state. Fantastic. So, we've got these two videos, and that goes to the video analyst. And that was the part I was curious about is, you know, where do you find... Because now you've got to scale that part. You've got to find people that are qualified to analyze those videos. And what are they looking for in those videos? And what kind of training do they have to go through? And I was curious if there was AI enhancing that part of the process in any way, helping them ingest however many hundreds or so on the videos to kind of go through. Yeah, super great question. I love this topic. The scaling continues to be an interesting problem that we are trying to solve through a series of mechanisms that deploy adaptive learning techniques. Essentially, we're really trying to identify ways to use optimization procedures to serve up the minimal amount of labeled information necessary for the raters to rate. And they're working independently. So, it's good. So, we can get confirmation of a report from, let's say, three individuals, which is the minimum number you need to have a majority rule consensus on the outcome for a particular video, which I think is super important. They're operationally working within the context of the video length itself. So, they're able to score while it plays. That's good. So, it's relatively fast. Now, this is not something that right now the FDA is able to approve. But separately... So, the video analysts are formal, certified, trained. But we've been experimenting with the ability to mine empathy from citizen scientists that operate in the crowd on crowdsource platforms like we're all very familiar with. Microworkers, Amazon Mechanical Turk, Prodigy. And amazingly, you can find these people. And they want to do more interesting things with their... These sort of micro worker type tasks. And so, we've been able to...
It is a process to cast a wide net, identify super-recognizers, who are quite good at these features measurements, seeing these features that we want to see and have measured and labeled. They do it at scale, and they do it fast, and they do it at high efficiency. We have a couple of papers that have come out very recently documenting their efficacy against clinical standards. So far, the data suggests that they are as good as a clinician. That's cool. Wow. And, you know, Cognoa has not signed off on this. This is not Canvas DX. So disclaimers, disclaimers, disclaimers. Yeah. I think it's kind of interesting to think about that, because there are people that are out there in the world that are really good at this. Yeah. And it's going to include moms and dads and kids of doctors, and it's going to probably include humans who have forms of autism, and just people who are generally really interested and care a lot. Now, they don't necessarily know the task. In fact, we had to make sure, in most cases, they didn't know the task at hand. They didn't, because you don't want to bias the results towards the hypothesis. So, you know, they're not knowing that they're assessing for autism, per se, but you can imagine a situation in which that would be possible without biasing the inputs. So anyways, I think there's this future in which we could do a lot more with this concept of mining empathy from the crowd and having humans do more interesting things that are much more helpful for human health and wellness than what they might be doing otherwise. That response blew my mind. I did not anticipate it. That's so cool. Me too. Yeah, man. I was about to turn it over to Jack and Alan to get their response, because I was like, that really got me, man. That's amazing. I didn't anticipate that. So, Alan Dagger, have you guys encountered any such thing in your work before? I just love the idea of lowering the lift on how you process this data and being able to get regular people to process it is one thing. How close are we, though, to potentially having an AI recommend a person get evaluated? We're at the very beginning of the funnel, so nobody has to be able to make the assessment that this person has.
It needs to be checked, but somehow there's a pipeline out of data that's occurring naturally in the world or through, you know, some sort of relatively low cost or not too invasive product or app. And I think you think about the social problems that Dennis's work is pointing new avenues toward. I mean, one is self-harm and suicide. You know, so suicide in veterans is the highest young men, highest group that is presenting with suicide. And they're probably emitting these cues that a lot that if you crowdsource empathy, you could start to detect it, that other people around them may not have the right language for it. Right. Yeah. And may not see it. And so you get to them six months before they're really in crisis. And that's what we need, you know, for these kinds of social problems is to return to kind of this deep wisdom that Dennis is talking about. It's revolutionary in a lot of ways. Same with kids. Like little kids. You know, you take your three-year-old to preschool and you kind of know, like you see the spectrum, right, or the range. And you know, some kids get into serious trouble. We should know earlier through these friendly crowdsourced empathy forms. It's powerful. Well, that's one of the, you know, Alan, you mentioned how far off are we from an AI that can flag it for us? And then Dak, you talk about the significance of getting in there as soon as we can. And so I just, it's a confluence of all these things that I'm thinking about, because, you know, for anyone, we're talking about diagnosis and anyone that's ever watched an episode of House or I don't know, just listen to our conversation. You know, it's all about collecting information, identifying symptoms, and then finding a connection, detecting a pattern. And, you know, usually you think the best way to do that is just ask the patient. But we're talking about young children, we're talking 18 to 72 months, you know, what are the most effective ways and forms of data collection at that point? And, you know, how can AI and machine learning help us to streamline that process? You know, what do we do in that scenario when they're so young and so small that we can't ask them? We can only observe. Yeah. 
I'm going to have thoughts, research thoughts on all of this stuff. Like colleagues of mine, um, out of Emory in the Marcus Center have done a lot of work on eye contact and observations, eye contact exhibited by babies when they're presented with familiar images, like of their mothers and deviations of them being predictive of a future autism diagnosis. Interesting. So there's sort of partial answer to the question, is eye contact going to be every aspect of this? Is it, so in other words, is that going to be sensitive? Yes, probably, probably overly sensitive, non-specific, right? Um, therefore you get this sort of, the question then becomes like, are you, are you false alarming too much? And does that false alarm have a negative downstream consequence on the child or the family? And we should think about that and be careful, of course. Um, but sound is another aspect that's interesting. There's potentially some early pre-verbal audio signals that could be utilized, perhaps in context with, with eye contact kind of measures, non-invasive, baby babble, basically, that could be predictive. I'm super curious, uh, to, uh, you know, in, in all the work that you're doing, has that process and has this process of trying to streamline, of trying to observe, has it revealed symptoms, uh, you didn't even know existed or were looking for? How has this been, you know, outside of the scope of autism, has, how has this been applicable and has it revealed, uh, avenues of opportunity and, and ways that can be used, uh, for all types of treatments and diagnosis? For, for the developmental spectrum, I think, yes, like there's a number of other developmental delays that are meant that we've been able to find and identify actually through some of these techniques. Like just global developmental delays, not otherwise specified speech and language delays. We've worked in Bangladesh where there's a high, um, high frequency of stunting in children due to malnutrition, which is an incredibly sad scenario. The stunting of course, manifests as many neurodevelopmental conditions. These kids are just not growing and getting enough nutrition to grow the way they need to be growing.
And so they have autism and speech and language and a bunch of other stuff. So there is definitely ways to take and adapt everything that we've done for Canvas DX directly to those kinds of problems, which starts to expand the reach and find children, perhaps even including dyslexic and complex cases of dyslexia and other things of those natures. This opens up a whole new world for how we think about the applications of computer vision and how we think about concepts like domain adaptation and transfer learning, stuff that's super popular now and concepts of meta-learning, low-shot, few-shot, no-shot concepts, which is all of what we're working on right now, trying to figure out how do we minimize the requirement for input without any cost to accuracy. We want to maintain this clinical standard of accuracy that the Canvas DX system hit for the FDA while continuing to iterate on the ability to go down in age, even below 18 months, perhaps. I mean, again, I'm probably talking too much, but at some level, it doesn't necessarily matter now if intervention programs that we have are only going to work when they're two. There's a bit of a gap between detection and intervention, and we want to shrink that gap as much as possible. But we do know if we intervene between two and eight, ideally before six or by six, kids are going to progress in ways that are super positive without compromising their diversity. It's providing kids skills to thrive the way they want to thrive, to live the life they want to live. Yeah, yeah. That's a great segue to kind of move into intervention and treatments and talk about that a little bit. Autism is known to be a spectrum, right? Not a gradient. And I would imagine treatments and intervention exists on a spectrum as well. There's no one-size-fits-all treatment for everybody. How do we identify the best treatments, and is there aâ€¦
What is your role for machine learning and guiding one towards the best course of care? That's a question for me, I guess, right? I'll listen to anyone who wants to talk to me about this stuff. I don't know anything. One, we've experimented with this, with wearable augmented reality, with Google glasses in particular. Love that stuff. It's a perfect form factor because it's lensless, it's easy to ignore the peripheral monitor in an instant. It fits on most kids' heads, not all. Some of them didn't work that way. It provided us with an opportunity to naturalize, to take into the homes some of the aspects of standard therapy that is thought to be, and has been proven in certain circumstances to be effective, which is essentially teaching kids how to understand emotion. This gets at the question of affect and effective range and perception of empathy and things like that. But right now, the applied behavioral therapy that we use standardizes around the importance of reinforcing the recognition of the base emotions, the standard universal emotions, the Ekman emotions. Happy, sad, surprised, afraid, disgusted, neutral, contempt. You're amongst friends here. We've talked in great length. That one I know. That one I got. With this, we light up the box, the peripheral monitor, when they find a face, which is one aspect of the intervention, which is really great for them. They're like, oh, cool, I found a face. And then it's immediately providing them with what it's telling them. It's a happy face. And we were able to show this is very efficacious. Is it a one size fits all situation? For the particular treatment arena, given that most of these kids suffer from, suffers maybe the wrong word, lack the ability at this point in time, when we're trying to intervene to differentiate those faces, then it becomes kind of a baseline foundation to get started. Helps them sort of like it's it's essentially a.
It is designed to be developmental age appropriate, and you can iterate across all verticals you can think of to make it really interesting and engaging for the human who is working with it. And this is sort of getting to your question, like we've envisioned an area in which the game deck choices are super diverse such that they can select among them to almost create their own treatment choice. You know, where we know something because with some, you know, maybe restrictions and requirements, essentially, as far as ensuring that we deliver a dose that we believe will be minimally viable to have a treatment effect, the rest is kind of their their choice. And we learn more about them through that than we would ever otherwise learn because we're, we're doing they're on mobile, they're on app, they're playing the game, we're getting game metrics, we're communicating with them through the game, this is Stanford's IRB approved protocol at this point. And we also learn about how much they play of what and what that means for their, their movement, you know, their progression towards positive things that we, the end, like the FDA cares about, points, measure change that is believed to be positive and in the right direction up into the right, I guess. So ultimately, that kind of system enables you the like, because it can be widely distributed enables like a wide coverage of diverse humans who have the condition autism to a point where we have enough empirical data to actually know, like, what works for one type of kid and what works for another type of kid. So it becomes prognostic, perhaps, over time. Yeah. Well, that was those
Two instances, I was excited to hear, talk a little bit more about him and chat about him because one, the Google Glass thing, you single handedly, that use case right there, like explain to me why Google Glass needs to exist better than the untold millions they spent on trying to convince me otherwise. That was the first time I was like, okay, now I get it. That's a reason for that thing to exist. That makes sense to me. And so I was really thrilled by that, man. I thought that was such an incredible way to use that technology. But the guess what thing to me was a stroke of genius because yes, you have the game element, but you're also solving that problem of there's not a lot of data. So you're doing two things at once. You're creating a fun activity and an educational game that the families can play together. And if they opt in, they can share all of this information that just helps you make everything better as you move forward, which begs the question, and I'm sorry if it feels like I'm picking on you, Dennis, but do you, how important do you see gamification moving forward? We had a guest a couple of weeks ago talk about gamifying the workforce and just seeing this different energy, this different generation sort of entering the workforce with different expectations. And I'm curious, you know, we have those two examples there. How closely are you guys looking at gamifying all of this moving forward and adding that sort of dynamic to the process? Yeah, I think it's incredibly powerful. I think it has to be a large part of the fabric of the future of artificial intelligence in medicine, period. People want to play games, and they're engaging and they're fun. And right now, the way we play games, yeah, it's not all that beneficial, right? I mean, ultimately, screen time is a bad thing, perhaps for a lot of circumstances, but the kind of games that children tend to gravitate to are the ones that are popularized in ways that, you know, and we can talk forever about like, you know, the whatever, you know, fortnights and whatever else is of the world, you know, where there's first person shooter type games. And to steal a quote from a friend of mine, Billy Zane, I should have mentioned, such an awesome character. I've chatted with this. He cares about child health.
He came, his concept is first person healer games. And I'm like, yeah, that makes a lot of sense. And of course, then all of a sudden this movie came out. Um, uh, what is the name of the movie that has, uh, the character from Deadpool in it? Um, that just came out. Anyone, anyone, all of you said, Billy Zane, and all I could think of is the phantom. I just, I'm lost. I'm 30 years back. He's a cool character. Um, sort of inspired me to think a lot about mining empathy and this first person healer concept. But, um, you know, I think games have a significant, important role to play in both education and treatment, health monitoring, because you can do a lot with it through iterative design, essentially that's adaptive and nature over time that sort of works with the human. That's, you know, the stakeholder that we want to, you know, who we want to work with, who we want to treat, who we want to educate, um, while building data as you go and using that data in real time, and then also using it in the future for all sorts of iterative model design, which I think is super important. Super cool. Alan, looks like you were about to say something. Go for it. Right. One thing I wanted to ask is that, you know, so you have sort of task-based interventions where you gather data in a certain way. I'm also interested in, you know, where can you funnel somebody into the task-based intervention intervention based on sort of an observation of what's happening in everyday life. Like if you had, and as a thought experiment, if you had footage of imagine there's no privacy, you just have 24 hours a day footage of somebody, of a patient, what are the critical moments that you look for in that footage? How do you sort it? And can you get an AI to do that or be activated at the right time to ask the right questions, to record the right moments in order to funnel people into the right treatment? Yeah, it's a great question. One that I, a graduate student and I were just chatting about actually yesterday, because we got a bunch of video data, um, and we're from the guess what gameplay, right? And we're trying to, um, one of the tasks that he's working on, the student has been working on was the ability to
It is designed to detect eye gaze through game play. The cool thing about the heads-up game, I dropped my phone, is that it creates a structured distance between the partner, the play, the mom, for example, and the child. There's only a certain height difference. You get pretty reasonable coverage of the child's face, and you can see where they're looking. We can ask questions about, can we measure eye gaze and eye contact? Are they engaging? Are they making sufficient amounts of socially motivated facial engagement? Which we think is kind of important, probably, when they're communicating in this pro-social game situation. But it's super messy data. The graduate student has been working on scrubbing it. As he's doing it, he basically created a way to look at a thousand videos. He did it in two hours. I don't know how he did that. But he was just really quickly just kind of annotating, annotating, annotating. He thinks, and I believe this, that that kind of process can be learned. His annotation process can be used to train an engine at the same time. It's a little bit like snorkel AI you may have seen or something like that, where you take human annotation behavior and you model it and apply it to a specific subdomain. And ultimately, you can scale it. And I think that's actually really interesting. Yeah. I mean, if it's facial engagement, that would be really interesting. I think we can start to do that now with some of what we're developing. If it's eye gaze, that's another one that we can do. I wonder if there's a way that in everyday life you can have a device that captures the moments that are actually causing people stress or any kind of negative emotion and work backwards from there. Because it might be that you're living comfortably with autism and you don't care about this eye gaze problem. You know about it, or maybe you don't know, but you're able to function in everyday life happily. Whatever is causing the actual negative emotions, I think that's what you really want to care about. So I think if there's some way you can capture that and work backwards. It's so amazing that you bring that up because one of my big...
Questions coming though. And the reason I mentioned at the top that I was so excited to talk about, you know, solutions and treatment and augmentation, all these things is because for me, if someone is vision impaired, the question is, how do I help them see again? If someone is hearing impaired, it's how can I help them hear better if they have mobility issues? All right. Is it doing it in the wheelchair, crutches, exoskeleton, whatever. And this is by no means me downplaying the challenges inherent to any of these situations. But my point is simply when it comes to autism with ASD, it is so nuanced, the challenges aren't as cut and dry as some of the other things that are out there. So, you know, how do you, how do we build effective tools, AI driven or otherwise, to help these individuals navigate their disorders? And what do those tools look like? And you're scratching that itch right there, Alan, with that question of like, can we identify those things and work backwards? Such a crazy idea. Yeah. Well, I mean, in a way, you know, it sounds kind of creepy when you talk about recording people's lives or, you know, introducing more data. But in a way, it's kind of you want to cut back on the data, alert the AI when there can be data. When should it be recording? When should it be asking questions? You know, when are you potentially feeling pain and you want to ask the patient, what kind of pain are you feeling? You know, where is it hurt? How would you characterize it? You know, that kind of thing while it's happening. Or you want to ask the patient sort of, you know, what is it that you can, like, that's driving your confusion in this given instance? Like, have you lost focus? Is there a toxic relationship involved? You know? So many variables. Yeah. Yeah. And it's sort of, you know, this speaks to treatment as well. If you have autism, perhaps it's not a problem necessarily to live with autism and be high functioning, but potentially if somebody in your life is not responding in the right way or doesn't know, perhaps the intervention is for them. They should know how to form a healthy relationship with somebody with autism. And there you might want to, so there are more specific sorts of interventions you could do then. Yeah. And it's particularly true for the, you know, the adolescents and adults and population now that's progressed and, you know, needs.
It is a collaborative effort that is engaging in the workforce, schools, and on both sides. You need to understand how to work with each other and embrace new diversity. Identify what might cause anxiety, what might cause a tantrum, and figure out how to work backwards so you don't have that happen. The kids that we're working on are young and arguably less complex than the adolescents. Yeah, that has to be. Which makes our tasks somewhat easier by no stretch of the imagination. It isn't easy, but it's somewhat easier. The layers of complexity haven't piled on yet, necessarily. For sure. We're coming in the homestretch. I got my eyes on the clock over here and I'm cognizant of everybody's time. I always love when we get towards the end to go blue sky and talk to me about the future. We've touched on little things here and there. We've posited some things in the 24-hour video surveillance that Alan brought up. It seems to be all four. Kidding, kidding. Yeah, yeah, yeah. I'll cut that out. Seriously, talk to me about the future. What are we excited to see? Where do we hope to be in the next, I could say, five, ten years, but also five months? Everything's moving so quickly now. What's on the horizon that's getting everybody really stoked and really excited and where they hope to see us with all of this coming down the line? Dacher, I'm going to call on you first. I want to hear you. What do you think? One of the things we know from kids with ADHD and some of the impulsivity disorders is social skills really matter. They just have to learn how to sit in a classroom and have a conversation and take turns when they're speaking with friends and so forth. This is the theme of our show is emotions are multidimensional and they are social. Alan's been working on, with Hume, some mimicry data that just captures how well people can mimic 25-year-olds.
I think the basic science combined with really accurate assessments of where you struggle, which is true of anything, it's individualized diagnosis. I think it's going to be a big opening in the space. And, you know, Dennis's work is revolutionary to get it early and to get it into the hands of parents. That's a game changer. So, I'm hopeful. Yeah. Alan, I do want to get your thoughts on the brighter future of tomorrow and what's going to happen, but listening to Dacher talk and bringing up again, all the work you guys have done in mapping human emotion. One of the things I was thinking about earlier, we've talked before in this show about accounting for geographical differences, cultural differences, age differences, all these different things. And I'm wondering, you know, when it comes to the different ways in which emotions are processed and exhibited by those with autism, is that data sort of siloed into its own space for now? Or is there a great value of including that in this sort of baseline of human emotion? And I'd love to hear your perspective on that and what you think. I think it should be included. I think that, you know, if autism is a spectrum, you know, and I think many other disorders are spectrums as well, from healthy functioning to different kinds of, you know, eccentricities or sort of dimensions. And, you know, so much of that hasn't been explored. And I do think the data, when you say it hasn't been segregated, I think the way it's been studied has been very largely segregated. There's developmental psychology, there's emotion science, and emotion science, you don't study these patient populations, you don't typically study kids, and then there's intersections. But typically, if you're going to do a study, it's one or the other. It's either a study of, you know, healthy functioning and variation, or it's a study of one population, and not of the overall.
I think one of the really exciting directions to the future is to study neurodiversity in more of a data-driven way, not focus on diagnostic categories being your prior, but focusing on symptom dimensions being extracted from data and understanding which dimensions are actually things you care about, which dimensions are causing people negative emotions, stress, tantrums, all of those things, and working backwards from there to solve those issues. And then you have, you know, what are the critical moments that reveal those dimensions in life? And how can we gather data at that critical time? So, we lower the lift on finding the right, because essentially, what the doctor is doing when they're diagnosing a patient is a form of data processing. That's pretty much it. I mean, if you can lower the, and you know, that session is very expensive. It's a very expensive form of data processing. And so, the fundamental problem is to make the data processing cheaper, right? I think we need that in every dimension of clinical care. So, I think that it's really important to find what is the sort of lightest weight intervention to gather data, and what is the right time to do it, and how do you process that data? I think a lot of that could potentially be working backwards from where do we experience the most negative emotions? It can be alerted to sort of document that, and then find the root cause. Wow, that's cool. Amazing. And Dennis, you've given us a taste and told us a little bit about the things that you guys are looking ahead towards, but I always love to hear it, man. What's your blue sky? What's the sky you're excited these days? Yeah, as Alan just said, it's super important. I think just riffing off that just a little bit, I think absolutely, like the candesthia system for me is exciting, but really hopefully just the first of others that will help globalize the access to care, just like we can do here in the United States. And one thing that's just kind of a little boring, but exciting for me anyways, is the FTA, and how they're starting to embrace adaptive models and utility of adaptive models in practice. They're getting there. And I think in the next five years, they'll be completely there, and there'll be these scenarios in which you have with them.
It has established a predetermined change control plan that enables you to adapt your algorithm as you go effectively and securely and safely, but make those changes without huge interruptions to the access to that new improved model by the stakeholders. And of course, all of that with the United States has to be covered by insurance and we cannot be charging our families out of pocket for these kinds of solutions. But I do think like making the data and decision process of more affordable and scalable is what's going to be happening really in the next next two years, really with Kavanoa and Canvas DX. But in the next five years, globalization of the same sort of solutions to countries where there isn't enough infrastructure, making sure that we address problems in Bangladesh and the Philippines and other low middle income countries so that they can have the same sort of services that we can get here. And I think it's through these games and mobile based solutions that have embedded AI that we can actually achieve that goal where we're getting complete diversity inclusion. We understand how the models perform across all the demographics you can think of. And it's I think through the get through through deployment of systems like this, it's going to just increasingly happen in the next two, three, maybe five years where we start to realize a lot of the potential of mobile AI in health. Wow. Awesome. Thank you so much, guys. We're over time. It pains me to say I got to wrap things up. Pardon me. I got to wrap things up. But before we go, I got to thank our guest, Dennis. You've been so generous with your time. It means a lot to us, man. Thank you so much. And keep up all the amazing work. So awesome to have you here. Co-host for life, Dakar, your voice and insights are like a warm blanket for me. And our listeners agree. Thank you so much for stopping by and hanging out with us again, Dakar. I love seeing you every time, man. Alan, another great episode, sir. Always a pleasure. Thank you to you. And of course, you. That's right. You, the listener. We appreciate you most of all. Thank you for spending all of your time with us today or however much time you spent. I don't know. Whatever it is, the fact that you're hearing this part, it means you made it to the end. And that means a lot to us.
So thank you so much for hanging out with us. We try our best to pack a lot into a small package. So if there's a question I missed or something you want to know more about, let us know. Just shoot us a question over at thefeelingslab.ai. I can't wait to see what you got. And hey, if you really enjoyed today's episode, and you're feeling saucy, go ahead and throw us a five star rating on iTunes. Couldn't hurt. I'm actually not 100% sure it helps either, but I know it couldn't hurt. So go ahead, send us a review. That's going to do it. We'll be back next week with a new guest, a new topic, and a bunch of new questions. Until then, farewell for now, my friends from the Feelings Lab. I'm Matt Forte. Thanks again, everybody. And please stay safe out there. Transcribed by https://otter.ai
