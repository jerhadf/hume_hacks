Hello, world, what is up? Welcome to a Feelings Lab special conversation. This is Hume AI, Mila and the National Film Board of Canada presents the International Conference on Machine Learning's Expressive Vocalization Workshop and Competition. You know, you've got a good title. When it's so long, it wraps around the souvenir t-shirt twice. Joining us today an extraordinary panel of event organizers to share their thoughts on expressive vocalizations, the competition, and if we're lucky, a little bit about themselves as well. In no particular order with us now from the Hume AI team, Dr. Paniotis Tsourakis, aka Panos is here, a computer scientist and AI researcher with expertise in deep learning and emotion recognition across modalities. He's recently joined Hume AI as an AI research scientist. Also coming to us from the Hume AI team, Alice Baird, an audio researcher with interdisciplinary expertise in machine learning, computational paralinguistics, stress, and emotional well-being. She's recently joined Hume AI as an AI research scientist. Next up, a core member of Mila, Gautier Gaudel, is an assistant professor at the University of Montreal in the Department of Computer Science and Operational Research. He has published several top-tier conference papers on generative adversarial networks, or GANs as they're commonly referred to, and recently, specifically, worked on laughter generation using GANs as part of an artistic collaboration with the National Film Board of Canada. Definitely going to have some follow-up questions about that. Also working on that artistic collaboration with the NFB from DeepMind in Montreal, Corey Mathewson is here, a research scientist with DeepMind and a lab scientist with the Creative Destruction Lab. Corey holds a PhD in computing science from the University of Alberta with the Alberta Machine Intelligence Institute. His research focuses on studying the interaction between humans and machines with a focus on conversational dialogue systems and human-robot interfaces. Last but certainly not least, I know him as my friend and co-host of our podcast, The Feelings Lab, but I suppose more relevant to this conversation, he's the CEO and chief scientist at Hume AI. Dr. Alan Cowan, an applied mathematician and computational emotion scientist.
Developing new data driven methods to study human experience and expression. This is quite a lineup. Welcome one and all and thank you everyone for being here. I know you're all super busy. So making the time for this chat means a ton to us and we really, really appreciate it. How's everybody doing? Everybody doing all right? All at once. Awesome. Great. Honestly, I would love to get, you know, a recording of your voice like describing my reading my bio. I want to put this on my website. Let's see if it's just like. You got it. Anytime, anywhere you write, whatever you want me to say, I'll say it for you. No questions asked. Done deal. That is how I shall repay you. And thank you for giving your time to us today. Let's jump right in, guys. Let's get to it. My understanding has always been competitions and workshops like these exist, not just because they're fun, but for a reason to spotlight a specific sector of the research community, to raise excitement, interest, accelerate progress in the field, all these things. So to that end, we've talked about vocal bursts before on the show, but Alan, what is a vocal burst for those that are still in the dark? Why are they so important? I figured you could help shine a light on that one for me. Yeah. I mean, vocal bursts are these non-linguistic vocalizations that we make like laughs and sighs and gasps and yelps and hmms and hmms and huh and huh. Hang on. Give me five more. Keep going. I can't do my blood-curdling scream today, Matt. It's going to hurt my voice. This is what the people tune in for, Alan. But these sounds are incredibly powerful. They're rich. They're ubiquitous, particularly laughter. But every sound we make and huhs and huhs and hmms, they just happen all the time and until recently have been largely overlooked in emotion science and not to mention machine learning. And there's just not much data out there to train algorithms to understand these vocal expressions. Yeah. You mentioned they're ubiquitous, they're non-linguistic noises, so that means they're universal. Do they vary from culture to culture or is it across the board? Human beings. This is what we do. When we look at vocal bursts versus facial expressions versus speech prosody, language, vocal bursts are the most universal across all the cultures we've looked at. Generally, I mean, we just ran a study with
Five different cultures, we had hundreds of thousands of samples from tens of thousands of participants. And we identified that there were 25 dimensions of vocal bursts total and 24 of them were preserved across cultures very, very well significantly. So vocal bursts, like facial expression, we did the same thing, and it was 17 out of 28. So vocal bursts are incredibly cross cultural. That's amazing. Last one, and then I'm going to go around the room and start asking everybody else some questions. But do they offer any insight beyond what we're capable of observing by focusing purely on language? Is there something that it kind of opens a door to that we weren't seeing before we started looking at them? Huge. Yeah. So if you just look at language, most of what we communicate is kind of semantic. We're the facts of what's going on. And we kind of rely on vocal prosody and vocal bursts to convey what we really are feeling. And it's just a communicative element of what we're saying. So when we're frustrated, we don't usually say we're frustrated. We say, ah, it's the wrong song, or you know, you did something wrong. And then the person knows to correct their behavior that propagate that if it's a machine learning algorithm, they can propagate that and improve the algorithm. Right. So it's really important in terms of speech prosody. So while we're talking, we have vocal intonation, tune, rhythm, and timbre of speech. That is a really important signal as well. But it's much more subtle. And it's much less readily identifiable. And it's less universal. And there's fewer dimensions. So maybe 16, we've identified 16 dimensions of speech prosody that are conveyed across cultures. Whereas with vocal bursts, there's 24 dimensions that are conveyed across cultures, and they're just much more identifiable. And when you hear them during a conversation, it sets the tone of the whole conversation, it really almost overrides prosody. I know I said this would be the last one. But I do have a follow up because when you said you've hit 16, right, and language, is that a limit? Is that you've hit a wall? Is that a limitation of any kind of technological thing that you're waiting on? Or is it just you feel like that's it? We found them. There's 16. Good work, everybody. Go home. It's a lower bound. But you know, with increasing data, there's going to be a decreasing return on what we find. I mean, we're already looking at a pretty broad range of cultures.
We've looked at speech prosody in particular, we've looked mostly at four cultures and across those four cultures, we had 16 dimensions with hundreds of thousands of samples. There's lots and lots of countries to look at. We could look at 170 more countries. We could look at over 6,000 languages, but there's just going to be a diminishing return to that over time. I think we've captured the majority. Very cool. Amazing. Well, thank you for that. Let's go around the room. As it were, I cracked the window ever so slightly in all of your stories in my introduction, but I'd love to hear from each of you a bit more about your journey and find out why vocal and speech emotion. What is it about this corner scientifically that excites you and why you're here and involved with this workshop and challenges? Who wants to go first? If not, I'll call on somebody like the old days, but let's see. I can go because I want to reflect on something that Alan said. He mentioned that vocal bursts were intercultural, but it's also intergenerational. This is something that actually is the reason why I'm here. I originally got contacted by the NFB, which is like National Film Board of Canada, because they were basically producing an artistic project for an artist who wanted to do a project around laughter. And the original inspiration of this project by this artist, which is Etienne Paquette, by the way, is the way he managed to communicate with his grandma, I think it was. Yes, with his grandma. So his grandma is deaf. And so he has a hard time to communicate with this person in a verbal way. But he basically telling he was telling us this story that was so touching about the way that through laughter, they were able to basically like basically, when they were together, they were laughing all the time. And there was there was it was the you know, even when I'm talking about it, like it tells me something like they were managed to, they managed to exchange, you know, like emotions and feelings. And, you know, I don't know, there is something very interesting in this aspect that this is in some sense universal, like this vocal burst, burst has something that is universal across
ages across culture. And this is in some sense what what it's one of the pieces that connects all of us. Yeah, it's it's so inspiring to me. And so I had this like wonderful opportunity to kind of contribute within a artistic project. And you know, be able to, I mean, for me, you know, I'm a scientist, most of the thing I do, I wrote a research paper on like 10% in the world read them. For once, you know, I could do something I could tell my parents, you can go it's safe, you will understand. And you know, it means something to me to be able to like have this reach. And yeah, that's, that's how I got. That's a beautiful. Do you want to talk about universal? Everybody just wants to do something they could show mom and dad and have them be proud. That's amazing. Thank you for sharing that. Corey, do you want to go next? I believe and keep me honest, you're working on the on this project as well, right? Yeah, 100%. I've been working on this for just over a year working with coach a and Elif Muller and the National Film Board, some fantastic producers there. I got looped in because my background is in performance theater. So like I was building robots and conversational dialogue systems to like do comedy shows alongside, and then measuring how audiences would sort of react and interact with them. And some of the biggest laughs that we ever got from the audience was when we just incorporated some little thinking sounds before the punchlines would come. So the conversational bots would like come up with a response line. And then before we would synthesize that as audio, we would just put in these little thinking sounds. And those thinking sounds were just six audio recordings that my dad did talking about having something to show your dad. And he was like, if it's gonna take a long time to come up with what to say next, why don't you like put in a sound of it thinking? And I was like, where am I going to get those? And he just did six like Alan just did here. And it's like, yep, those work, and the audience loves them, and they work perfectly. So I think a lot about like how we interact with each other and how we interact and use technology to share our stories. And I think that there's a lot of innovation in these neural models that allow us to tell stories that we wouldn't otherwise be able to tell.
And share an interest in creative ways. And so innovations on those technologies is what I find really inspiring. And this kind of like feeds into that. What would I Yeah, sorry, go for it. No, no, no, I just you're sitting at a very exciting crossroads. We've had, we've had comedians on the show, we've talked about this. And every time we talk or try to talk about comedy, there's, there's always we as with any emotion, the ineffable thing and the thing you can't put your finger on. But I love and I could dig into a whole hour just sitting with you about bringing robotics in breaking down and scientifically approaching comedy. There is a whole other conversation for you and I to have at another time. So don't go far. But what were you going to say before I interrupted you for very selfish reasons? Yeah, I mean, I already I'm one sentence in and I got an invite to the next episode. I love it. I was gonna say, my grand challenge, I think challenges are great. I think grand challenges are great. And having a North Star is fantastic. Because a lot of innovation machine learning comes from setting something up that feels unattainable. And then the community just all of a sudden figures out how to work and make an innovation to progress towards it. Right? That's like what happened with ImageNet and convolutional neural nets. That's what happened with Netflix recommendations. That's what happened with the Lodner prize and Turing tests and all that. So like, I think that setting those grand challenges up. So in my head, my North Star, my grand challenge is having a stage with robots on the stage performing a show. And in the audience, there's a room full of robots. So there's just robots on in the audience watching robots on stage, and they're laughing along and enjoying the show. And to get to that level, you need the audience to be able to react and they're reacting with vocal bursts, right? And there's no emotive generation possible right now. And so I see this is like a fundamental step towards my North Star of being able to like peek into a theater and just seeing the beauty and horror of the nightmare or dream that is robots performing to other robots. So glad you said that. Because as you were going on, I was going to push you to acknowledge that for someone independent of any of the context we've provided to wander into a room and suddenly discover sentient robots laughing and performing.
That is very much a nightmare to realize without understanding what's happening in that space, but I do fully recognize why that would be your North Star in your dream. That is a very cool dream to have, but we'll get into deeper of what we could do with that. We'll get to that in a little bit. Let's go over to Alice. I'd love to hear a little bit of your story and why you're excited to be a part of this challenge in this area in this field. Yeah, sure. I've always been interested in the subjectivity, I guess, that is involved in paralinguistics. I think with vocal bursts, we have a bit more of a consensus between people about what they might be expressing. I think that makes it an interesting area to work with. I think as Corey said, challenges generally just push innovation in a way that is exciting and there needs to be a bit more. I think we've written about it in some of our work recently, and I think Alan's mentioned it as well, but there's a lot to be done in this area and there's a lot of importance with vocal expression and particularly nonverbal vocal expression. I think this is an exciting time. For me, it's just that challenge of understanding the subjectivity was always where I got into this area. I started with understanding sincerity. That was the first paralinguistic trait I worked with. Awesome. No, it's great. Thank you for sharing that. It's a far less terrifying North Star than a room full of sentient robots laughing at you. We're doing great. We're getting a little bit of everything. I'm loving it. Vanos, on over to you, my friend. What about you? Tell us a little bit about yourself and what you're excited about in this area, man. Yeah, so for me, I mostly study speech prosody and mostly because vocal bursts weren't that much. We don't have enough data to, or at least we didn't have enough data to study them in the affective computing community. And most of them were like laughter or fear that were being studied. But now here we have the data, we have the data to understand vocal bursts and we have the data to generate vocal bursts. And for me, it's very important to make the interaction between human and animal.
as naturally as possible. And in order to go to this direction, yeah, we need to consider the vocal bursts and yeah, truly understand them. For sure, for sure. Fascinating. All right. Well, first of all, thank you everybody for sharing stories and telling me all that's fantastic. I appreciate that. Let's, let's go back to the expressive vocalizations workshop and competitions. Talk about that. Alan, I got a one sheet here with a bunch of data on it. And as far as I can tell the ideas we're looking at the problem of understanding and generating emotion, vocal bursts. Cool. We covered that. There are from what I see three key sub challenges of focus. Is that right? Yeah. Yeah. So there's three sub challenges. One is focused on recognizing vocal bursts across multiple tasks, recognizing the emotions conveyed in vocal bursts, and also recognizing a little bit about the person making them. So their age and country. And this is just the basic task that we've never really seen truly state-of-the-art algorithms trained for. So it's going to be really exciting to see this. On the generation side, that's a very different end of machine learning. And so it's really exciting to combine both of these challenges in one competition. And it's really, it's possible and it's practical because of this, the fact that we're providing this huge data set of vocal bursts for the first time. So no matter what people do with this, it's going to be novel, I think. And people are going to be training state-of-the-art algorithms to generate each different emotion that we're providing in our data sets. We're actually providing 10 emotions, amusement, fear, anger, I think. Some other ones. I'll memorize it. But it's a great place to start. Particularly, we have some that are really distinguishable, like amusement and sadness or fear. Then we have some like amusement and embarrassment that are going to be more challenging. I could be crazy, but I could have sworn I saw, because I just closed the window, I'll have to reopen it in a second. But I saw fear comma horror that you had two. And we dug into this in a whole episode, but I was going to ask a larger question of how do you arrive at those 10? But also, did you intentionally choose two that the public may perceive as similar to see? Talk to me a little bit about the decision that went into that. So fear and the horror distinction is a subtle one, but it's one that we see across cultures. And so it's...
It is a challenge, but the information is there, and we wanted to set up some dichotomies that were really easy, or not really easy, because it hasn't been done, but that would probably be easy, like distinguishing a screen from a laugh. And some that are more difficult, like distinguishing two different kinds of laughs or two different kinds of screens. And I think that's going to be really important because so much is lost if you're just categorizing things as laughs or sighs or gasps, just broadly, what kind of type of vocalization it is. This is really getting at the meaning. So yeah, with fear horror, horror is kind of a blood curdling scream that you'd hear in a horror movie. That's not something you hear very often except, weirdly enough, like when you go to an amusement park or some place where there's no horror at all, hopefully. Depends on the amusement park, exactly. It's true, it's true. And then fear can be more subtle. Sometimes it's not a scream, sometimes it's a gasp. There's a negative surprise fear continuum. And it sounds scary to recognize these things, actually, it's really, really important. Imagine you're in a situation where somebody is reaching for an emergency phone booth, right? Like you'd see on a college campus, and they can't quite get there, but you get a scream. It should be able to capture that there's a scream, right? There's so many use cases for this kind of thing. And so these distinctions are really important to focus on. Got it. All right, so just to go back. So we've got the multitask challenge, we've got, I think, generation, and then the third one is a few shot, right? A few shot challenge? Yes. The third challenge is really about personalization. And what few shot means is that you get a few samples of somebody forming a vocal burst, and it helps you guess as to what their next vocal burst means. So it's basically a personalization model. And personalization for understanding emotion is going to be so important to the fields. Sweet. All right. So those are the three high level challenges. Let's break them down a little bit further. Alice, can you go a little bit deeper into the multitask challenge for me and just tell me kind of what you guys hope to achieve with this challenge, what you'd love to see why it's important and just break it down a little bit better. And remember, I'm not nearly as.
We're hoping to see there's some sort of codependencies between those labels and be able to explore that, what the benefits are of jointly learning these things together, but also from an efficiency perspective, see if we actually can obtain good results with emotion and other traits, we can still obtain stronger results for emotion. So I think that's kind of the core things there. Yeah. Yeah. Alan, just out of curiosity. Yeah, it's perfect. Is any of that, any of the multitask stuff factor into some of the work you guys have already done to create your emotion maps over at Hume? Because I, you often, whenever you explain them to me, it sounds like you guys are sort of taking all these different facets into consideration. Is this an expansion of what you guys are already doing? Or does that make sense? Or did I connect two dots that weren't supposed to be connected? No, that's an expansion. And our data is well suited to this because we get experimental data from multiple countries with different people essentially forming the same expressions and labeling them themselves. And so there's an ability to actually disassociate these different attributes. And it's interesting from more of a processing perspective, almost a neuroscience or a psychological perspective, whether these things, learning these things together produces more efficiency or value than just learning them separately. Because your brain is taking sounds that are actually correlated, like people who are younger or children or teenagers having a... Even though we don't have those in our dataset, having a higher pitch voice, women versus men, for example, and also relying on pitch to do other things like distinguish the emotionality of different vocal bursts. And so being able to encode them jointly might actually allow for the brain to process these things more efficiently. And so that's something we can test in an artificial neural network.
Super cool, super cool. Thank you for that. All right, we just a moment not too long ago, we were talking about the sound of laughter project and generating laughter. And so I'm going to say it's probably not a terrible idea to ask Gautier and Corey, if you guys can help me sort of at my head around the generate sub challenge and sort of the exciting things within this sub challenge, what we're hoping to achieve what you're looking for. Yes, absolutely. I'll go quick and say like the hardest part about working with like these computational models is generating laughter. It's the hardest part of comedy to write, like, generating laughter, like actually making people laugh is the hard part. And laughter is just one expression that you want to sort of like, make happen to elicit, but it's not the only one. And so in our previous work, we worked with a data set that was maybe 20 times smaller than the data set that we have here. Maybe a little bit less, and it was the best that was available at the time. So we're super excited at the quality, scale, size, diversity of the data that's here, because it doesn't just cover laughter, it covers a whole bunch of different styles. And then, you know, on a bit more specific stuff or emotions, maybe I'll throw to Gautier, you can share some more details. Yeah, so what's really interesting about generating laughter, and I think vocal bursts in general is, is the fact that it has never been done before at scale. So the previous project, I think it's from what I remember the first project that tried to do this. And what's really interesting about, about this is that, as Alan mentioned, you don't have to generate some semantics. So when you want to, you know, there is a lot of like voice generation, you know, but that tries to generate voice, but it's a specific task where you're asking, generate a voice that's gonna read this text for me. So you're asking something very specific. Here, we're asking something different. It's like just generate a laughter, and they give you no condition, no constraints, just generates me generates laughter or generate horror. And so this is when you think about it.
Even for a human, this is hard. Like, if I'm asking you to generate, like, generate honest, you know, like, like true laughter or generate what is horror, you have to think about it. Like, what is horror for me? What? Okay. Okay. How can I convey this emotion? Like, we are paying actors to do this because this is hard. And, uh, so this is why this task is amazing. Uh, and like, uh, you know, from just an AI perspective, but also from a human perspective, try to understand what, you know, try to deconstruct how a machine can learn this, this generation, this emotions. And I get into these ideas through the laughter project. So I was like, oh, laughter is amazing because the task is so well-defined and this is so hard to like generate the whole diversity of laughter. And then I met Alan and I was like, oh, it's not about laughter. It's about vocal bursts. It's even more diverse than laughter. Like this is about emotion. This is what's interesting. So, yeah, I'm very excited with this challenge because one of the main, uh, bottleneck and one of the main thing that was preventing the community to like move forward for this task was the access of, of data. As Corey mentioned, uh, you know, uh, the big data challenges were always the thing that was like kind of, uh, accelerating progress, like really, uh, it was the necessary step, you know, like releasing this large amount of data and tell people, okay, guys, you have to give your best and like try to solve this task. And so we are at this point where we can, I mean, like you can provide the data and that's awesome to the community. And, uh, yeah, if you look at lots of tasks, like image recognition, uh, translation, um, I had other tasks in mind, but like lots of big, uh, like, uh, movie suggestions, all the big, uh, challenges in, let's say artificial intelligence, like a huge bottleneck for the progress was the release of the data, like the release of the data, making this like publicly available. And once like such like datasets has been like, uh, made publicly available progress was huge because the whole community, it's not about like one group, one people.
Even one group, it's like the whole community that was working on it and tried to improve the benchmarks. That's very exciting. Sounds very exciting. What a brilliant and obvious, and that seems obvious, but I wasn't thinking of it before you said it, that yeah, this is hard to do, even for humans, much less machines, to genuinely do this sort of thing and generate these vocal bursts if they're not organically created or stimulated in that way. I get it. I'm on board, but I have to ask the question, if, let's say, a best case scenario, we figure out how to do this perfectly, what are some of the use cases where we could employ this beyond the terror theater, where they're laughing at each other, which I love, obviously, but slightly beyond that. From my experience so far with vocal bursts, since Alan never explained them to me, and I've understood what they are, is I've only processed them as an observational thing. I can use them as more information for me to process and understand someone's state. I don't know necessarily off the top of my head what the great benefit would be to being able to organically generate them. That's not to say that it doesn't exist. I'm just saying I'd love to hear what they are, what the use cases would be once we nail this. Yeah, absolutely. I think that a lot of the work that I do is in the shape of building tools for people to tell stories that they otherwise wouldn't be able to tell, or to inspire, or to augment, or to allow creativity to run free. So, we can generate text, we can generate images, we can generate audio. It's a constellation of tools that is going to empower the next generation of creatives. And I kind of hope that they're more creative than we can be, and they have ideas. When you see your niece or nephew that's 3 years old that knows how to use an iPad already, imagine what they're going to be able to do when they have a soundboard that can generate a cartoon that's completely personalized. So, that's very exciting to me that we're unlocking the key tech that's happening behind this generation. Over and above that, I think that there's a lot of opportunity to build the flywheel now. So...
A lot of what you're seeing now is being able to generate text, being able to generate images, and then those images are being shared. Those are inspiring other people that otherwise wouldn't have the technical chops, but are now being inspired to gain those technical chops, which is easier than ever. So they're now getting better at developing these things. So we're building a community of creators that are helping to build the tools that are going to further enable the community, if that makes sense. So, yeah, two levels of enabling and uplifting that's going on. I love it. That's amazing. Thank you. All right, unless anyone wants to add on to that, let's go on to challenge number three, sub challenge number three, the few shot panels process of elimination means I'm coming to you, but so talk to me a little bit. Alan kind of helped me understand it a little bit better. But for whatever reason, this was the one that I was having trouble wrapping my head around, but I think I kind of get it. But can you explain a little bit more about the few shot sub challenge and the significance of it and what you find exciting about that? Yeah, sure. So this is a personalized task. So meaning that we want to personalize so that the algorithm, the model can learn more of the person's voice characteristics, okay, such as a pitch, for example, as Alan said earlier, and we want to find the model that is more, more, more, let's say, find some personal characteristics of the human. And we do this by giving the model, two samples from the same person. So we're trying to predict the emotional prediction of the third sample. So yeah, this, by this, we are trying basically, to shift a bit the model towards that person's voice characteristics and the response. Okay, I think you just correct the code for me here. So this is this shot in particular, unless I got this completely wrong. It's the difference between we know, is surprised for everyone. And we know, is me being surprised versus Alan being surprised? Yes, exactly.
And actually, let me add, Matt, so if you, if you, if you've got, I think this is something that iPhones do still, where you'll, you know, you're setting up your iPhone and it says, okay, say this phrase, or say this phrase, that's actually it doing few shot learning to be able to understand your voice better. And so this is the same thing here, but it's for understanding people's emotional expressions, which are actually arguably more, you know, variable across different people. Got it. Very cool. That was fun. It's like, when you remember the name of a song that you can't get it to your tongue. That was great. Thank you for that. All right. Well, that covers the core bits and pieces of the challenge, the sub challenges, and we've covered a ton of ground so far. You know, who, who are you hoping to reach out there? You know, we're sending this out into the universe, we're putting the call out there. It's out there. Who are you hoping hears it and answers the call? And once we answer that, how do they participate? That's where we're at. So who's the ideal person and how do they get involved? Yeah, I mean, we want to see participation from a pretty broad array of researchers. So this is an area where you have the opportunity to collaborate across huge disciplines that normally don't collaborate very much, even, you know, first of all, within machine learning, because you can look at a whole new test like generating and recognizing vocal bursts from, you know, they're just, they're kind of complimentary, but typically, you don't see them in the same challenge. And then across different disciplines, like psychology, like, you know, there's computational linguistics, paralinguistics, there's regular linguistics, all these different areas that are going to be really interested in what we're doing here. And I hope to see people take models that have been trained in other domains and apply them for the first time to vocal bursts, because there's just, it's just basically like a treasure trove. It just hasn't been explored much. And there's just so much opportunity for different teams to get involved. Very cool. I saw also on the bottom of the one sheet, Expo, Expo, or you say they're not all caps. I assume it's Expo, not Expo. I've never heard it out loud. We get to decide. Do we like Expo?
I'm going to go expo for now until told otherwise. Expo it is. I saw Expo is accepting contributions outside of the competition from related fields of research. What's, what's that all about? What's that for, uh, Alan or anyone? Yeah. So we're having talks from, you know, really leading emotion scientists like Dr. Kellner has been on the show. Um, I know that guy. Yeah. Um, and leading, uh, computational paralinguists like, uh, Bjorn Schuller, uh, you know, we have, uh, the involvement of people are from different communities already. So there's an opportunity to discuss papers in this domain where you're bridging to these disciplines for the first time. And so that's what we're looking for with papers submissions. We're accepting papers on a wide range of topics, uh, applying computational methods to understand vocal emotion. Um, and we expect to see some interesting contributions. Very cool. Awesome. All right. Uh, we're coming in the homestretch here. Something I've enjoyed doing on this most recent season of our show, the Feelings Lab is taking a look at the not too distant future, talking about advancements and changes. You're most excited to see blue sky, perfect world kind of thing. I mentioned at the beginning of our chat, competitions and workshops like these are often a great way to stoke the flames, accelerate progress. Let's go around the room one last time. What are you most hopeful for? What would be the most exciting outcome to see on the other side of this in a couple of months? What do we think? Who wants to go first? Yeah, I can start and just sort of say, um, I mean, they're modeling audio is difficult and it's difficult because there's a lot of information packed into a single second. There's like tens of thousands of samples, single individual predictions that need to be made every single second. And that makes it really hard to characterize and model. So you have to use like the newest and most innovative kinds of techniques in a neural networks and machine learning. Um, that isn't likely to be how it will always be. Presumably there will be methods that figure out how to model and characterize these sorts of challenging snippets of audio. And that's going to enable us to do even harder multimodal stuff in the future. And so I really hope that this kind of dataset is like exciting and inspiring to people to say, Oh, maybe we could do this. And then maybe we could make this innovation so that we don't need.
As big of a model, we could get by with this number of parameters, but, you know, transform the data in this way so that we can structure it and understand what's happening underneath it. And by kind of shaping the data in a way that inspires those kinds of creative associations, I see a pathway to innovation that will then, you know, allow us to do exciting multi-mode audio, video, text, and everything at once. Awesome. Very cool. Alice, what do you think? What would be the most exciting thing on the other side of this workshop and competition to come out of it? What's, what's the most hopeful thing you got? What do you want to say? I think like how Corey said, audio is, uh, is often left behind as a modality. I think we have a lot of work in computer vision and even NLP, and the reason is as Corey said, that it's much more difficult to, uh, to do and work with because it's a lot more samples you're working with and it's time dependent and collecting data in that area is much more difficult, um, because of the time dependency and also the ethics behind recording people. So, um, yeah, I'm excited to see how people are going to use the data and, uh, see what, uh, people come up with and just start the conversation, I guess, a bit more and collaborate with people. I think that's a really exciting thing. And the discussion at the workshop as well will be a nice, it's a full day event. So, uh, we'll be able to discuss with people in the same space as us, which is going to be really cool. That sounds really, really cool. For sure. Thank you. Uh, Panos, Gauthier, we're down to you guys. Uh, whoever's got it first. What's, what do you think? What are you excited about? Something I'm very excited about is the fact that audio in some sense, audio data is a bit less sexy for a scientific paper than image or, um, textual data. And for the main, one of the main reason is that it's actually impossible to visualize this data. Like in the sense that, uh, you know, how do you put in a paper, how do you put in a paper, your data? Like, you know, you can show images, you can show pieces of texts. How do you put audio in, in, in, in, in the paper? And so it makes, you know, the, kind of the audio thing a bit less accessible. And so for me, the blue sky would be.
It's a sort of like a revolution where the format of publication makes these data formats more accessible. So like, for instance, where we could allow to put audio sample within the paper for people to listen to the data, because we are doing this a lot, data scientists, we are looking at the data, we are looking at the images, we figure out like how we clean it, etc. There is this like iterative process all the time, like going back and forth, like data modeling, etc. If it's harder to visualize the data, it's harder to do the modeling too. And so like the blue sky would be a world where like the audio data is as accessible as the visual data. Yeah. I love it. That's great. Thank you. Thank you for that. Panos, best case scenario coming out of this competition and workshop, what do you want to see, man? I know it's a tough position. Everyone's already given their answer. So what do you got? Yeah, so I think this is the start of something great, having this data set and this many data and for vocal burst and the array of emotions that we study. After the competition and in the workshop, I expect to see a few studies for the generation that push the boundaries of generating vocal bursts and, of course, modeling vocal bursts. But even after the workshop, after all this is done, I hope that researchers will study more vocal bursts and using this data, they can have so many possibilities and go forward in so many different domains. I mean, these are three sub-challenges that we have in the competitions, but there are a lot more that can be done with this data. Amazing. Thank you. Love it. I appreciate all you guys. Alan, unless you want to add anything, I was just going to throw some dates and some last information and get out of here. Did you have anything you wanted to add before we wrap it up? I love that. I just want to say also, in terms of future directions, this is a whole channel of communication that is not, is basically ignored and is completely complementary to language.
I hope that once some of these problems are solved in machine learning, we start to see application areas open up, and this could really help people. So that's what I'm looking for. I love it, man. Well, thank you, everybody. All right. Last thing, I got some dates here. As of April 1st, 2022, the challenge is open. The data set is available. I'm also seeing here people have to have predictions and paper submissions end by May 20th, and the ICML workshop is scheduled to take place this coming July. We'll throw a bunch of handy links into all the posts that go out there to make this easier. But if you're listening or watching and you want more information, head on over to competitions.hume.ai. That's where I got a lot of my info today, competitions.hume.ai. Any other places we want to plug before we say goodbye? Nope. All right. There we go. Thanks again to our panel today. Honestly, it's been a real treat getting to chat with all of you, and I cannot express my appreciation enough. Thank you. Thanks to those listening and watching out there. We can't wait to see what I'm sure will be some incredible work still to come from all of you. On behalf of all of us here and the team at the Feelings Lab, I'm Matt Forte. Thanks again, everybody. Stay safe out there.
