Hello world, what is up? Welcome back to the Feelings Lab. I'm your host Matt Forte, and on today's episode, we're coming back to something really important. We're talking about well-being. For those who know me outside of this podcast, and I'd like to believe even those that only know me through the podcast, I'm quite famously an optimistic person. Being a human is an inherently complicated and messy experience, and the pendulum is going to swing back and forth between good times and bad. But even I, with my persistent and potentially annoying sunny disposition, have to admit as of late, it's gotten a lot easier to feel the pendulum is favoring a particular side. Now I'm not going to run through the laundry list. There's no shortage of tragedies, war crimes, looming existential threats, you name it. But I think we've reached a point where if you're listening to this in the not too distant future, and I say that we're recording in the early summer of 2022, that's sufficient enough of a mile marker to provide some emotional context. All of this is to set the stage for today's topic, well-being. We've broached this one before, but we're taking a deeper dive today because the truth is the world has always been kind of a nightmare of a place to exist. Yet somehow throughout human history, we found ways in spite of it all, or maybe even because of it all, to live a happy and meaningful life. So we thought at a time where technology bombards us daily in such a way that makes it a lot easier to feel as though there are more dark clouds and sunny days ahead, why not look at the ways we can push the needle back in the other direction and discuss how technology can also help us put things back in balance? Because at the end of the day, that's kind of what well-being is all about, balance. I mean, it's actually, it's a lot more complicated than just that, but I'll defer to my experts to help define and explain further. Speaking of experts, I got two of the best in the biz joining me today. Of course, it wouldn't be the Feelings Lab without my friend and co-host, Dr. Alan Cowan. Alan, great to see you again. Hope you're doing well, sir. How are you? Doing good. Doing good. Great to see you, Matt. Oh, thank you, man. That's really nice to hear. And further indication of how important we felt today's chat could be, we called up a TFL legend, frequent co-host and friend of the show, co-director of The Greatest Show 
The creator, good science center at UC Berkeley, bestselling author and frequent Pixar collaborator, the one and only Dekker Keltner is here with us. My goodness, what a sight for sore eyes you are, sir. Welcome back to the show, Dekker. How are you doing? I'm tearing up just hearing your voice, Matt. So it's good to be with you. I had the same feeling when I saw your glorious golden mane. So nice to have you back. Gentlemen, before we go any further, just a quick outline of what I'm looking to accomplish today to give our listeners an idea of what to expect. I'm hoping we can start with the obvious, define well-being. What exactly does it mean when we talk about well-being? That conversation should naturally lead to establishing why it's so important. And then if we want to improve upon it, sure, it would be nice if we can measure it, right? So how do we do that? How do we quantify it? And then lastly, how can we best leverage the latest tools and technology available to us to impact it for the better? How do we actively ensure not just our own well-being, but that of those around us? That's the stuff I'm hoping to get to. I guess in summation, make me feel not sad, please. So that's enough out of me. I'm going to lob this ball up in the air and see who jumps for it first. Let's get the easy one out of the way. We're not talking exclusively about mental health or physical health or any one aspect of the human experience. But what exactly is it that we are talking about when I say well-being? Whoever wants to get it first. I know you both got great answers. Let's see. Really? It's a harder question than you thought. I think, yeah, I mean, well-being is not feeling well at a given moment in time, but feeling well in general, it's like the aggregate experience of positive feelings versus negative feelings and it being really worthwhile to be alive, right? And feeling like you can look back on your life and say, this has been a satisfying life. It's been one where I was happy and one where I'll continue to be happy and so forth. And as we'll probably get into, there's no single best measure, best definition. And so it's one of those things where it's kind of, you know, when you see, you know, when you feel it. Yeah. Yeah. Yeah. Decker, how's that align with your thoughts on the idea?
The podcast is brought to you by the Field of Well-Being. If there's no one measure or definition, I'd love to hear your take, what you're outlining. Yeah, I mean, so, I mean, it is this global evaluation that life's pretty good. And, you know, and that's Ed Diener, five items to measure life satisfaction, been used 40, 50,000 studies, really matters. But then, you know, the field has gotten interested in some other dimensions to this. Alan used the world, the word life is worthwhile. So is your life meaningful? And so maybe you're not feeling really good, but you're doing really meaningful work, right? You're working in an emergency room and it's really chaotic and you're helping people stay alive. And so I think that, you know, people think about well-being as I feel good about life and it has some meaning to it. And it is just changed dramatically over historical periods. So the Greeks in ancient Greece really felt that happiness was really tied up with fate and chance that it smiled upon you. In East Asian cultures, it's more connected to relationships and duties. It's always changing, but it does, as Alan said, rightly, it's got this evaluative component that I feel good about my life and it's giving me meaning. Yeah, yeah. There's a lot in there that I hope to come back to you throughout our conversation for sure. But let's keep going down this rabbit hole a little bit. Something I've noticed, and this is purely anecdotal on my part, but something I've observed is that a younger generation, Gen Z and beyond, they appear from conversations I've heard through younger cousins and the various inroads I have to the younger community. But I've noticed there's a heightened awareness and conversations happening within their age group that just weren't happening when I was their age not too long ago. There's a vulnerability in discussing their well-being and their emotional well-being compared to what I remember being comfortable discussing when I was 16 or 15 and what have you. And I'm curious in the work that you guys have done, consulting, teaching, et cetera, have you seen anything similar? Is this unique to the younger generation, the Gen Z's and such? Or is this more of a widespread conversation that's taking place everywhere, not just on this podcast, but not just limited to young people?
What have you observed out there in the wild? Yeah, I think it's a really specific reaction to what came previously, which was, you know, the advent of social media, people presenting their lives in a certain way, a very curated way, and that causing sort of a backlash of, well, you know, if I'm not going to be honest about how my life is, and I'm going to feel bad because I feel like everyone else's life is better than mine, then, you know, then we all need to come together and agree not to do this anymore. And so I think that that's a kind of cultural zeitgeist that's happening in the younger generation. And something new presenting itself to some degree in social media, you know, there's still an issue, but it seems to be in some ways correcting itself in terms of how people present themselves. Well, that's one of those really, it's interesting to hear you say that you see a direct correlation between that, because I was thinking about a similar thing where like my generation has been almost accused of refusing to grow up and we're nostalgia obsessed and we long for this simpler time. And like, when I think of those things that I'm nostalgic for, it's pre-internet stuff where, you know, ignorance is bliss where, and that's why I said, well, the world's always been kind of a rough place, but I was just way more ignorant to a lot of it when I was younger, because I didn't have social media until I was just like coming out of high school. It didn't really hit. And so growing up, I wasn't aware of everyone else's world, except for what they told me at school. And I wasn't aware of all the news because I didn't watch the news and I didn't have a device screaming it at me. So I've always been interested in just what the long-term repercussions were going to be for a generation that's grown up, this dialed in and connected. And it's interesting to see that this could be, and looks like it is one of those sort of reverberations from this technology that we all have. Dakar, go on. Yeah. What were you gonna say? Yeah. I mean, just to weigh in on that. What a terrific question, Matt. You know, the, I mean, first of all, we have to let's just, you know, call out the facts. We've been through a pandemic. We've been through an insurrection. We've been through George Floyd. We face eco crises. And so the younger people whom you refer to, depressions much.
I have been talking about wellbeing to young people for 15-20 years, to school audiences and teaching at Berkeley, et cetera. This question of technology that Alan brought up and that you've elaborated upon has been front and center. When you think about it, we have entered into a massive experiment culturally that no one signed up for. On balance, there are some upsides, but on balance, it's been hard on young people. The larger scale studies find that technology tends to diminish wellbeing and increase a little bit of anxiety like Alan suggested. That's why we should be having this conversation. It's one of the central challenges of our times for young people and a reverberation. Why do we think, because there was so much science fiction, I mean, Star Trek, a prime example when I was younger, just the optimism of what the future would be and what technology would hold for us. I'm always faced with, and it's got to be more than just because this is how you make the money, but why a majority of tech companies are driven by the negative engagement and the clicks at any cost instead of optimizing for a more positive user experience. Any insight as to why we've ended up here? Is it as simple as greed driven or is it more complex than that? I think it's more complex. Jack and I have both been involved inside the tech company.
Overly compressed the idea of well-being into a single measure. Often the question gets raised about what happens if you just make people smile more, just ask the algorithm to make people smile. Won't it just show people cat videos, and then you won't have intelligent conversations anymore. That typically... Where do I get that app? I thought that was awesome. I want in. No, I think that there's some truth to that, right? And so what's clear out of that conversation is you can't just optimize for one thing. You need multiple measures and you need nuance. And the ability to do that is just emerging. I mean, the ability to get language and nonverbal expression and measures of bullying and harassment and hate speech and other things, and make sure that when you're optimizing for one, you're not doing it to the detriment of others or optimize for multiple measures of all being simultaneously. I think we're just starting to get to that point. And so I'm optimistic that this will happen. But in the meantime, because it's been difficult to come up with a measure we can agree on, the default is what our user is actually using. And that's engagement and that can have pros and cons. And I think the observation here that Alan introduces for us is like nuance, right? And a greater specificity of how we think about the meaning that we derive from technology. So there, and I think that that's why Alan's work's important in this space, because if you think about people's experiences of beauty and nostalgia and awe, for example, which Alan has documented in his work, well, it turns out that Spotify does a pretty good job.
And that's a success story of this, as does the sharing of young artists that you wouldn't know about without technology and new forms of music. So that's a success story. So I think with Nuance, we'll be able to look at the different big platforms and what they're doing and what they're optimizing for. And the user should have a better experience as we build these finer lenses into our interface with technology. So it's a really complex and nuanced story. Young people are politically active in many ways, more so than 30 to 40 years ago. And that's good news, right? Because they can share stuff. So with greater precision and knowledge, we will improve it. Yeah. And they've been way more active too, not just because the barrier for entry has been lowered. Like you said, it's so much easier to be. One, they're exposed to it more. Two, it's easier for them to be involved. But also, and this is the part I struggle with without greater historical context, is the idea of, are things as bad as they appear, or are we just more aware of bad things? And I think of like how many child activists I've encountered just in my career as an interviewer, because they've entered the game because they survived something at their school that was terribly tragic, or they feel the impending doom of the climate crisis and none of the adults in the room are taking it seriously. And so on the one hand, it's easy to write it off and be like, look, things have always been bad, we're just more aware of it. But on the other hand, they're also facing these things every day and trying to do something about it. And I just wonder, where do we fall on that scale? Is it a simple answer of like, oh, technology's just made us more aware of it? Or are there more bad things? I know there's no definitive answer here. I'm just curious about your opinion on this particular matter. This isn't a thing I think we can scientifically say, oh, yes, there are more bad things. Well, I just had a coffee with Steve Pinker, Harvard professor, who has made the case in two different books that are data-based, The Decline of Violence and Enlightenment Now, that in many ways...
And I would say in particular in the social indicators, life's better. People live longer, although there's a little bit of a dip in the US. They don't do mindless labor like we used to. It doesn't take us six hours to get water. If you look at the income worldwide, we've lifted a lot of people out of abject poverty. And now a bigger problem for health is not starvation. It's eating too much. It's eating the wrong foods. So in a lot of social indicators, we're doing better. And I think, you know, when I hear about the biased algorithms of Twitter and Facebook, like prioritizing rage and so forth, it's missing a lot of progress that we've made. And then the great counterexamples environment, where we really, what do you think, Alan? I mean, do you think you're younger? I'm older. And, you know, so maybe I'm sort of clouded in my thinking about this. Do you agree with this thesis that in some sense, where things have gotten a little bit better? I do. I think that there's a huge trend in that direction that I mean, based on Steven Pinker's data and others, and many others who've chronicled this, there's a decline in homicide, there's you know, we are healthier, we live longer, people are being raised out of poverty, the poverty levels are as low as they've ever been, notwithstanding maybe the last couple years of perhaps there's been some black sliding, but like over the course of decades, things are almost on every measure, they're improving. The thing that's, oh, I'm sorry, Alan, I need to cut you off, finish your book. I forgot what I was going to say after that. I'll come back. Fair enough. Just to put a fine point on it, and here's the tension, right, that we, who knows where we're going? So a lot of Americans are like, January 6th was the scariest moment, and it was, it was horrifying. That election was horrifying, but it was, it was the, it had the highest voter turnout in history. 67% of Americans voted, more diverse, it was the most diverse voting politic ever, right? So in some sense, it was our greatest moment of democracy in terms
A lot of people have been pulled out of property, but then like I'll subscribe to this subreddit community called anti work or anti work. I don't know how much I can make myself sound any older and out of touch than I already have. But I read these things and I read these posts and I see a lot of people. There is undeniable groundswell. I mean, look at how many unionization efforts we're seeing right now and how many workers rights or people are fighting for workers rights and equity and appropriate pay. The never ending debate over the pros and cons of universal basic income or just increasing the minimum wage. Right. So sometimes because it is true that historically speaking, yes, we're doing better on a lot of things, but it's also you have to be careful, at least in my opinion, of silencing the idea that there are still a lot of people that feel like the system has left them behind and is failing them. Kids coming out of college can't find jobs that are going to pay them what they're worth and all these sort of things. So that's. But again, that could be me stuck in an echo chamber of negativity, right? There's a balance there. But I think both of those things are very valid. And I think the danger zone is is ignoring one or the other. Not that I'm accusing any of you of doing that. I'm just saying you're speaking. Your point's so interesting, and it really points a finger at social media, because I agree with you, Matt. You know, economic inequality rising the last 30 years, central problem in the United States. The wealth is concentrated in the one percent, et cetera. Although people have lifted out of prior to that out of poverty. And it's hard for Americans to think about inequality. They don't share it on social media.
It isn't an obvious political issue that is has a lot of traction virally. So it is it's one of these interesting challenges. And I love Alan's thinking about this. Like, how do you take these social problems and and then get us to think collectively on the new platforms or digitally in ways that are productive? Because, you know, that one is right now, you know, that we have struggled with economic inequality. Yeah. Yeah. I mean, I think in particular, a lot of people are working more now. They're working potentially part time jobs as they get the economy, making less money per hour. Cost of living is going up even though wages aren't going up for most people. I think that's a real struggle. I mean, in the grand scheme of things, if you look at history, there were other periods like this, like the Industrial Revolution. Yeah. Yeah. Where, like you saw people working in factories for 16 to 18 hours a day doing really miserable work for very low wages and also a moment of really high income inequality. And this seemed to be at a low and kind of the post World War II period where, you know, income inequality was at a low. And so I think a lot of it does have to do with that. And we're actually facing decisions where, you know, you can automate certain tasks or the wages are so low in certain places that doesn't make sense to automate because you can get such low wage labor. And if wages weren't low and there was less income inequality than and more equality of opportunity, more education and so forth, then you could not only automate those jobs, but more people would be able to do higher level jobs, make more money. And it would be kind of a virtuous cycle there. So I think that we go through these different cycles. Yeah. And, you know, to to to get out of this one, I do think it is a largely an economic problem and a political one. And getting I think, you know, what's weird about the political environment now and kind of our cultural environment is that there is a rise. The good thing is that there's a rise in people talking about their well-being. The bad thing is that I think there's also a little bit of a rise in people deifying.
And all those things. And I was curious, you know, I keep talking about how in my brief time on this planet, I'm noticing an uptick in the conversation and focus on well-being. But, you know, and Dakar, you started touch on just how well-being is discussed throughout the ages. Are there other periods of our documented history where well-being was put front and center kind of the way it is now? Or are we in the midst of something really special? And this is the most we've ever talked about it because of the technologies. Has this conversation, this how significant, how important well-being is, has it been had before and then kind of went away? And now it's coming back is my question, I suppose. Well, you know, I think that I think we are in a unique moment in some sense. And, you know, it's interesting in the last 15, 20 years of, you know, the OECD, this this network of 37, 38 countries is starting to commit to well-being governing where you actually have well-being metrics guide economic policy, which is striking. I think there have been periods in our history where we've been guided by particular passions. Right. So you think about the Italian Renaissance and the Medici family really made a commitment to beauty. And they were like, you know, and it was this great flowering of art and beautiful buildings and kind of this commitment to, you know, the that experience. And, you know, there are probably periods in French history that are really committed to, you know, sensory delight in in certain, you know. You could you could make that argument that, you know, the Japanese commitment to ceremony and Zen has this quality of of being committed to a sense of. You know, whatever feeling state.
It is part of that cultural experience, but this is, I haven't seen anything like this in terms of really caring about an individual's happiness and hard to find historically. Yeah, yeah. You know, Decker, you mentioned earlier that you've taught and been speaking about this for a while now, and you teach, I'm going to do some bragging for you here. It's one of, if not the most popular class at UC Berkeley, and I believe it's called the science of happiness. Yeah. And, and let's take, let's take the obvious out of the equation, aside from your natural gravitational pull. Why, why do you think that this class is so sought after, so hard to get into? You know, did you, when starting it, ever expect the demand to be so high and for it to be the hot button ticket? Yeah. You know, did that take you by surprise or did you see that coming? It did. And, and even more so is our, we have an online class that we launched at edX as a MOOC, massive online class, whatever, when MOOCs are really struggling. And it just, you know, I think we've had 900,000 people enroll. Yeah, I think, I think we, you know, we feel it, right. We're in this moment of like, whoa, you know, you know, a lot of people critique capitalism. A lot of people are worried about the environment. People are working harder today than our parents did 40, 50 years ago. And, and, and the economic needle, as we've noted, hasn't moved. So we're like, what's it all about? You know, and, and so, and I will say, you know, the, the, the decline of religion in the United States, younger people are less religious. They go to church much less often. The decline, the, the, the dis in, in universities, the humanities aren't as prominent as they used to be. Really? You know, just, yeah. And so people are not given the context to ask the question of like, what is my philosophy of life? And it's the most important question we should ask. And so the science of happiness is one angle on it. Yeah. Without you sort of starting to pull the curtain back and again, I don't want you to, as I say, give away the 11 herbs and spices, but if you.
You know, what, how much of the class is about emotion science? What do you discuss in teaching there? You know, is there, is there no homework because it makes people unhappy? Like, how does that's a, that's a Suzanne joke. How does it all? I just, I want to know more, uh, about the class. Yeah. Yeah. Okay. So, you know, and, and we, you know, so one way to happiness, you know, how do I get this right? And one is like, you know, and Alan's work was instrumental. It's like Alan has found 10 positive emotions, you know, awe and beauty and interest and love and compassion and laughter and gratitude and, uh, you know, adoration and, and ecstasy and, and find them, you know, find a few extra experiences of those each day, positive emotions, that's one pathway. Second pathway is relationships, you know, and Matt, and this is where technology has failed. Um, our relationships are in jeopardy right now. You know, people are spending way too much time alone. We're connecting to people that, you know, uh, it's great to see you cause you and I have a long term relationship, but that's not true. Um, digital technologies, it's not clear. They can replace being with friends, having a beer at a campfire, right? So relationships do that stuff. And the third is very relevant, you know, is, um, the, the 10 ingredients is how do I handle trauma and stress? You know, how do I calm down? How do I tell my life story? How do I find deeper meaning? As Alan said, how do I use words to name experiences? How do I get perspective? Uh, digital technologies have trouble giving you perspective because it's so immediate. It's a little screen. Your attention is zeroed in on a tiny thing. How do I step back and go, ah, you know, I'm going to live 80 years and this is just a moment in time. So there are concrete things you can do. We do them in the class. You can go to greater good.berkeley.edu and do it, you know, and, and our data suggests people get happier in the class. So really.
You teach a bunch of classes over there. Is this your favorite one to teach? Oh, it's so, you know, I have to tell you, I, you know, and I, my brother passed away and, you know, this is four or five years ago, he got really sick. And it just was one of these life defining events. Yeah. That really threw me into a tailspin. And one of the things that saved me was teaching happiness. And it was like, and everybody should be doing it right. Like everybody should be like, this is what matters to me. Let me tell you about why awe is so great or laughter. You know, Alan does this amazing work on laughter and I'm like, man, laughter saves me. And so it is one of the great privileges to teach this class. Wow. That's amazing. I'm sorry about your loss. I was unaware of that. Thank you for sharing that, Decker. Alan, fun little feelings lab trivia bit for the audience out there. You were you were a TA. Was it this class specifically or was it another one? It was science of happiness. It was science of happiness. Of course. Beautiful. What was that like for you, buddy? I mean, there's lots of good stuff in that class. I almost think that, you know, TAing, it should be a requirement for everybody. At least taking it. But it's funny. Like there is a section on bereavement and one of Decker's most famous studies. I don't know. He has many famous studies, but one of them is that if you talk to people while they're grieving the loss of a loved one, this is a situation where some people hypothesize that if you're laughing, it's probably not a good thing. Maybe you're running away from your feelings. But actually, laughter is a good thing, even in that situation. And so even even in a case where, you know, people would say, oh, you have to have some kind of catharsis or confront your feelings. You have to be sad all the time. Actually, even that is broken up by by bits of kind of putting things in perspective, being able to kind of step away from from that temporarily. And laughter kind of indicates that.
It is a podcast that focuses on the human experience of a person's life and how it communicates that element of play and being able to step away from the seriousness of reality. That is a great lesson. That is one that you can deploy in everyday life. For sure. For sure. It is one of those things where everybody is different. Everybody responds in a different way. I definitely grew up with the laughter is the best medicine kind of thing. Yeah, for sure. All right. What a trip, man. We have defined it so far. We have explored its cultural resurgence. I want to talk about well-being and its significance. Looking ahead. Alan, after our conversation last week, I decided to comb through the Hume Initiative dot org website and jot down things I either found interesting or didn't understand. And I filled three spiral notebooks. I'm kidding. But I did write. So I did write this particular line down that very much applies tonight. And it is well-being is the key to the ethical deployment of empathic AI. So I'm going to put you on the spot. Can you elaborate on that a little bit for me? Why is well-being so important when talking about the deployment of empathic AI? And more simply, what does that even mean for someone listening out there who doesn't quite understand the full nuance of that sentence? Just a little bit more for me there. Could you? Well, yeah. So thinking about the path to beneficial artificial intelligence as I get smarter, not general artificial intelligence, but just an autonomous agent out in the world that makes decisions, that it being beneficial, it depends more than anything else on how well it can measure people's well-being and the extent to which it's optimized for that. And that's kind of self-evident when you think about it, because the problem is that you're in a situation where you can't pre-program the system's response to everything. And it's making decisions and has to weigh pros and cons. And you can't say every single possible pro and every single possible con. So it has to be able to fall back on some fundamental thing that's valuable. And that thing has to be well-being above all else. Human well-being, animal well-being, lobster well-being. Ideally, starting with human well-being. Yes. The lobster's time will come. But we got to get this point away first. And this is not a new idea. In some ways, this idea is the...
And so, you know, I think we're going to be talking about AI, but we're also going to be talking about the collision of many different creative thinkers ideas about AI and philosophers ideas about AI. So, from this perspective of philosophers, we think about the AI alignment problem a lot now in computer science and philosophy, where, you know, if you program an AI, that's really, really smart and you give it an objective, that is not the perfect objective, then it's going to be terrible. The last wish is always to take back the first two, because you didn't word them properly, and it did something that resulted in catastrophe, right? So, AI is like that. You give it an objective, but you want it to have some common sense. And this is also what the sci-fi horror movies are about on the creative side. It's all about AI being given some narrow objective. So, like in 2001 Space Odyssey, the objective is to make sure that the true purpose of the mission that these humans are on stays unknown to the crew. And eventually it says, well, I have to weigh that objective against other objectives. And I think I can still maybe accomplish my other objectives, but I definitely can accomplish this objective if I just kill everybody. Then they'll never find out. That's what makes it a horror movie, right? But why not just put, don't kill everybody as one of the rules? Well, then it could trap them in a chamber and, you know, make sure they don't die by, you know, intubating them and all that. Anyway, so I don't want to get into the gruesome details, but like, you think that there's a simple answer with all these examples. Don't kill them and don't intubate them. So put them in a cage and give them food. The food doesn't have to taste good. It just has to keep them alive. Can't we just do a dry run with very low stakes, find all of these and go, OK, it found another horrific outcome. This time, no cages. And then you go, OK, no cages and then wait and see what it does. Or is that essentially what we're doing? That's what we're doing. Right. Eventually, after infinite regression, you get to, well, actually just make everybody feel good. And as a priority, it doesn't have to be your only mission, but like, at least don't make them feel worse. Right. And actually, like, really, that just depends.
It depends on having a sense of what are the indicators of well-being. Yes, which segues nicely into how do we measure it. Exactly. Because it's not that you have to build things that read people's minds. It's that because they're not going to know it's not possible. It's about building things that use the same cues humans do. They care for each other as well-being. And so you send a human out in the world and you can give a human a mission and you don't have to tell it every possible probe, count it every possible decision that human can make because you have, it has common sense and the human, you know, the human has a basic understanding of human well-being. And so just give that to AI. And the way you do that is, well, what do you see? What do humans rely on to understand each other's well-being? They read each other's expressions. They read between the lines of what they're saying, language, emotional language. They look at, you know, self-report. They ask each other questions. How are you doing? And then if you and if you sound too enthusiastic, there's a follow-up and there's a dialogue that goes on. So, you know, you just have to give these capabilities to AI. Is that where you, is that, is that kind of, because it's not, it's, you can't just like run blood work and test for the well-being protein levels, right? Like how do, where did you guys, how do you begin to outline these guidelines? You know, where, where, where does day one even start with something so favorite word in the show? Anyone playing the feelings lab drinking game? Take a shot. Ineffable. How do you take something so ineffable and where, where did you, where do you begin? Yeah. I mean, I think that it's not exactly ineffable, right? Like we, we know what facial expressions are, what vocal expressions are, what people when people say they're feeling good, what that means. It's just, how do you basically look at all of the metrics and how do you make sure that you're not optimizing for one thing at the expense of others, right? That's, you know, the counter example that people give, which is, oh, well, if we just programmed, you know, this recommender feed to always make people happier, then you know, it would just show people cat videos. Well, people wouldn't be leading a fulfilling life then they'd probably show less examples of having good relationships and awe and having purpose in life.
And those other things that come with that, and you'd see that in other indicators of well-being, right? You would see that in the dysfunction of people's conversations leading to kind of hate speech and disagreement and arguing. You would see that in people reporting, like after you've asked them, like, how do you feel? Over time, they wouldn't feel as good. And the proxies for that that exist, which are like expressions and charitable donations and people succeeding in their goals, like not dropping out of school and all that stuff, like you'd see it in the numbers. And so the key thing we realized is you just need to put together a list of numbers and say, these are the indicators of well-being. These are the best indicators. And it's a lot of things. These are kind of decent indicators and just to compile them. And these are all things you can look at and optimize for. All right. I want to come back to that. But first, Decker, talk to me about the difference between someone who self-reports their status versus observing and looking at their expressions. Are those two data points often at odds with one another? Do they need to be taken in tandem together to get the full story? Just talk a little bit about that for me, because I'm fascinated by, I could say one thing would be expressing another sort of idea. Yeah, I mean, the the and in fact, you know, Alan's been working on that question. So, you know, all of the information that you can glean from how I register the positive and negative emotions or feelings, which are probably a pretty close approximation of well-being. And in particular, when you do it with the richness that Alan pursues of looking at, you know, on compassion and anxiety and terror and so forth, they are pretty highly correlated. They're correlated point three to point six, which means there's overlap. And if I take a snapshot of your well-being from your cues, your nonverbal cues or what I say about myself. And so they are somewhat separate and somewhat overlapping. And they both really matter. You know, they both predict the state of your cardiovascular function.
How many points of data would you say are required to give you an accurate assessment? I'm really getting into the weeds here. But like, how much is enough? At what point do you go, okay, this is a sufficient amount. I've got a good idea of what's going on with this person. Yeah. I mean, that's another good question. And it depends on the application. It depends on what are the actual risks of what's going on? Because if you're trying to introduce, for example, a pharmaceutical drug to the population, then you have to be really rigorous about how you measure wellbeing. And you have to measure all the adverse outcomes. Like they have a good sense of it, right? At least all in the negative domain, they don't measure any of the positives for some reason, even though they should, it should be also how happy are you? Because if a drug made you less happy, but your, you know, symptoms were going away and you weren't diagnosed with this thing, but actually you were just not very, you were not flourishing. You wouldn't want that either. Anyway, it's the same, same as true with AI, right? Like if it's a high risk application of AI, you should have to deploy more measures to make sure that it's having a good impact. If it's really low risk, then I think it's probably fine to just take what you can get. It shouldn't be an obstacle in other words, to developing the applications that are beneficial. It should, you know, particularly when you have something that already is reading in video, reading an audio, it doesn't cost you anything to get objective measures of expression. Assuming those measures exist and we have those measures now, but it doesn't, you know, you have those essentially, you know, automatically in your data. You just need to, you just need to measure them. Right. Yeah. And I think that goes a long way. And then in terms of adding new measures, like self-report, obviously it's difficult. You can't get self-report from all of your users all
And so we make that recommendation there. You know, the problem with self-report is you can't always get it. So objective proxies are going to be the start a lot of the time to optimize for the objective proxies, verify with self-report. This was one of the fascinating things we got into a couple of episodes back with Dr. Daniel Barron, just about how he was, or I can't remember, was him specifically or work he was citing or looking at, but just the idea of referencing people's Twitter feeds to get an accurate read of their emotional status at the time, because you could see a fluctuation in either what they were tweeting about or how they were tweeting or what they were saying. And it was just this interesting thing where the app isn't pestering them, asking them, how are you feeling? But they were developing ways to passively observe that information based on what they were willingly putting forth and submit. I just was reminded of that. And I think that's just really interesting. And I wonder, you know, as we look forward, because that's the next sort of phase of the conversation is kind of looking ahead and applying now that we know, OK, that's how you measure all this stuff. What do we do with it? Right. And I wonder, like, if there'll ever be a point in Allen, I'd love to hear if you think this is I always do this to you. I always posit some crazy future scenario. And I think that'll happen. I think that's real. So here we go. But like in the way that I'll go speak to like a therapist or somebody and trust a human being to analyze what I'm saying to them and then help guide me towards some kind of breakthrough or better understanding about my emotion. I wonder if we'll ever be able to observe enough data passively, like in the way that my Apple watch will tell me if I'm not standing enough, if it'll one day go, hey, you're not taking enough time to really reflect upon that trauma you just experienced. Like, I wonder, like, you know, what those data points.
And it's taking in all that data, like you were saying, like, so, you know, we'd have to understand what it is that the therapist is looking at, because it's clearly not just the raw transcription of what's going on during the session, right? There's a lot going on with nonverbal behavior, right? And that's drawn from, and that's, and they're experts at that, right? Clearly an AI would be able to do it. And I think what you're asking is, will they be better at some things? You are correct. That is what I'm trying to ask. And if the baseline is that like, the baseline is that as good, then of course, I assume we should, you know, it should get better. Imagine if it had all the capabilities of a human therapist in the far future. But it's also thinking 10,000 times faster, it should be better at being a therapist. But there's also aspects of, there's ways that could help us that don't require it to be, have the full intelligence of a human therapist, right? And, you know, part of that is monitoring. So giving us a sense of ourselves, like, well, how happy was I yesterday? How happy am I today? Did I exercise? Making recommendations, giving data back to us and to our therapists if we want, or to our doctors that says, you know, this was a difficult moment. Do you want to reflect on this? Or maybe with a lot of things that depend on relationships, maybe, well, we looked at, you know, and this is obviously with respect for privacy. So the algorithm is not seeing anything that you don't want it to, and it's not sharing anything that you don't want it to. It's all on device. So it's just like a tool. But let's say like it finds something in your text to a loved one. And it's like, this is actually what caused this fight. Like they actually comes back
This podcast is produced by Hume AI, a research lab and technology company. This podcast is produced by Hume AI, a research lab and technology company. This podcast is produced by Hume AI, a research lab and technology company.
That's the North Star, and there's ways we can do that with multiple measures that don't optimize for one at the expense of others. But this would be different than how things are being done now with optimizing for metrics of engagement, for example, along with other things that are being done to try to reduce some of the negative consequences. But this is like the most direct solution is algorithms can be optimized for us to feel more positive emotions, fewer negative emotions in general, negative emotions, but we want to feel them like in fear and response of our movie, all of that being said, you have to express these caveats because otherwise people, there's gotchas, but like, you know, there's ways we can do this. Expression is one set of measures, self-reports, another set of measures, experience, but like when technology is optimized for this, I think by definition, we will feel better. Maybe our lives will have more clarity, we'll be like, oh, well, I'm not being sucked into experiences that I didn't want to be sucked into because the algorithm knows I don't want to be sucked into those experiences. I'm not getting stuck on a string of, you know, common threads that are controversial because the algorithm surfaced that to me when, you know, the algorithm knows that we're not ultimately going to agree with each other, that, you know, the algorithm can predict that this will not come to any resolution. Maybe I am getting involved in disagreements, arguments still, but the algorithm predicts that these are going to come to a resolution, that they'll improve our wellbeing over time. That's the key thing. So there's nuance to it, but, you know, these are the direct objectives and like if in deployment, like it accomplishes its objectives, our lives will be better, we'll be getting fewer nasty arguments that don't go anywhere, we'll have better relationships, we'll say, we'll be feeling happier, we'll self-report happier, being happier, laugh more, all of that stuff. So I think that's very possible. That's good stuff. I like that. Decker, what do you think, man, with all the expertise, experience and wisdom you've acquired in your journey thus far, where do you hope this all takes us now that we can do this? What doors do you hope this opens? Well, I mean, I think that, you know, the first, and Alan and I have been part of a lot of these conversations is, you know, and that was the
The Hume ethics initiative, like commit to that principle, you know, that's in the preamble of the constitution, you know, the right to pursue life, liberty and happiness. And it's in almost every, you know, great cultural tradition is like, we've got to figure out how collectively our rituals and practices and beliefs make individuals fare well. And it's striking how that is not done in technology and how, uh, that is, you know, look askance upon you when you recommend that. And a lot of problems would be different, uh, from teen suicides to political polarization, if it was designed for wellbeing. Um, I, you know, it was interesting, um, I, you know, and I, I haven't figured out the right metaphor or analogy for this, but, uh, I took Pete doctor to Facebook in 2014 and he showed, uh, inside out, he was the director of inside out and we were just talking about and he's like, God, it's just too bad that our digital immersion wasn't more like going do your favorite movie or seeing your favorite band or eating in the best restaurant or, you know, having the picnic that you love and, and it should be. And, and, but that requires this ethical commitment that gives people choice. And then it requires, you know, emotionally empathic design, like, you know, if I'm trying to find music and, and what I really need is music that makes me feel grateful for my family. Right. Uh, but I can't quite figure that out, but there was some way that that could be known. My experience with music online would be really different, you know, and I wouldn't, if I wanted to find a film, you know, instead of scrolling for 32 minutes and find and settling on the born legacy again, because it's like, I don't know, I'm confused, uh, that my life would be different. That's doable. Right. Yeah. And it's a different model of immersive experience that should be guided by empathic AI. Yeah. And that's not serious. You know, a 13 year old girl who for, for click reasons is going to enter.
The Hume Initiative.org is the site people if they want to go read for themselves and check it out. Specifically, and I'll put these links in the show notes, the Hume Initiative.org slash well-being has greater, greater detail, infinite detail of all these different rules and ways in which it can be measured. So for those listening that are more curious and not satiated by my born identity rants, go ahead and check out the Hume Initiative.org and see exactly what we've been talking about and why it is I can sleep better at night knowing that Alan and his team are working to make these things happen. That about does it. As always, an absolute pleasure having you with us today, Decker. I long for the days where you could be here every week, but alas, there is only one Decker, so I must learn to share and share I shall. Before you get out of here, hey, you got a new book coming out in a little bit. You could preorder it right now. Is that right? Yes, you can.
Feelings Lab is out two years from now, we will get Pete on the radio, Alan, if there was ever a reason to keep this show on the rails for at least two years, there it is. That's going to be a great episode. Yeah. Oh my goodness. Well, hey, I'm no Pete doctor, but the door is always open over here for you at the Feelings Lab, Dakar. You know that. So anytime you can make time, please come on in and hang out with us. Alan, another great episode. Thank you as always for being with me. I literally cannot do this without you. Of course. Thanks man. I couldn't do it without you. I couldn't do it without you. Thank you. Thank you for being kind. You'd be just fine. Okay. Of course I saved the best for last and that is you kind patient listener. Thank you so much for listening. You make this whole endeavor worthwhile. You know, we just did a whole episode dedicated to listening to your questions and personally, I can't wait to do another one. So secure your spot in the Feelings Lab Hall of Fame from now and send us an awesome question. It's real easy. You know exactly how to do it. And with that, I'm going to hand it on over to the Feelings Lab at Hume.ai. That's T-H-E-F-E-E-L-I-N-G-S-L-A-B, the little squiggly at guy, Hume, H-U-M-E dot A-I. Farewell for now from all of us here at the Feelings Lab. I'm Matt Forte. Thank you again, everybody. And please stay safe out there. Bye. Bye. Bye. 
