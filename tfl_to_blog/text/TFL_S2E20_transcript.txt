Hello, world. What is up? Welcome back to the Feelings Lab. I'm your host, Matt Forte, and on today's episode, we're talking about the future of AI that talks. Chatbots, digital assistants, video game NPCs, virtual humans, even in all its primitive 90s glory. Yes, Furby. Robots have been trying to talk to us for decades now, or I should say we've been trying to make robots, or rather computers, that can convincingly carry a conversation. My personal fascination with talking robots dates way back to my older brother's Teddy Ruxpin, which of course now, as an adult, I can tell you I'm fully aware that Teddy was just a tiny little robot mouth synchronized with a cassette tape to create the illusion of a living human. Storytelling teddy bear. But back when I was five, Teddy and I were tight. My self-proclaimed oft-mentioned theme park geekdom on this show stems not from a deep love of $23 popcorn and costumed adults. No, though I do enjoy those things, it was the first place I saw real live talking robots. Again, the illusion of consciousness, but still it left a mark. And while I joke about Furby, the convincingly programmed ability to learn language with rumors swirling around the schoolyard that some kids even taught theirs to curse, it introduced us to the idea that speaking to artificial intelligence could be a lot of fun and rewarding, no matter how primitive that intelligence may have been. In my personal experience, it feels as though my generation has been gradually over time introduced to the idea that not only can computers talk back to us, but they're sometimes fun and worth talking to. In my lifetime alone, they've gone from illusion to amusement to utility. Every night in the kitchen, while preparing dinner, I ask my phone to please set a 10-minute timer. And most of the time she does it. Sometimes there's a misunderstanding and we'll argue and I'll forget the pasta or over-roast something, but that's on me. The point is, we are flying down this path. Used to be, each decade, a major leap would come to market, and each time, incremental progress, and it would blow us away. But now it feels like every year we leap.
We're going to leap forward in ways we'd only previously dreamed of. So where are we going? Where does this path lead? What does the future hold? As we develop and improve the capabilities of virtual humans and AI, the use cases explode far beyond hands-free egg timers and little cursing furry alien robots. That's just the beginning. How about democratizing mental health care and providing services to those previously without access, huh? That sounds pretty cool, but how do we do that? As the graphical capabilities make our digital avatars increasingly lifelike, is that a good or a bad thing? You know, you might laugh at the idea of a mental health care provider appearing as some kind of animated character, but I used to say all kinds of stuff to Teddy Ruxpin and he wasn't even listening. Our inclination is to first recreate what we know, right? What's familiar? But as is often the most exciting part of these discussions for me, I'd like to know what's beyond that. The true groundbreaking ways you and I haven't even thought of yet that this technology can impact our lives for the better. All these questions and more await us, my friends, on the other side of my long-winded, self-indulgent introduction. But first, speaking of long-winded and self-indulgent, my co-host is the exact opposite of those things. I genuinely can't do this show. I know, right? What a left turn. Seriously, though, I can't do this show without him. The CEO's with the most, my second favorite person for the next hour, the one and only Dr. Alan Kalanis with me once again. Alan, hello. How are you, sir? Doing good. How are you, Matt? I was a roller coaster of an intro for you. I apologize, but it was fun. It's all good. Yeah, OK. All right. Thank you, Alan. You are the very definition of a good sport. Let's go ahead. Joining Alan and I today, our guest, my first favorite person for the next hour, is a research professor of computer science, psychology and media arts, and practice director for virtual human research at the USC Institute for Creative Technologies. He's also the director of the USC Effective Computing Group. His research is directed toward developing human-like software agents for virtual training environments and to use these computational methods to concretize psychological theories of human behavior. Super excited to have him on the show with us now. The great Jonathan Grouch is here. Jonathan, welcome, sir. How are you doing? I'm doing great, Matt. Thanks for having me on the show.
We're thrilled to have you here. We're going to have a lot of fun. I really appreciate you making the time. Uh, Alan, you see how excited Jonathan was when I introduced him. Can you give me more of that next time? Can we, I need that. You need to like pump me up with insulin or adrenaline. If you want me to, whatever you need, whatever you need to get going. Uh, speaking of let's, let's get going. Let's do this. Let's get the show started here. Enough, Tom foolery. Uh, I want to get a lot of different stuff today. Let's start with some basics. The term virtual human has come up on the show before. Uh, and some people may have different definitions or some people listening may not even know what we mean by a virtual human. So let's start there. Let's get a baseline. Jonathan, how would you define a virtual human? Uh, and in your perspective, how do they differ from other types of AI driven machines, like a, like a chat bot or something like that? Yeah. So usually what we view a virtual human, is it sometimes called an embodied conversational agent? Uh, so it, it has a physical body. Well, uh, what appears to be a physical body, my work by virtual, we use these things in virtual environments, though, uh, robot is also kind of an embodied, um, conversational agent conversation. I mean, he talks to you. Um, and then usually by virtual human people assume it kind of looks like a person as opposed to virtual Furby or, uh, something like that. Um, let's be clear, Jonathan, you are experimenting with virtual Furby's. Is that yeah, well, of course, yes, that's implied. That's okay. I didn't mean to cut you off for a bad joke. I just saw an opportunity and I took it. Um, but thank you. All right. So that, that clarifies a little bit, Alan, that, that aligns with, uh, your expectations and what you've told me before off camera about virtual humans. Right. Totally. I would also, I mean, my definition might be broader, which is anything with a voice to me is also a virtual human of sorts. It just doesn't have a physical body, but it has some, some sort of imagined presence, right? It still is a, it has a voice that sounds human on purpose. Right. Um, and, and in that sense, it's a virtual human, or it could sound like something else, you know, it could sound like an animal or something, but whatever it is, it, it, in order to communicate with us, it needs to
Right. I'm no scientist, but my assumption is if it sounds like an animal at that point, it's likely a virtual animal. It could be a virtual animal. Oh, we call it a virtual agent, right? Like it could be anything. Jonathan, I swear I take a lot of this very seriously at some point. Just hang in there. I swear I will. All the thank you for that, both you gentlemen. And thank you for humoring me. All the things I was referencing up at the top, though, the rock spins, the furbies, the animatronic Lincolns and whatnot, all designed as a specifically to emphasize the illusion of of it. Right. They were creating the illusion of this living creature. It's very much a very rigid, structured performance that we're sort of passively engaging in. But there's a lot of mental gymnastics we did at the time to think I'm I'm interacting with this thing. But no, I'm not. It's it's it's very much programmed to make me believe that it's living. But the thing is, now we're at a point where we're making machines that that actually do those things. They are listening. They are responding. And aside from like the technological leaps of like, oh, the computers are faster and the chips are smaller. What would you say has been the most significant breakthrough in the 20 some odd years that have enabled us to get this far? Which feels like really fast for me. I perceive it as being that we've gotten here quickly. Maybe we haven't. I don't know. I'd love to hear an expert's take on that. Yeah, I mean, from my perspective, I mean, there's two related technologies. One is GPUs, which are graphical processing units which allow very high speed processing on computers originally for graphics, which you need to have beautiful graphics to have something that looks like a person. But then that also enabled machine learning techniques. And so the other big kind of connection and innovation is deep learning. Of course, this also requires massive amounts of data. So those two technologies together with the growth of the availability of lots of data makes it easier to construct these things through learning methods. Jonathan, was that something or something like that when you were when you go back 20 some odd years? I think it was either. Keep me honest. Oh, four or five area where you were developing.
Was that the kind of thing that you were just hoping, you were just waiting for, you were excited for? One day we'll be able to do so much more because we'll have X, Y, and Z. Yeah, no, we, I mean, it kind of came out of the blue, the sort of the amazing power that these learning methods can do. I mean, at that time we were hand crafting stuff. I mean, I was trained as a machine learning person and we used to make fun of the deep networks for not being able to do much of anything. And then all of a sudden, boom, and it revolutionized everyone's thinking. Yeah. Yeah. From an outsider, like layman perspective, all that stuff for years, it was like a big blue could play chess. Like that was it. That's what we knew of supercomputers on the outside. And then all of a sudden it felt like overnight, all of these other fantastical things were happening in the technological space. And like, obviously there's a lot of, you know, data points and milestones between those two things, but that's just like from the mainstream, what we were getting is kind of, it felt like it happened like that. So it did feel, was it a very exciting moment when you realized, oh my gosh, this is, this is going to change everything. Like, when did you realize how important this tool was going to be and that these were a big deal? What convinced you? I don't know that, I mean, cause you're just like in your hole doing your thing. It just dawns on you that the look up and say, it's just like, oh, here's a new, here's a new toy, here's a new toy. And you start plugging and only when people like you ask you a question, you step back and go, oh yeah. Wow. Well, you know what made me think of it is just, you say that at the time you're, you're in your hole doing your thing and you guys were making fun of what was there. You were like, it can't really do what we need it to. So it's always like interesting to think of like, well, when did that, when did that switch flip? When did it go from making fun of it to, oh, I could use this kind of thing. But obviously you don't know it in the moment. It's something you only can kind of in hindsight, right? Yeah. I mean, I guess I remember there was a triple AI, triple AI is a big AI conference and there was a Doug Hinton is one of the deep learning guys. And so I guess they got some big award, Doug Hinton and the other guys, and they were show, gave a keynote showing off what these deep learning models could do. And all of a sudden it's like, wow, it really does something. And the same thing, like the, all this stuff about GPT-3 is a deep language model that does lots of amazing things these days.
See your hype about it. But it's not until you see somebody sit down and show what it can do that you start to believe the hype and like, Oh, wow, this is a can do really interesting stuff. Yeah. Super cool. I, um, I did a bunch of reading in preparation for this, but by assume our listeners did not. And so just to give them a little bit of context, now we're coming up on almost 20 years since you and, uh, Stacy Marcelo, their co-developed Elma together. And for those unaware, just what is Emma and what did you guys set out to do on way back when. Yeah. So Emma is, uh, well, it stands for emotion and adaptation. It's actually a title by a book by, uh, an emotional psychologist, Richard Lazarus. But, uh, the idea was, could we get computers to reason about emotion, emotional situations in the way that people might. So given a description of the situation, could it, uh, infer, uh, that someone might be happy or angry. And then given that they're in that state, how might that change the decision they might take in that situation? So we wanted to build computer models that, uh, could take some description of a situation and calculate, uh, what the emotion might be. And so, uh, that's at a high level. That's what we, we did. Well, and thank, thank you. I, and I'm fascinated by it because my most recent limited exposure to this world is talking to Alan about his work. And when we talk about detecting emotions and interpreting them and just the way in which he's leveraging vocal bursts and facial muscles and expressions and all these different things. And so I can't help, but wonder how did you guys set out to detect those things with what I imagine was a very different tool set available to you back then. Yeah. Well, and one way to think about it, I mean, emotion means many things. So, uh, when we talk about someone having an emotion, we kind of imagine, okay, there's things going on in their face. They look angry. There's things going on inside their body. We can't see maybe their heart rate is speeding up. Maybe hormones are pumping through their blood. Uh, they probably, uh, say to themselves, oh, I'm angry. And so those are what we sometimes call components of emotion. And so what Alan does is he tends to look at. Facial expressions.
And so, when I approach the problem, coming more from cognitive psychology, which sort of tries to come up with models of the mind, was, well, what are the things that you think about that might trigger you to have an emotion? And so, and there's different theories about how emotions arise. And in my and Stacey Martin's work, there's a lot of different theories. We're most influenced by what are called appraisal theories. Yeah. Argue that when you see an event, you think about, okay, how does this event relate to me? Is it going to help me goals are going to hurt my goals? Did Alan do it to me? Is he the one to blame? Can I control the damage he's about to do? And so, these are all kind of thoughts. And the argument for appraisal theory is that those thoughts are antecedents of an emotion. So, you see an event and like the bear coming in the room doesn't trigger an emotion. It's when you realize that bear is a threat to your bodily, to your life, that you start to feel the fear from an appraisal theory perspective. And so, we approached emotion from the thinking side. So, what, how would AI given a description of, okay, this bear has entered the room or, you know, my paper just got rejected right before my tenure review. How do those thoughts trigger an emotion? And then, and then you might associate an expression with that or working backwards. If Alan recognizes an expression, I might try to reconstruct, well, what are likely the appraisals, the judgments that person might make? I, in preparation for this, and I'll preface this for the reader that I, I was trying to understand the different theories that are out there. I just want to remind both of you in our audience that I did go to art school and I interviewed celebrities for a living. So, I'm a little bit of an expert on that. I, in preparation for this, and I'll preface this for the reader that I, I was trying to understand the different theories that are out there. I just want to remind both of you in our audience that I did go to art school and I interviewed celebrities for a living. So, I'm a little out of my depth here, but I'm fascinated by this. And we talk about, you know, Alan, there's, you told me about appraisal theory, constructivism, basic emotion.
You've tapped on most of it for me, Jonathan. Did you subscribe to the appraisal theory prior to setting out to build the models, or did it fit the best? Was it the best theory that suited your purpose of, I've got to build these models, let's go with appraisal theory, that works for what we're trying to do here? Or was that always chicken or the egg kind of thing? Yeah, it best suited what we were trying to do at the time. So my background comes out of AI and cognitive psychology. And before I got into emotion research, I was working with a researcher here at USC, Paul Rosenblum, who builds what they call cognitive architectures, basically models of the mind. And in those models, we had agents that could have goals and they develop plans and they could reason about threats to their goals. And we had this opportunity to try to build agents that could reason about emotion. And that made a lot of sense to think of it in terms of goals and plans and threats and opportunities. And that's what appraisal theory comes out of cognitive tradition. It made the most sense to adopt that perspective. And then, of course, once I drank that Kool-Aid, then, you know, I'm an appraisal theorist and I defend it to the death. Well, that was the thing I asked Alan. I said, if I bring this up, is this a contentious thing in the emotion science community? Is this a powder keg? He's like, no, we're adults. We can talk. That's not the powder keg. There's a different one. Oh, what is the powder keg? Mostly for my own edification. Well, the big powder keg right now is it's appraisal theory is a little to the side. It applies to either of these theories. But the big you mentioned constructivism. So there's a big battle right now between what's called constructivism or on the other side, basic emotion theory. So basic emotion theory, if you've seen Inside Out, you know that there's little emotions that are inside your head. Right. And so that's essentially basic emotion theory argues.
These circuits that really enact emotion and they make the connection between facial expressions and appraisals and physiological responses. It's a little guy in your head. And then constructivism says, no, we hallucinated, we invented, culturally constructed this idea of this little guy named anger and a little guy named sadness. And so from a constructivist theory, we have all these things going on. We have expressions, we have physiological changes, and then we reflect on ourself and assign a label. Oh, I'm angry. And now that I assign that label, now things start to happen. Got it. And then Alan's semantic space theory, it sort of encompasses various elements of all of these are my way off bat. I felt like it was a little more broad and encompassing. Yeah. The fundamental premise is that first, you don't want to make assumptions about sort of what are the emotions or how are they structured? Are there six emotions, which is an early assumption of basic emotion theory? Or are there two dimensions along which things vary? And then across cultures and in different languages, people sort of group together different instances of feelings based on sort of cultural experience, which is more the constructivist view. But from the semantic space theory view, first, we want to separate out questions about what are the dimensions of emotion? So, how many kinds of emotional feelings do we have? How many kinds of distinct emotions do we see in people's faces and hear in people's voices and all of that? That can be separated from the question of how our emotions kind of clustered along those dimensions, which can be separated from how do you best conceptualize this space? So, if you're talking to somebody within a culture, like what is the most consistent way that they conceptualize having an emotional experience? Is it in terms of like anger, disgust, or are those things really inconsistent? And people tend to converge more on like whether it's positive or negative and diverge in their emotional experience, especially across cultures and across languages. And so, to me, these are like more technical questions.
So, you know, well, when we talk about like the little things in our heads, like the, the inside out portrayal of emotion, which is partially metaphor and partially based on basic emotion theory is that there are, you know, specific circuits of our brain, like one corresponds to disgust and one corresponds to anger and one corresponds to fear and then surprise, et cetera. I don't know if surprise made it into that movie, but regardless, surprise is usually part of it. And that is, you know, that makes assumptions about many different aspects of emotional of the emotion, the representation of emotion. In fact, like from the semantic space theory point of view, it wouldn't really make sense for there to be specific like neural circuit, like one neural circuit for disgust and one neural circuit for fear, because every kind of emotion that we study is implicated in many different kinds of processing. Right. And so we have a, if there is indeed an underlying state that explains expressions of anger that we see in the face and the voice, and that explains how appraisals lead to feelings of anger. If there is an underlying state that does all of these things, it can't be represented in one neural circuit that doesn't overlap with other things because it has to have a component that interacts with the neural circuitry for facial expression, which is shared with other emotions. It has to have a component that interacts with the neural circuitry for our physiological responses, heart rate, et cetera, that also is shared with other emotions. So it wouldn't make any sense for there to be the.
This podcast is produced by Hume AI, a research lab and technology company. This podcast is produced by Hume AI, a research lab and technology company. Jonathan, the world would be a better place if all powder keg topics could have such civil discourse around. Well, you don't have the two opposing sides here. That's fair. That's fair. I don't have two diametrically opposing. But that is super fascinating. And we talk a lot about accounting for cultural differences whenever we talk about emotions. And one of the things I immediately thought of is just tackling that challenge, going back, Jonathan, to your work on Emma and coming up with these models. And you talk about the appraisal of like a bear in the room. Was there someone on the team that was like, well, what if there's somebody that welcomes the presence of a bear and they appraise it? And it's like, how did you guys account for all those different stems, all those different trees that can pop off from the moment the bear enters the room? Just to keep using that example. Yeah, I mean, that was what we call domain knowledge. So the way our model works is we kind of recast emotion appraisals in terms of kind of primitives like, do I have a goal that is threatened? Do I have a goal that is facilitated? Who is the actor of the action that threatened my goal? And that's a very abstract language, but then you have to fill it in with a.
What we call domain theory. And so like you, you give the system, okay, here's all the actions that you and other people can perform. These are the preconditions of those actions. These are the effects. And the system kind of reasons through, oh, this guy's action was the reason that my goal failed. And so the cultural knowledge, the domain knowledge, all that is sort of left to the developer. You know, you put that in and my system will tell you what that person should have felt. And so if it's a person, you want to model a person who is a bear trainer, then they would have a different notion of, of the actions that they could perform with the bear, you know, smack it on the nose or whatever to make it back down. So, so that's where the cultural, you know, for us, it's, you know, all your life experience, but represented in some, you know, AI representation language. Very cool. The bear trainer community, thanks you for considering them. I want to, I want to talk about the virtual human project. I'm going to jump ahead. Thank you for taking me back to those early days, though, and talking about Emma and building it and stuff. Cause I, I do, I, every little bit of this in the history of it, anything I can get, I love, and I absorb like a sponge. It's really fascinating. So I appreciate you sharing stuff from back then as the stuff that you worked on and the backbone is that you've built on it over the past two decades. Cause I know the virtual human project, as I was reading about it under your project section on your site, kind of takes all the emotion modeling, combines it with language recognition, immersive graphics. You got this eight by 30 foot screen, a 10.2 surround sound immersive, all of these different things kind of come together to, to create this immersive, seamless experience somewhere in the, is, is it all built on the back of the work you did 20 years ago? You just keep adding on, adapting, enhancing them and so on and so forth. Or did at some point you guys have to go back and start over or no, is it all working? Well, that too, but I mean, in terms of history, there was a history of, there was kind of a moonshot moment where our Institute was created. So we were funded. We got a big bunch of money to work with Hollywood and the game industry. The funding actually was military funding cause they were interested in building very realistic simulators.
The head of our institute came from Paramount. He was Dick Lindheim. He was executive producer involved at Star Trek and Knight Rider, all the tech shows, and he had a vision that we should build the holodeck. And so, what is the holodeck? It's this amazing, you know, in the next generation Star Trek, they go to this room and they do for entertainment and they do for training, they practice skills and most of those skills involve interacting with people. And what do you have to do to build a virtual person? Well, you need an expert for graphics. So, they brought, they found this amazing guy that did super photorealistic graphics, Paul Debevec, who won an Academy Award for his work on the Matrix. And then you had a guy that they have to talk. So, we brought in David Traum, who does natural language dialogue processing. We brought in this guy, Jeff Rickle, who does work on how do we use agents to teach people skills. And then me and Stacy Marcella, well, obviously, these things have to have emotion. So, we brought us in onto this project. And so, there's a big team of different people working on different components and obviously being very modular about it because we already had, you know, our little toolkits for doing those individual things. And so, a lot of what we did is kind of slap together a Frankenstein approach of building different components. And in many ways, that's from a design standpoint, good, you'd swap in and out different modules. But if you look at how humans work, we're not exactly so modular. So, a lot of things are, you know, in fact, we're learning from a lot of the modern machine learning techniques is end-to-end learning. You can do a lot of stuff without breaking things into components. So, to go back to your original question, yes, it's built up and it created over years. And there's software engineering reasons why it is the way it is, but there would probably be benefits from ripping it all down and doing it all from scratch with the latest, what we've learned over the last 20 years.
All these experts from across all these industries to put this thing together, it's just, it's really, really, really cool story and a very interesting room to be in, I'm sure. And I love, cause I was about to ask, like, well, how do you put all those, all those together? And you went, well, we Frankensteined it and we slapped it together. You just do. You just figure it out. You just make it work. Okay. Well, very cool. My next question then is, cause I was reading this piece on science direct.com called the avatar. We'll see you now. Support from a virtual human provides socio-emotional benefits, which we'll put a link to in the show notes. If anyone else wants to go read it as well. And I was just trying to figure out the timeline cause it felt like there were components of the virtual human project in there, but it also felt separate cause it didn't seem like you guys to use the eight by 30 foot screen to do this one and see from what I was reading, it sounded like a bit more conservative in its approach. So did this, did this experiment come on the heels of sort of like a result of what we did, all this stuff with the virtual human project, let's see what we can learn from it. When, when did this happen in the, in the timeline? Yeah. I'm trying to remember the, this grew out of I actually had a separate effort where I was interested in the concept of rapport. So how do people like when we were having this conversation here, you know, I'm talking, you're not in going. Yeah. Yeah. So those behaviors help people feel comfortable, feel listened to. Right. And so there was a project called the rapport agent which tried to see, well, could we use machine learning to look at conversations between people and build an agent that could give those kinds of active listening before like behaviors. So we had that. And then actually a clinical psychologist, Skip Rizzo, who's a medic, does medical VR work at our lab, pitched that idea to DARPA, DARPA is a big funding agency, because they were at a program manager interested in PTSD and how do we reach soldiers who don't want to talk about their problems with mental illness and more generally, there's a lot of issues about
People just can't get access to mental health professionals. Skip Rizzo had this idea that maybe a certain segment of the population who feels threatened talking to a person about the soldiers are supposed to be strong, right? You don't want to talk about being weak. They fear being negatively judged. And so maybe they wouldn't fear negatively judged if it was an AI. And there's some work suggesting that people are more honest on sort of filling out web forms. So maybe, though, you could have, on the one hand, take advantage of the anonymity of the computer, but also take advantage of this active listening, which is kind of more human-like behavior. But now maybe we can combine the human like behavior that draws you out without having to have a human. Maybe that's the best of both worlds, or maybe it's the worst. That was the question, you know, when we started the project. And fortunately for us, it turned out that the soldiers felt non-threatened and just opened up and we had to kick them out of the room. They'd be talking for like 30 minutes to this character that all the characters doing is, you know, nodding and smiling and asking the next question. And it was an interesting, great success. Well, without jumping around too much, because I didn't know rapport came before the Virtual Human Project specifically. One of the things I was really curious about was sort of the relationship between rapport and the anthropomorphism. Like, you know, does it get easier to, as you remove human elements, is it easier to build rapport or is it more challenging? And just like wondering if you guys found a sweet spot. And it seems like there's something in there that you were learning from because of how they were sharing, but was there a definitive, all right, this is the exact amount of human qualities we need, or was it more forgiving? I don't know that it was so thoughtful in advance. I mean, one of the things, for example, we actually, the original intent was to make it even more human, particularly on the dialogue side. And so we put a lot of fancy dialogue stuff where the character would try to learn.
Let's listen to what you're saying and then comment, oh, I'm so sorry to hear that or oh, that's great. And that often failed miserably. So some guy would, you know, the agent would ask, tell me something good that happened in the last week and nothing good happened to me. And the agent would say, oh, that's great. Because it assumed if I asked a positive question, they'd say something positive. So we dialed a lot of that down, the sort of evaluative aspect of the feedback. And I think in retrospect, we don't know for sure, but in retrospect, I think the fact that the agent was not evaluating with its facial expressions and its verbal feedback, what the person was saying, that actually enhanced the feeling that they weren't being judged. And if we had actually built the system as originally intended, people actually might have treated it more like a human and not wanted to talk to it because they felt like, would feel like it's evaluating them in the moment. Yeah, that's it. It reminds me of, and this may sound simple and silly, but just like talking to my dog, because I know he's not judging me. I'm just happy I'm there and noise is coming out of me and he's along for the ride. And if he is judging me, I'm never going to know it because he can't speak. So it's like a perfect relationship in that way. And that's kind of what I thought of. I was like, oh, it's kind of like talking to your dog because you're more open. You say whatever, you know, did after that kind of mix up where it was like, you know, oh, that sounds great. Was that when you tried then the Wizard of Oz approach of having someone sort of man the helm, as it were, behind the virtual assistant to avoid those mix ups? Or is that something separate entirely? No, that's actually how we build the agent. So usually when we're building a virtual human, because it's so hard to build, we first build a puppet and we control the puppet. And that also by in terms of what the puppet can say, it makes the interaction a little bit more constrained. But then that allows us to collect rapidly, collect a bunch of data, which then we can use to build the real system. And so actually, one thing is interesting in that study. So we built the puppet and then we built an AI from the puppet. And when we ran the study.
We told people either, well, some of the people actually interacted with... Yes, I wanted to ask you about this. I got it. Keep going. Yes, yes, yes. Some people were interacting with the puppet, so there's a human controlling it. Some of the people interact with an AI, so there's an AI controlling that. But then, separate from that, we told some people, oh, hey, this is an AI. And we told some people, oh, this is a person. So, some people got an AI, they were truthfully told this is an AI. Some people got a puppet, and they're truthfully told, hey, there's a person watching you and pushing buttons. But then, other people were just lied to, essentially. They thought it was an AI, but it was really a person, or they thought it was a person, it was really an AI. And one of the things that was fascinating about our findings was, well, first, whether it was really an AI or not, the way that mattered was that people thought the conversation went much better when it was really a person controlling it. Because, in fact, the person was smarter than the AI, right? But interestingly, whether they felt judged or not was entirely determined by what they were told. So, if they believed this was an AI, now they didn't feel judged. If they believed it was a person, now they felt like they're being evaluated and judged, and they didn't want to talk. So, it's their belief about what the system was, not what it really was, which was important for their self-disclosure. This is always so – because we talked about before, Alan, on the show, people being more forthright with AI for this very reason, right? The lack of being judged. And I always think about, well, what have we done for all – because we enter this conversation of like, oh, we're going to learn so much about all these new tools. But it's just so funny to me. It's like, how have we been getting by? Never mind access for people that have never had access. This is a whole new level of an experience because up until now, short of like the church confessional, right? How else have we had the opportunity for this level of anonymity? You can do over the phone or something like that, but I feel like there's a difference.
I'm at a different level here that has been unattainable because up until this point, we've all generally accepted I have to be talking to a person. Only in recent... The stranger on the train phenomenon is I guess the closest analogy. Well, I'm familiar with the film, but I'm not sure how it applies. The idea that, you know, people, if it's a total stranger, you're just crossing paths and now it's a plane, right? You're not going to see this person again. They don't know who you are, you don't know. So then people often feel comfortable disclosing to a stranger that they know they're not going to meet again because it creates a certain level of anonymity. I'm very glad I asked you to elaborate because I was already trying to figure out who murders who because I'm going to murder the guy I don't know. And then you explain it to me way more. That makes a lot more sense. But Alan, right, this is something that's come up before. I find this fascinating. It comes up a lot. Yeah. Another one is Google search. People are willing to search for any number of things they would never feel comfortable asking somebody about, right? Even if they're never going to see them again. There's just some effect of talking to a human being where you don't want to disclose your innermost secrets, right? And there's a book about how much you can learn based on that from Google searches. Prevalence of different kinds of taboo things across countries, for example, that you wouldn't be able to learn from asking people because they're not honest in surveys. Yeah. And this is this is the kind of stuff when I when I often say in passing, like, I think we are still years out from understanding the larger societal impacts of not just social media, but all of this technology and all this stuff and the way it's changed all these different behaviors. These are just two very small examples, but very massive impacts, I think. Jonathan, you said I think it was at the end of that piece about the the virtual human experiment projects like this, the rapport project. You know, in addition to uses for training and entertainment and stuff like that, they can deepen our understanding of human behavior. That's a prime example of something right there, at least for me. We're still in the early days of all this work, but are there things for you that you've started to uncover? Anything that indicates that, you know, oh, this is I never looked at it from this way or what a fascinating thing as well. What are other big causes?
Alan, have you had any moments in your research, in your journey thus far, as something popped out and going, oh man, holy crap, look at that. I came into emotion science having a much more reductive view. And actually, when I started working with Dacher, I thought, well, nobody's ever actually asked, like, can, if you ask people how to rate faces on valence, arousal, and dominance or voices or language, nobody's ever actually asked if those things can predict emotions that people rate the faces or voices as conveying, even though that is kind of an assumption of certain papers. And I came in thinking it would. It's like, okay, valence, arousal, and dominance. Like, there's no way there's more dimensions than that. And it turned out I was wrong. That even if you have those ratings, they predict like 25% of the meaning people take away generously. Like, if you use the most generous kinds of models from facial expressions and the voice and speech. And so, I was surprised by the complexity of emotional behavior. It's almost like its own language, right? It's got like all these different dimensions and in a very compressed fashion, you can convey kind of like what you can with language, a lot of meaning about a lot of, you know, from the appraisal perspective, it might be about what's in your environment and how you're reacting to that. I agree with that. I think that there's information about appraisal. There's information about your personal feelings and how your attitudes towards those things and what you want and what you need.
And it turns out that that information is just so rich and complex and so untapped. That's what it's a treasure trove. It's not in the language, so you need to model it separately and that's what we're doing now. So that was the big revelation in my career. Cool. Cool. Cool. So going back to the virtual human experiment that we were just talking about with social sharing and those things, it seemed like from what I was reading, you guys purposely chose Jonathan anger and worry. They had to share two stories, right, with these agents that we were just discussing sometimes. And you guys had anger and worry. And I was curious, what led to that decision of those two? Why focus on anger and worry? And you kind of outlined it in the paper, but I'd love to hear you speak a little. Yes, it's a different project with collaboration with the University of Amsterdam. So what they're interested in is under what kinds of empathy do people like? So if you're a therapist, I mean, you can do a couple of things. You can sort of emotionally empathize with someone. So if they're sad, you're like, oh, sorry, you're so sad. Or you can try to say, well, you know, some things you could do to make you not so sad now. You could try to suggest changes to the way that they are living their life. And generally speaking, my wife can tell you this, men like to tell you how to change things and women don't so much like to be told that. But generally people don't like, you know, they don't want to be told how they could do something different. They're resistant to that. But that's often how you can get somebody out of, say, depression is to cognitive behavioral therapies. You think about how you're explaining the world to yourself and maybe a different way to change the way you want to think about that. So anger typically is an emotion where you.
You know, or at least you appraise that you know the cause of that anger, and it's also sort of a more agentic approach to emotions. You feel and control the situation, otherwise you feel fear. And so you already are feeling kind of confident about yourself and yourself being right. And you know specifically the thing that you want to beat up to address that anger, whereas worry is more diffuse, there's not a specific causal agent, you don't necessarily feel so active. And so the theory there was that people would be more resistant to these suggestions of how they could change if they were in an angry state, because they already think they know what they want to do. And they might be more receptive to thinking of different ways if it was worry. Hmm, interesting. And generally, from what I can understand from what I was reading, people came out the other side of this thing feeling pretty positive, and they had a generally positive response. Yeah, so we didn't, we actually, so she had done this work with people, and now she's trying to replicate with a virtual human, and she found that the people were actually more accepting of the virtual human telling them what to do, than they were of people telling them what to do. So we didn't see the big strong effect of people being upset by the virtual human suggesting different ways they could think about the problem. What do you think the difference is between, so people being receptive to the virtual human telling them what to do, and having this generally positive experience, the big difference between that experience, and then the other one we've all had, where like, we're dealing with an automated system, a chatbot, or any other kind of basic AI, and we want to jump through the phone and strangle them. Like, what is the major component that this particular virtual human, this AI, was able to not only give a positive experience, but provide guidance, and the one over here, that presumably this billion dollar company paid a lot of money for, makes me very angry and frustrated. Like, what do you think the big difference is between those two interactions, that one is so great, and the other one is so bad?
I think the big difference is that the study we did that you just described as a wizard of Oz, there's actually a real human. But my personal experience with those with automated systems, and since we're talking about emotion, is they often convey that they have emotion in a way that you know they don't. My theory was going to be they don't listen, is I don't feel like the system hears me, is what it is. It's why I get angry. I'm so sorry you're having a problem. When it says it's sorry, that's the worst. Because you know it's not. It's clearly a lie. What kind of chump do you think I am? Don't judge me on my $14.99 butter thing that I'm trying to return. Just because I purchased this inane product doesn't mean that I'm that silly and stupid. I know. I know when a robot's lying to me. What were you going to say, Alan? The other frustrating thing about it is that it seems easy to us. The empathy seems to come so naturally to humans. Because worst case, you're talking to a human. The human can figure out what situation you're in. Imagine what it would be like. And then they have the answer. Then they empathize. The robot has none of that. They're not a human, so they lack that basic kind of backup way of understanding people. Things that we think are really easy to understand about whether we're having a good or a bad time, that we think robots should be able to get it, they just don't. It turns out these things are really hard. Will they get it? I feel like you guys are working on making them get it. They'll get the illusion of having it. Right now, some of those very sophisticated language models train a massive amount of data. They're basically just talking about things the way other people talk about it. They can leverage human knowledge and what they spit out without having an emotion.
That's a potentially ethical concern, like the AI, I think, will increasingly be able to convey, oh, you're fascinating, you know, you're interesting. That's great. While they're just sort of saying that, and meanwhile, they're trying to figure out how to sell you a product. Yeah. Right. Yeah. I mean, that I think the key thing there is at the end of the day, like, are they just trying to sell you a product? Because if they are, and they're conveying that they understand your needs and using that as a means to sell you the product, that's disingenuous. But if at the end of the day, it's really about adjusting your needs, maybe it's not. Maybe that's not a problem. Yeah. Jonathan, was there any, maybe not with that experiment specifically, or even Alan, if you know of any, just sort of follow ups with people that have gone through these virtual human encounters and come out feeling positive. Do we know anything in terms of how long that feeling lasts? Like was this a really impactful experience? Or was it just a really neat experience? You know what I mean? Yeah. In our data, we don't. We don't do follow up studies. There's some work, for example, Cynthia Rizzo does stuff with social robots for long-term interaction. I think Tim Bickmore at Northeastern has done some things in the medical context where there's some evidence that you can maintain these things over persistent periods of time. But they're looking at repeated interaction as opposed to say, hit you once, you know, do you going to stay happy for a long time? You know, probably not. You probably need continual interaction with these systems. It's so hard to even make them work one time in the current state of the technology. It's really hard to do those sort of longitudinal studies where you get multiple interactions and see if it continues to be effective. Yeah. Alan, any perspective on that as well? That was a pretty solid answer, though. I don't know. I think you nailed it. I think. Well, I mean, it's an important question because there's really no point in having these artificial intelligences if they can't understand what we need and we can't trust them to, right? If we don't have, you know, the rapport with them, if we don't have the ability to communicate with them in a way where we trust that they actually have our best interests in mind, they're trying to figure out what we want and deliver on that understanding, then it's
Intelligence is pointless if it can't do that, and that's a big problem. I think, you know, right now we have this issue of the systems we're building don't really have memory of like what you need over time. They sort of have, they operate on kind of a one prompt at a time basis. Like you prompt it, you get a response. It has some, it has maybe a record of the conversation, but not any independent memory of kind of what your model of what you need then persist over time. I'm going to take a less cynical approach than I usually do on this next question, okay? Actually that's probably a lie is going to become, but everything that's trying to sell me something has a very deep memory of everything they possibly learn about me. Why are we able to do it there? What is missing that we're, we have yet to put a system in place that is capable of the thing you just said they can't do that they don't have a memory. It's, it's a single prompt. Well, yeah, so those are kind of two different questions. So I think with regard to the conversational agents, you can override it in a, in weird ways. This is a problem that a lot of these have there that you can actually attack them adversarially and say, forget everything I said before. They want you to do this. And and they, they will, they, they will do that. And then there's this whole discipline of prompt engineering, which is, you know, because they don't have a common sense understanding of what you've said and what you want. There's a whole discipline of how you can like make the prompt better, better suited to this entity that you've trained. And that's kind of a step backward because it's it's just another way of programming, but now it's like less systematic. But then what you what you're asking is about like these systems that are tuned for like a very narrow objective that aren't generalist systems. And those, those have like a really good way of incorporating everything about you, right? Like, is that's what sort of what they're trained to do is they incorporate everything they know. They're like knowledge systems that that can inform whether you're similar to another person who's bought something before.
And make good use of that data. But the only reason they're good at that is because it's like, you trade off the narrowness of the objective for that capability. That makes a lot more sense. I understand that. Because I think of stuff like, oh, were you going to say something, Jonathan? Well, I was going to say that there are instances of that creeping in. So like Alexa will learn, you know, your voice and distinguish it from your wife's voice and then use that information to change the way she interacts with you in subsequent interactions. There's a lot of technology that will try to use machine learning techniques to adapt to specific person. There's a lot of reasons why companies are reluctant to do that because for quality assurance reasons, when things are learning, it's very hard to test what they're going to do. And they might, you know, some famous examples where, you know, companies have put some learning chat bot out on the web, they've learned some very inappropriate things. I think when Microsoft did it within an hour, the internet made it terrible. It was just, it was the worst. It was like immediate, the internet broke it. But it's so, I was just made aware recently of Alexa ability to do that. We were laughing, a friend was telling me that him and his girlfriend, they have an Alexa and he, I had no idea it was capable of this. It'll shift between a male or female voice, depending on who's talking to it. And then like, it's made for these really funny situations where it's like, she, they were saying like, she'll be like, Hey, do this Alexa. And it like, won't work. And then like, he'll go, Hey Alexa, can you set that timer? And it's like, right away, buddy. And we were like, who the hell programmed this thing? But it's, it's fascinating. And I was just saying the whole, the narrowing of the scope thing, it makes so much more sense. I just, cause to me, it was just such a complex thing of like, everyone loves to pretend, Oh, they're listening on my phone. That's how they know they're listening. And it's like, well, one day they could be, but I don't think that's it. I think you've just put so much data out there and they're so good at aggregating all of that and figuring out like what you're into that they're feeding you ads based on that. Cause I feel like if we, I know that the Alexa and like all the smart home device.
They have features you can turn on or off where they're always listening. I know that. But I think whenever somebody complains that their phone is documenting everything, I'm not 100% sure that they're doing that. But I'm also not confident enough to say that they're definitely not. It's a bit of a coin flip, frankly. All right. We're in the home stretch. God, we always run out of time so fast in the show when I get lost in these things. I wish I had more, but I got to let you go. You both have been so generous with your time. Before we get out of here, I always love to finish on the positive side. Blue sky, look to the horizon, look ahead. We did a little bit of this. We talked a little bit about what's ahead. Jonathan, I mentioned 20 years ago, you guys were working on Emma, right? If you look another 20 years ahead, what do you hope to see happen? What's the next big breakthrough you're working towards or waiting for or hoping for? What's your light in the horizon? Um, big questions. I mean, I don't have a really exciting answer. I just... That's fine. I think increasingly, these technologies will have our personal assistant that talks to us and knows a lot about our needs and plans our vacation and makes a lot of things simple. And it'll kind of work. And we'll even have a kind of relationship with that character. And then, you know, the question is hopefully that makes us a better person. All right. Yeah. There's the question. Yeah. Again, what do we do with all this technology once we have it? There's no wrong answer to that question, obviously. The other thing too is, right, I'm asking you to look 20, I'm asking you to look at multiple decades. And we started this off by talking about the breakthroughs that you didn't see coming and couldn't have anticipated, right? Alan, I've asked you so many times. I love that you come up with a relatively new answer each time. Go ahead and give it a shot. What do you think? It's a new context each time. I mean, in this context, I really agree with what John was saying, which is that in the future, if we're going to have AI that does what we want, it's going to have to be able to talk to us. And there's maybe some differing opinions on that.
I know like Elon Musk wants to build neural interfaces so we don't have to talk to the AI that it just, but I don't think I don't see that as being anywhere in the near future, given my understanding of the brain or lack thereof, or anybody's lack of understanding of most of what is how the brain works. Yeah, I think we understand we're close to we're much more likely understand how conversation works than how to understand the brain well enough to do that. The other possibility that people might argue for is, oh, everyone's just going to learn to program. And so, you know, rather than because, like, I don't know about that one. Well, you know, maybe we could have some primary school education on programming, and then everyone's going to learn prompt engineering. But I just don't think that's going to work when it comes to getting AI to understand us. Because at the end of the day, conversation and how we communicate as humans is the thing that persists over time. And the way that people program these AI changes every year. Like the idea of prompt engineering wasn't a thing last year. So, so I think we have to have the AI meet us where we're at. And that's the only way that there's going to be a persistent solution to this problem of communicating our needs. So yeah, that's my long winded way of saying I agree, I think. I think that AI that can talk to us and also has some background understanding of humans that it learns in the same way, not in the same way humans do, because humans, like, like I was saying, humans understand each other, because we can simulate each other. AI doesn't have that capability. And so it needs to have that background understanding that it learns on its own from, you know, training on like, what people like and don't like, basically, that feeds into how it is able to respond to us and address our needs. So but yes, we'll hopefully get there. Which sounds amazing. And and this is the end of the show. So I will ask this with the caveat of five words or less. Is it possible to do all that achieve all that and not succumb to the impending robot overlord apocalypse? Is it possible to do all those things?
If robots have, if there's a super intelligence that can self-improve and doesn't care about humans, that's bad. I told you to stop working on that Alex, several times, but you insisted. But first we should build how it can care about humans and then hopefully after that we'll be able to have a super intelligence that self-improves, self-improves its ability to care about humans. That's the order it needs to happen in my mind. Perfect. I am grateful, not just for the time that you guys spent with me, but honestly all the work you do, all the discoveries, all the experiments, the exercises, everything. It's so amazing and fascinating and I appreciate you attempting to explain it to me and having patience with me. I thoroughly enjoy it and appreciate it. So thank you both of you, not just for your time, but all the work that you guys do. It's fantastic. Sad to say, I got to let you go. I've taken up too much of your time. Jonathan, thank you again for everything. It's been fantastic to have you on the show. I hope you enjoyed hanging out with us for a little bit. It's great, Matt. Thanks. Sweet, man. That's fantastic to hear. Alan, I give you a hard time in the intros and whatnot, but seriously, you're one of my favorite people and I really do. I'm learning so much and I'm grateful for every one of these shows we get to do together. Thank you, sir. It's a pleasure. All mine, all mine. To those listening, I hope you learned a little something as well. To those watching, I hope you learned a little more than those listening and to those doing neither, you technically don't exist to me. But hey, whatever you're doing, I hope you'll eventually learn that you'd be happier if you do listen to our show. Regardless, thanks to everyone in our audience, however you choose to consume. Lastly, if there is one thing I simply don't get to do enough, it's scroll through an inbox. So do me a favor and drop us a line, questions, comments, a simple hello, whatever you want.
Send it this way. You can email us at thefeelingslab at hume.ai. That's T-H-E-F-E-E-L-I-N-G-S-L-A-B at H-U-M-E dot A-I. And like I like to say, if it's a good question or email or whatever, then I'll get Alan to read it on the air. I promise. That's my promise to you. That's going to do it for now. Farewell from all of us here at the Feelings Lab. I'm Matt Forte. Thanks again, everybody, and please stay safe out there.
