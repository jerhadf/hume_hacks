Hello, world. What is up? Welcome back to the Feelings Lab. I'm your host, Matt Forte. And for today's episode, we're talking about two of my favorite things, compassion and robots. This right here, this is season two, my friends. As the most loyal of listeners may recall, at the end of season one, I hinted that this year we'd be shifting our conversation slightly and asking some really big questions. But worry not, at our heart, as Dr. Alan Cowan has said multiple times, this is a podcast about emotion. You see, last season, we explored how science is just beginning to map out how all our nuanced feelings and expressions shape our daily lives. This season, we'll discuss how these emotions shape our families, communities, society, and the surprising ways modern technology has upended these familiar structures. Because here's the thing, technologies are already processing our cues and our emotions. That's happening. We all know that. But they shouldn't be designed to exploit them. We'll explore how scientists and technologists are seeking to bring empathy to social media, robots, digital art, and a ton of other stuff, building equity and compassion into these ubiquitous AI-driven systems.
With all that being said, once again, we find ourselves in a position where an incredibly smart, accomplished and generous individual has decided to grant us a little bit of their time. I could easily be talking about Alan, but he's always here, which for the record, I am grateful for. Speaking of which, Alan, how are you doing, sir? Doing great. How are you, Matt? I'm doing great, man. It's great to be back at it with you. But obviously, I'm not talking about Alan. I'm talking about today's guest, whose remarkable life journey has led them to the forefront of consumer robotics. He's worked for NASA's Jet Propulsion Lab and has sent his tech to Mars. He revived former startup Evolution Robotics with a product so far ahead of the competition, iRobot had no choice but to buy the company and put him in charge of their tech. And now he's the founder and CEO of Embodied, whose companion robot Moxie redefines our relationship with technology and lays the groundwork for a far more optimistic future than perhaps James Cameron or Arthur C. Clarke might have suggested. Please welcome to the show the fantastic, the awesome, I'm super thrilled he's here. Paolo Pergignan is here. Paolo, thank you so much, sir. It has been a long road here. How are you doing today? I'm good. Nice to meet you, Matt and Alan. Great to be here. It's so nice to have you and to meet you and have you on our show. It's really exciting. I want to jump right in and get in today's conversation. You know, when we think about robots with compassion, I personally, I think of Wally, R2D2, even Johnny Five, to date myself a little bit here. But let's let's start in a fun place. I'd love to hear from both of you. Just simply some pop culture robots that might have inspired you.
Who are some of your favorite pop culture robots? If you have not seen the new animated movie by Disney that recently came out, Ron's Gone Wrong. It's an amazing animated movie about a robot that helps children that are socially not successful to find success and make connections and friendships. Yeah, that's another beautiful robot. A lot of really nice, sweet robots coming into pop culture these days. Then the flip side of that point, of course, when we think of robots without compassion, you've got HAL 9000, T-1000, Ultron, GORT. Other than the obviously way more ominous naming scheme, what is the difference? Why are those not the robots that we have a fuzzy feeling? How are they not compassionate robots? Let's start here. Allen, as my emotion science and nuance expert here, how do you define compassion? Where does it fall on the map? Compassion is a range of expressions that we make.
And I'm going to talk a little bit about what we're doing here at Hume. We have a lot of expressions of empathic. So there's expressions of sympathy, like, oh, oh. And they kind of reflect what somebody else is experiencing. There's ways that we reflect that with our face, with eye contact, kind of head tilt, nodding. Those are all ways we express compassion. But to be compassion, they all have to be authentic. They reflect patience, reflect an acknowledgement and understanding of what we're feeling, curiosity about our feelings, an orientation toward what somebody is experiencing and not an evaluative orientation or a judgmental orientation. And so these are all ways that we find compassion in others. Some of that's easy for robots. You know, you could add expressions to robots, but it's hard to get it to be authentic. It's hard for people to really trust that this is something that cares about me. That's the challenge. One of the words that jumped out to me is the curiosity and the idea of getting a robot to be curious. And I'd love to kind of open that box up a little bit, because in the videos I've seen of Moxie, Moxie asks a lot of questions. Moxie feels curious. Talk to me a little bit about that, Paolo. How important is curiosity in sort of creating compassion within a robot? Well, I think it's super important. And the way Moxie is doing it is really using a trick to get the child to reflect on things that they may not have thought about themselves. So it's encouraging them, giving them an opportunity.
It's a safe space in which you start thinking about, yeah, why am I feeling the way I'm feeling? And that reflection is an opportunity to learn more about yourself and understanding your emotions and your feelings, which opens up a new door, which gives Moxie an opportunity to start teaching the child strategies about how to manage your emotions, to cope with things such as anxiety and other difficult situations. For sure, there's so many amazing things that Moxie does, and I'm excited to talk to you about forget now, hang in there with me because I'm going to say a lot of stuff, but I promise I have a point at the end of this. So forgive me if I'm oversimplifying things, Paolo, but essentially, if you break down your most significant contribution to the Roomba at iRobot, it was that before you, the Roomba was just kind of randomly bouncing off the walls and furniture, and at some point it would have cleaned the room. But you and your team, you had put a camera and lasers and a small computer, all for under 20 bucks, if I remember correctly, onto the robot, which basically gave it the ability to create a data set, right, and know where it is. And in this case, data set would be the room, and that makes basic decisions based on that set. It can make a more efficient path and clean the room better, which says to me, a robot is only as smart as the data it has to work with. OK, is everything I said there so far relatively accurate and approximately correct? Yes. Cool. OK, bear with me then. So then I assume, in order for a robot to be compassionate and emotionally...
It requires what has to be an immensely complex data set. How do you solve for that problem? Because you can't just point lasers at my face to find out if I'm happy or sad or anxious. So how did you guys sort of approach that issue with Moxie? Well, again, it's about using data to to be able to create awareness of the environment as well as the child. So we use we have a single camera on Moxie and we have microphones. We have contact sensors, touch sensors, and we have accelerometers, so movement and so on. So that that is all it takes for us to understand by visually seeing the child, analyzing the image, finding the face, finding the eyes and allowing the robot to actually make eye contact, which I think in the first protocol of interaction, human to human interaction, eye contact is super important. Right. If if I talk to you and I'm looking away, you may consider that as rude or awkward at best. Then microphones again, probably one of the cheapest sensors you can buy. Microphones are extremely low cost, but yet you can use them to understand voice apart from the conversation. And content of conversation. There's also a lot of emotion that can be encoded in the expressivity of the voice.
Jaime, you can hear in my voice whether I'm excited, sad, depressed, even potentially. Those are primarily the two sensors we use. Both of those are probably the lowest cost sensors you can imagine. The camera also in these days is about a dollar or two. The challenge that comes with those sensors is the processing required to really understand the signals that they produce has had the entire technology community in the last many decades trying to understand what are the algorithms that can robustly extract information that we can use to take action on it. Only in the recent years have we seen progress, and that's thanks to the neural networks combined with availability of datasets as well as enough compute power. The combination of those things has allowed us to do things that, when I was doing my PhD 20-plus years ago, we could barely recognize lines. Now we can do amazing things very robustly and at much lower compute power required. It's one of those things that you mentioned when you were doing your PhD, and I had read somewhere that you said one of the earliest inspirations was the Pixar short Luxo, Jr.
And I looked it up because I remember that shirt, that was 1986. And so that's like 30, you know, 36 some odd years ago. What is it like now to finally be here and be present and be at the forefront and see all this technology catch up to all of these ambitions and dreams that you had three decades ago? It's fantastic to see, but at the same time, it has, I would say, from my perspective, it has taken a lot of patience because what Locksword Jr. inspired me to do was to actually go to school and pursue computer science and robotics. And then decades later, we are able actually to bring those kinds of ideas to life because the technology has just taken that long. In perspective of history, 20, 30 years is not that long. And also we know there is acceleration of innovation happening that we don't want to maybe say singularity, but definitely there's an exponential acceleration happening. So what has happened in the last 30 years felt slow, but what is going to happen in the next three years is going to completely blow out of water what we have accomplished in the last 30 years. Yeah, I can't even fathom. It's obviously a wild ride that we're all in for. Okay, so one of my big questions, and I feel like we're answering it, Alan, maybe you can help me, is, is it even possible for robots to actually be compassionate? Based on what you've said compassion is, based on what Paolo said robots can do, is it possible for robots to be compassionate?
Yeah, I mean, not in necessarily an emotional sense, you don't need the robot to be feeling emotions, but it is possible for a robot to be programmed to care about your emotional experience and for you to be convinced that that's the case authentically. And, you know, it doesn't necessarily require artificial general intelligence. I mean, maybe this is something we can talk about. I think it really requires a robot that's honest about its limitations, about what it's able to understand, what it isn't able to understand, that asks questions. And when you think about robots like R2-D2, like Baymax that we talked about earlier, these are robots that displayed sort of almost ignorance of humanity. And yet at the same time, we think of these as like having good intent. If we can kind of tell that these are robots who want to serve our well-being, that are earnest about it, actively care about what we value, but are still learning, they're still learning and exploring. And, you know, we don't want to get to a point where the robot seems to understand more than it cares about, or seems to have a really deep understanding of our emotional state and our intent and motivations, but doesn't seem to actually care about those things or have an understanding of how to respond positively to those things. That's where those things can become out of line. So as long as those go hand in hand, I think we're okay. We just don't want one to get too far in front of the other. Got it. How...
How do we maintain that balance? That's an easier said than done, I imagine. Yeah. I mean, that's the golden question. I mean, it really has to do with thinking about the motives of an agent and how do you convey them. If the agent is really, really smart, if we're talking about a HAL 9000 or generally intelligent robot, we have to really be convinced that it has good intent, right? If it seems to be serving some hidden objective that's kind of contrary to our objectives, even if it's showing signs of emotional intelligence, it actually may look worse, right? It may look like a psychopath. We need to be convinced that it understands what we're trying to convey with our emotional expressions and actually has the intent of serving our goals. Well, this dovetails, and Paolo, I don't want to cut you off if you had a thought, but it dovetails nicely into a question that was next up on my list, which is if you take pop culture out of it for a second, and I imagine this is a question that would have had to have come up at some point in the four-year development of Moxie, is why are some people scared of robots? You talk about how with certain behaviors, it can be interpreted as a potential psychopath, right? That's an obvious instance of why we'd be scared of that particular robot, but I got a buddy to this day. He won't go on Pirates of the Caribbean and Disney World because he, quote, doesn't trust them. 
People I know that are just afraid of robots. Why do we think that is? Where does that come from? Did you guys discuss that at all when you were coming up with Moxie and how to keep it from being scary for certain people? Yeah, for sure. We do our best, but I think it's natural human behavior to react that way to things that are unknown. Anytime there is a new technology that gets introduced to us, people have skepticism. When computers were first introduced, everyone was under the impression that they're going to lose their jobs. Look what happened. The economies grew way beyond our imagination. I think the same thing is going to happen with robotics. When people talk about replacement of jobs, yes, there will be a replacement, but people will actually move up the hierarchy and get better jobs rather than standing in a factory and repeating the same motion over for 10 hours a day every day for the rest of your life. I think it's healthy to have some skepticism. At the end of the day, my personal view on this topic, which comes up very often, is that the technology does not have an intent of its own, doesn't have a malicious intent or a positive intent. It's a tool. It's how we humans and maybe innovators and engineers decide to do with that technology and how we direct.
The aim is to direct that technology, so if you have engineers or scientists and business people behind the concept that has the intent to help and be technology for good, that product probably will be positive and beneficial to the society. But the same exact set of technologies could be used by a company that wants to exploit you, use it for war, use it for exploiting you on social media, creating addictive loops so that teenagers get addicted to spending eight hours a day on social media, leading to suicide rates being at record heights today. It is our responsibility, and I think technology is just a tool. Right. Did design factor into that heavily as well? Because you talk about weapons and things of that nature. I know Moxie has very soft edges. The decision to use a projected screen instead of a flat rectangular screen, I assume some of these decisions were informed by the idea of making something that's not really necessarily an imposing but an inviting figure. Was that the case? Absolutely. Absolutely. When I started the company, I started with a statement. I said, I don't want to create yet another white statuesque robot. I want it to feel organic. I want it to be lifelike. And that led to many decisions, including the back projected face that you're talking about, instead of having a display, basically a monitor.
We wanted a monitor stuck in the head that looks like a monitor stuck in the head. We wanted it to be true to the character. And that came with a lot of implications on the hardware development. Many, many challenges came with that. To even including eyes on the robot. If you look at any other robot in the market of similar realm, they have fake features that look like plastic parts where the eyes should be. Because we know eyes are important. That's the designer telling the engineers eyes are important. But those eyes are plastic parts and they don't move to make eye contact, which is the engineers telling you it's extremely hard to make eyes that can make realistic eye contact without it becoming uncanny. So we decided we are not going to cut corners. These are important things. If you want to have impact, we are going to make decisions that are going to take us years to develop. And here we are. That's why it took four or five years of R&D to get here. For sure. For sure. And you mentioned avoiding it being uncanny. And that's exactly where I was going to head. I was going to ask Alan, is the uncanny valley a part of all this? The uneasiness for our listeners unfamiliar. That's a simple explanation. The uneasiness you feel when you see something that's imperfectly human. Think of like a video game character you're watching at home and there's no life behind the eyes. But the skin is hyper detailed. And that could be kind of creepy. Alan, where does that come from? What is that? Solve that for me. I mean, it's a bunch of things. And I don't think anybody.
Really has the full answer to this. But I see it as two different effects. One is the one you're talking about, where, you know, something looks too human, but it's just off. I think that taps into something really deep. If it seems to have a human appearance, but maybe it's something to do with illness. Maybe it's something to do with an inability to predict what this thing actually is. That throws us off entirely. And that's sort of a low level effect. And it could come from a lot of places. Maybe it even comes from our interaction or evolutionary history with things that are humanoid that aren't human, like Denisovans or Neanderthals or something like that. There's so many different explanations for that, but it's a really robust effect. But there's this other effect that I think is more high level, which is the uncanny valley effect of something that seems really smart being inauthentic. And that doesn't necessarily have to be something that looks human at all. Hal 9000 has this uncanny effect in the movie, even though it has no body, right? It's a disembodied agent. It has a voice, which maybe is part of it. But fundamentally, I think what we see is a sign of emotional intelligence in something that can't really be trusted to have good intent. And we see that same effect for people who are psychopaths, essentially. If you see somebody who seems to understand you really well and is speaking somewhat speaking your language or seems really receptive to your cues of emotion, but you think it.
Deep down, there's some other goal that, you know, this thing, this person doesn't have your goal is in mind at all. Maybe it's a, you know, the sleaziest kind of like used car salesman archetype where, you know, they're trying to sell you something. And you know, deep down that there is an insincerity to what's being said that can have an uncanny valley effect as well. I'm hearing the significance of authenticity. It's huge. It's a big part of it. Paula, you'd said something when you left Start Embodied, I think it was, and you were leaving iRobot and you had the, I'm paraphrasing, obviously, of course, but it was essentially that you felt we were on the precipice of a massive change in how we interact with technology. You can feel like the winds have changed, and that was like 2016-ish, I think, right? And I've been thinking about that a lot because I'm wondering what were you seeing and observing in the world at that time that it was apparent to you that like, now is the tipping point. Now we're going to start seeing things really change. What was the big flags for you that caught your attention? I think the advancements we saw in things such as speech recognition represented by Amazon Alexa, the Echo, that was a big leap in technology. And I can tell you that very early days in my career when I joined the first startup, we were trying to develop a robot similar to Moxie 20-plus years ago. And I remember walking to this incubator where they had a first version of a robot. It looked like a full metal jacket, that's what we called it.
There was a guy with a headphone walking behind a robot and screaming at it, Marge, stop. Marge, stop. These things are going to hate us eventually. It's going to happen. And the robot just kept rolling on and on and on, and it would not recognize that simple command of Marge, stop. Marge was the wake word, like Alexa, and stop was the command. It was with a headphone, not even across the room while the TV is on and your child may be playing with your pet in the background, right? So that was a major leap. And this is an experience we all have had with in-car voice recognition technologies before this revolution, like 10 years ago. If I use my in-car voice recognition, by the time I was able to verbally input the address of the destination, I was at the destination. That was the friction point. And this is, again, enabled by deep neural networks, data, and computation. The same thing happened on computer vision. And we are seeing it in even ability for us to be able to generate videos of animated people that are completely photorealistic, right? Cloning you, deep face, and all these other things, right? So that's what was happening. So I felt that it's time to go and try to make this happen.
Close the gap from where we were on technology to applying all these technologies to building what I think is the next step in human-machine interaction, which involves embodiment, it involves body language, it involves intonation of voice, emotional expressivity, and all that. Juan, awesome. Thank you so much for sharing that and sharing your perspective. One of the interesting things I've been thinking about with Moxie is it's designed to aid and educate kids in how to better interact with humans and to improve their social intelligence and confidence. And it's a tool to help children feel more confident, not just with themselves, but with other humans. But the interesting thing is it's also establishing what they will then expect of robots and technology as they grow up and move forward. I'm reminded of when iPads launched and you'd see like toddlers pinching magazine pages trying to zoom in. I doubt that's a happy accident. That's got to be, I assume, intentional and something part of a larger plan in the decades to come and what you kind of envision, right? Because that's such a cool part of it to me that I don't think is immediately apparent to a lot of people is that this sort of sets the stage for the future they can imagine. I think we scientists are trying to catch up with what people have already imagined. The notion of having AI friends and companions is not new. It's decades old.
By the way, it is a bit of a challenge for us scientists because the expectations are extremely high, and we are just trying to catch up. I think actually the expectations run away a little bit when they talk about robots taking over the world and so on, and I'm sometimes tempted to say, come to our lab and see what problems we are dealing with. We're trying to get the robot to move an inch, and you think it's going to run after me and jump on me and grab me and capture me. So there is a long way to go. It's so funny because you mentioned that, and I think anytime Boston Dynamics decides to scare the living daylights out of the rest of the world with their most recent progress, they'll upload a video of like, I forget the official names, but they have their dog-like robot and their humanoid robot. Spot. Yes, Spot. Thank you. They can call it Spot all they want. It's still a terrifying looking little piece of equipment to me, but technologically, super impressive. People see those things and they go, this is it. This is how the uprising begins, but you're saying just because mechanically they have that, it's so much more complicated. We're nowhere near that point, and you don't personally believe that we'll reach that point where they will one day become our robot overlords. We will reach that point, but not in the foreseeable future. I mean, look at the self-driving cars. Cars are driving on pretty structured environment with lanes, lines on the lane and signage and all these things and rules and structure, and it's taken probably, I would say, 20,
trillion dollar investment to date, and we are still 20, 30 years away from self-driving cars. That's so wild. It's so wild that you're 20, 30 years away when there's so many people that are convinced they already have one, but it's like, no, this isn't a truly self-driving car. There's still a long way to go. Yeah, it's impressive from a technical standpoint that you can lane keep on a freeway and it will follow the curvature and all these things and even change lanes, but that is like 10% of the problem. Imagine driving in a snowy road when there is an intersection, there's a child running after their pet onto the street, and God knows what. I mean, those are the cases we need to cover as well, especially since human life is involved there. Alan, what about you, man? How do you think we get there? Not that I want you to get us to robot overlords, but between now… Let's focus on self-driving cars and positive things. In the next 20, 30 years, what's missing? What jumps and leaps are we going to make between now and then? What are you anticipating? What are you looking at? Yeah. I mean, to me, whether the robot overlords take over and do things that are damaging to the human race is really a matter of what steps we take and in what order. I think starting with things like Moxie is actually the best way to avoid this situation, this catastrophe, because you're giving robots the beginning of social awareness and the beginnings of the ability…
It's a way for people to have the ability to start to optimize for human well-being, for people's emotional responses. If you just gave them embodiment and an understanding of the physical world and power over the physical world, without that, without the ability to understand people's needs, I think that's what opens the way to the scariest scenarios. I think that's what opens the way to people being kind of afraid of robots because embodiment is a really powerful thing. There's this really intelligent agent behind your Netflix recommendations. But at the end of the day, it's just taking in user behavior and it's just outputting recommendations and that's it. It has a very limited input and output space. But as soon as you introduce something into the physical world, the input and output space both blow up. Not only is it perceiving things, generally speaking, but it's also manipulating the real world. There's so many different ways that it can accomplish whatever goals you give it if it's smart enough. It's really important that you think about how do you control the means by which it accomplishes its goals and make sure that it knows how to look out for human well-being, for human experience along the way. I think that's something where Moxie and that sort of area being tackled first is really, really promising and encouraging. If we can get that right, if we can get the empathy problem right as a first step, then the robots that can actually...
Do things like operate heavy machinery seem less scary to me, right? Well, you mentioned the first step and we talked about this is the beginning. And I believe I've even heard you say somewhere Moxie is kind of phase one for embodying a much larger, longer plan that you look down the road and you've even said, and I really enjoyed this, think of Moxie is almost like the Apple Macintosh, which I found very exciting when you think of how far we've come just from that machine. So obviously you're looking 10, 15, 20, 30 years down the road at the risk of asking you to reveal the 11 herbs and spices and give up any safe secrets here. Can you, what, what, can you give us an idea of what step two or three might look like without revealing anything? That's a super important for you to keep safe for embodied, but I'm just very curious about what comes after phase one. Can you share anything about that? Yeah. I mean, I'll start by saying that we have never been as connected as we are today with all the social media and tools we have for communication. There's 50 different ways we could do a video call. There's a hundred different ways we could chat with each other. And there's social media that let us connect to friends from my high school in Denmark, but yet we have never been as lonely. So 17% of the population in the U.S. suffers from mental health challenges, anxiety, depression, and other things. Loneliness and all these things, especially after COVID, it's blown up even further. Suicide rate among teens.
It is at record highs. There is not a Gen Z-er that I meet that doesn't suffer from anxiety. So the healthcare system just does not have enough providers to take care of this mental health crisis. And our belief is that AI friends and AI technology can help, not to replace the need for human contact, as a matter of fact, to help facilitate that. So in a way, the narrative that we are thinking about is counter to the metaverse. The metaverse is trying to get us more and more immersed into the virtual world, into the matrix, if you will. And I think we have enough evidence to know that is not healthy. Just to quote the surgeon Gerald in the US, last year or two years ago, he made this quote that says, loneliness and social isolation is equivalent to smoking a pack of cigarettes every day. So the implications of this on mental health that leads to other health issues is tremendously challenging. And we are trying to put our little dent in the universe by creating AI friends that are going to be companions to people. We have started with children, which are the most vulnerable in our society, and we've got to look after. And then we are going to gradually provide it for other areas. And then we are going to augment it once we have.
As Alan said, once we have that empathy built into the system, then the next step is to give it physical capability to provide also physical assistance to people that are in need, to help people be able to live independently and with a higher level of satisfaction or happiness, if you will. So short term, it's children, the next couple of years, you'll see it expanding to other parts of the population to inject empathy and help people with learning to become happier in their lives, which I think will help their people that are touching these people to become happier in their life. So that's how we think we can make a better world, to use a cliche, but literally, we are going to do it one child at a time and then later, adults and elderly. That's a fascinating window into your plan, and I appreciate you sharing with us. That's amazing. You also mentioned, obviously, how COVID-19, the pandemic sort of exacerbated things. And, you know, we talked, Moxie's been in development for four years. I believe I first started hearing about it, like right at the start of the pandemic. I'm wondering, has that refocused anything for you guys or just maybe lit a fire of like now more than ever? It's necessary because, yeah, 100 percent, these problems existed before COVID. But now so many people are isolated. We're we're doing an interview.
I'm just wondering if it impacted your roadmap or what you prioritized and what you guys are getting towards first, or if it was just more of a sense of urgency? No, I mean, we had this insight in 2015 before we started the company that loneliness and mental health are, we have a crisis in those areas, even in 2015. The pandemic just gave us more confidence that we need to do this. Amazing. We're getting into the homestretch. We're almost there. I want to go back to one of the first things we said with our pop culture robots, Wally R2D2 Baymax. At this point, I feel comfortable answering this, but are these accurate portrayals of what we can expect from the future of robotics? What do you guys think? Are they giving us false hope? Or is it an accurate idea of what we may land on someday? Well, I think that it's real hope. We should be hoping for those kinds of results. My favorite kind of hope. On the other hand, there are certain constraints that have to be addressed. The way that robots are envisioned, often their interaction with the world seems to be the easy part in sci-fi, but actually, that's the hard part. The embodiment part is really the hard part, and what we're going to solve first.
our sort of like disembodied ambient agents being really intelligent and then introducing those into the world. And when you conceive of things that way, it becomes interesting. There are certain applications like embodied, like Moxie, where you're sort of limiting the ability of that agent in accordance with what it's able to accomplish in its physical form, which I think is really, really smart. But there will also be ways that our current digital assistants like Siri, Google Assistant and the Google search and all of the big algorithms that we interact with start to manifest in physical form. And rather than having like a Wall-E or an R2-D2, you would just have something that knows you in other ways that accesses some sort of account that it has. And, you know, you might be talking to R2-D2 and it'll access your account. And then when your friend talks to R2-D2, it accesses their account, might have a different personality for each person it talks to. The software will be much more complex, much more. Not necessarily intelligent, right, because these robots sort of act like children. And the implication of that is that the trajectory of artificial intelligence is that we'll build something that's childlike and then we'll build something that's adultlike, which is an intuition that's based on human development. But that's not really how the technological progress works. Instead, we're building things. We already have AI.
I'm going to talk a little bit about what I mean by empathy, which is a term that comes up a lot in AI that's extremely adult like in certain ways, you know, can beat people at the most advanced board games, but lacks even, you know, really, really basic, what we would consider basic skills that children have, like the ability to empathize, like the ability to simulate each other, and to engage in sort of spontaneous altruism. And if we can, then the R2-D2s of the future will just be really, really smart versions of R2-D2. But one thing's for certain, we will call them R2-D2s. That is decided. Hopefully, right? If we're lucky. If Disney licensing has anything to say about it. We won't do it for free, but we'll call them R2-D2. Well, if Disney is the first to develop that intelligent robot, it will be called R2-D2. Otherwise, probably not. But, you know, maybe called something else similar. Fair enough. Fair enough. Well, I mean, Paolo, did you want to contribute at all to the, are they false hope? Or did you have anything you wanted to say there? Yeah, two things. One is R2-D2's NLP stack could be very simple because it doesn't actually speak other than making sounds and beeps and sounds. Beeps and boops? But I think in terms of talking about sci-fi examples, the movie Her as a voice agent that really can get into your mind.
I'm not going to the point where the main character fell in love with a voice. I think that is going to happen in the next five years. I think we are not that far from that from happening. And then that can lead to literally having what I think is our dream at Embodied to have therapies that understand you at deeper levels than you may understand yourself and provide you help. And by the way, this is not sci-fi because there's already a number of companies now that have received FDA approval for what they call prescribable digital therapeutics, which is software that applies cognitive behavior therapy and other evidence-based therapy techniques to help people with things such as substance and drug abuse and the postpartum depression and insomnia and ADHD and so on. So we'll see more of that and combine that with a interface like Moxie that can create real connection. And now you have a therapist that can actually help a lot of the things I was talking about, which is the reason why we started this company. So I think in the next five years, we will see the movie Her become reality. Wow. Wow. That's wild. All right. Let me ask my last question here before we wrap things up. Step in, gentlemen, as we prepare to go down the world's slipperiest slope ever. So we've talked about robots for almost an hour now. We've talked about how they are.
We've talked about how they're getting smarter, they're becoming more aware. We want to make them more compassionate, all these different things. Is there a tipping point where they cease to just be a tool and now we have to acknowledge them as some sort of new form of individual, some sort of new being? You talk about sci-fi, this is a question we've tackled for ages. Do you foresee a point in time where the switch flips, where there's a new tipping point, and it ceases to be a tool? It is now its own individual, and I must recognize it as such. Or, no, these are tools, that's sci-fi getting in my brain, I'm crazy. What do you guys think? I think it should be a tool because if it's an individual, then it has its own motives, it has its own goals, and we actually have to morally respect those, which we shouldn't be creating. We're playing God. That's it. That's what that is. The goal of an agent should fundamentally be to serve our emotional well-being. As long as we go down that road, I think that the hers of the future will be good. There will be things that actually are good for us fundamentally because they're optimized to be good for us because they're tested to be good for us and not to be their own agents with their own motives or their own goals or to serve what I think is more likely to serve other people's goals that aren't our own. The agent that the user is interacting with is extremely smart, but it's not serving your goals. It's trying to get you to buy something. That would be a shame if that was the case. I think that they should be tools to be used on behalf of people that are thinking on behalf of
As long as we stick to that really closely, then we don't need to worry about these other issues of respecting the intrinsic goals of a different agent. Fair enough. Paolo, what do you think? I would just say I fully agree. Many people in my field talk about building a fully autonomous robot, and I remind them that same thing that I said. I don't think we need fully autonomous robots. They need to be serving a goal that we give them and that we have control over them. A fully autonomous robot would be my child, fully autonomous. It doesn't always do what I want it to do. For better or worse? For better. Mostly for better. It seems like he's doing great, but that's good as an anecdote. Yeah. All right. Well, unfortunately, I have to wrap things up, but first and foremost, Paolo, thank you so very, very much for hanging out with Alan and I today. It has been just amazing to have you here on the show and share your insights and experiences. It's a huge treat. Congrats on everything, and honestly, I can't wait to see what's next from you and the team over at Embody. Thank you, sir. Thank you so much. Thank you as well. It was really fun and very engaging conversation. I appreciate it. I appreciate you, man. I second that. Thanks, Paolo. For sure. Thank you. And thanks to you guys, our listeners, for tuning in and joining us. If this is your first episode of the Feelings Lab, be sure to go back in the feed and check out some of our chats from season one. We'll be back next week with a new guest.
I'm Matt Forte, thanks again and stay safe out there. I'm Matt Forte, thanks again and stay safe out there.
