# The Hume Initiative

The Hume Initiative is a nonprofit effort charting an ethical path for empathic AI

An initiative to steer the future of AI
Modern-day technology should, above all else, serve our emotional well-being.
But because it’s more straightforward to train AI to optimize for our engagement in an app or what we click on—behaviors that are easy to quantify—many of the algorithms we interact with every day have been trained to view our emotional cues as means to those kinds of ends.

This isn't always bad. When we engage with something, it's often a sign that we enjoy it. But our engagement can also be driven by an unhealthy inclination to outrage and moral condemnation, by emotional insecurities, feelings of envy and greed, or compulsive or addictive tendencies.

There has to be a better way.
AI should be able to tell the difference between good and bad drivers of engagement, between positive and negative emotional cues. It should use these distinctions to better optimize for human emotional well-being. And it should do so without defaulting to overly reductive methods, like surfacing funny videos and eliminating negative news, using strategies that honor the complexity of human emotion.

AI that learns from the full array of emotional cues in expression, language, and action can help us build healthier social networks and train digital assistants to respond with nuance to a user’s present state of mind. It can inform technologies that let animators bring relatable characters to life, apps that work to improve mental health, and communication platforms that enhance our empathy for others.

But empathic AI also poses risks.
Algorithms that interpret our emotional behaviors can use this information in ways that aren’t conducive to our well-being. They could surface and reinforce unhealthy temptations when we are most vulnerable to them, help create more convincing deepfakes, or exacerbate harmful stereotypes.

To steer the development of empathic AI that benefits human well being, we at The Hume Initiative asked prominent AI researchers, ethicists, social scientists, and legal professionals to weigh in on a wide range of potential use cases, ethical guidelines, and recommendations—the list of which will evolve and grow as new applications arise. We hope that these recommendations will serve as guideposts for the creation of truly beneficial empathic AI technologies.

## Serving Human Well-being

Empathic AI is integral to a wide range of technologies capable of advancing the greater good. It can help us build healthier social networks. It can train digital assistants to respond with nuance to our present state of mind—to how we say something rather than simply what we say. It can inform technologies that let animators bring relatable characters to life, apps that work to improve mental health, and communication platforms that enhance our empathy for others. Eventually, it could be used to create entirely new experiences, from new kinds of art to personalized VR worlds, optimized for specific emotions.

Despite the potential benefits to us as individuals and society, AI that recognizes cues of human emotion also poses risks. Algorithms that meaningfully interpret our emotional behaviors can use this information to manipulate us in ways that aren’t conducive to our well-being. For instance, they could be used to surface and reinforce unhealthy temptations when we are most vulnerable to them. They could be used to create more convincing deepfakes for deceptive purposes. Or they could exacerbate harmful stereotypes regarding the ways people of certain genders or ethnic groups express emotion. Thus, empathic AI should be used in keeping with clear ethical guidelines.

## Guiding Principles

Principle of Beneficence
An empathic technology should be deployed only if its benefits substantially outweigh its costs for individuals and society at large.
Auxiliary Tenets
Benefits are measured in terms of increases in well-being. Well-being is measured in terms of reported emotional experience (e.g. positive emotions, self-esteem, and satisfaction with life) and objective proxies of well-being (e.g. emotional expression, health outcomes, and goal attainment).

Costs are measured in terms of reductions in well-being.

Corollary: Developers of AI should strive to understand and improve its benefit-to-cost ratio. Every other Guiding Principle derives from this core tenet.

Principle of Emotional Primacy
Empathic technologies should be built to serve our emotional well-being and avoid treating human emotion as a means to an end.
Auxiliary Tenets
When our emotional behaviors are used as inputs to an AI that optimizes for third party objectives (e.g. purchasing behavior, engagement, habit formation, etc.), the AI can learn to exploit and manipulate our emotions.

An AI privy to its users’ emotional behaviors should treat these behaviors as ends in and of themselves. In other words, increasing or decreasing the occurrence of emotional behaviors such as laughter or anger should be an active choice of developers informed by user well-being metrics, not a lever introduced to, or discovered by, the algorithm as a means to serve a third-party objective.

Algorithms used to detect cues of emotion should only serve objectives that are aligned with well-being. This can include responding appropriately to edge cases, safeguarding users against exploitation, and promoting users’ emotional awareness and agency.

Corollary: Developers of AI privy to data regarding human emotional behaviors should strive to understand and improve its effects on human emotion.

Principle of Scientific Legitimacy
Claims about the capabilities, costs, and benefits of empathic technologies should be supported by rigorous, inclusive, multidisciplinary, and collaborative science.
Auxiliary Tenets
Because they are difficult to interrogate and can have costly implications, claims about the capabilities of an empathic AI system deserve particularly rigorous scrutiny.

If an AI makes decisions that affect human emotions, we should understand how it affects them.

Careful determinations of the capabilities, costs, and benefits of an empathic AI system can only come from precise, replicable, and appropriately generalizable studies of its behavior over time.

Corollary: Developers of empathic AI should conduct ongoing scientific research to examine its capabilities, costs, and benefits.

Principle of Inclusivity
Members of diverse demographic and cultural groups deserve access to the benefits of empathic technologies without incurring differential costs.
Auxiliary Tenets
When an empathic technology is developed using data from one cultural or demographic group, its behavior may not generalize to all cultural or demographic groups. Thus, testing is required to ensure that the empathic technology benefits each of the cultural or demographic groups whom it affects.

When an AI system is trained on everyday human behavior, it can inherit everyday human biases. Testing is required to ensure that AI systems are free from cultural and demographic biases.

The onus is on the developers and operators of any AI system that relies on measures of human behavior to ensure that it treats members of all demographic, cultural, and neurodiverse groups fairly.

Corollary: Developers of empathic AI should strive to make its benefits accessible across demographic, cultural, and neurodiverse populations and ensure that it does not impose differential costs on any particular group.

Principle of Transparency
The people affected by an empathic technology should have access to the information necessary to make informed decisions about its use.
Auxiliary Tenets
The behaviors of an AI system are difficult even for its own developers to understand. For this reason, developers should take special care to ensure people are able to make informed decisions about its use.

People who opt in to allowing an AI system to use their data should have visibility into the ways their data is processed and to what ends.

Informed consent procedures should be designed to convey both benefits and risks in a manner that is provably understandable and accessible to those affected.

Corollary: Developers of empathic AI should provide those affected by its behavior with the capability to observe and study its behavior.

Principle of Consent
An empathic technology should be deployed only with the informed consent of the people whom it affects.
Auxiliary Tenets
Individuals and communities affected by a technology should be considered the best judges of whether its functioning is aligned with their well-being.

As individuals and communities learn how an empathic technology affects their well-being in daily life, they should be considered the best judges of whether the technology remains beneficial to them, and should have the right not to use it if it is not.

Corollary: Developers should deploy empathic AI only with the ongoing informed consent of the individuals or communities whom it affects.

## Guidelines

Supported and unsupported use cases of empathic AI technology, developed by AI researchers, ethicists, social scientists, and legal professionals
To steer the development of empathic AI that benefits human well being, we at The Hume Initiative asked prominent AI ethicists, safety engineers, social scientists, and legal professionals to weigh in on a wide range of potential use cases, ethical guidelines, and recommendations—the list of which will evolve and grow as new applications arise. We hope that these recommendations will serve as guideposts for the creation of truly beneficial empathic AI technologies.

Scope
The present guidelines focus on what we call “empathic AI,” or any technology that is explicitly designed to respond to cues of emotion.

## Supported use cases

ARTS AND CULTURE
COMMUNICATION TOOLS
EDUCATION, SELF-IMPROVEMENT, AND PERFORMANCE
HEALTH AND DEVELOPMENT
PLATFORMS AND INTERFACES
RESEARCH TOOLS
ROBOTS AND VEHICLES
Arts and Culture

Supported use case
Emotionally intelligent photography and video capture tools

Supported use case
Expressive digital character animation tools

Supported use case
Empathic art, music, and design tools

Supported use case
Emotionally rich content to enhance empathy and understanding

Communication Tools

Supported use case
Technologies for empathic digital communication

Supported use case
Accessibility tools such as speech emotion transcription

Supported use case
Customer service tools optimized for customer satisfaction

Education, Self-Improvement, and Performance

Supported use case
Empathic education and professional development tools

Supported use case
Athletic and professional performance optimization

Health and Development

Supported use case
Empathic tools to improve mental health assessment and treatment

Supported use case
Childcare tools that support positive developmental outcomes

Supported use case
Pain, crisis, and threat assessment for emergency triage

Platforms and Interfaces

Supported use case
Social network algorithms to improve safety and well-being

Supported use case
Empathic digital assistants optimized for well-being

Supported use case
Search and recommendation systems that improve well-being

Supported use case
Ambient computing applications supporting user goals and well-being

Supported use case
Memory and reminder tools that support user well-being

Research Tools

Supported use case
Experimental research tools for scientific inquiry or social good

Supported use case
Observational research tools for scientific inquiry and the social good

Robots and Vehicles

Supported use case
Empathic social robots

Supported use case
Safety, comfort, and social awareness in vehicles

## Unsupported use cases

Unsupported Use Cases
MANIPULATION
DECEPTION
OPTIMIZING FOR REDUCED WELL-BEING
UNBOUNDED EMPATHIC AI
Manipulation

Unsupported use case
Manipulation

Deception

Unsupported use case
Deception

Optimizing for Reduced Well-being

Unsupported use case
Optimizing for Reduced Well-being

Unbounded Empathic AI

Unsupported use case
Unbounded Empathic AI

## Measuring Well-Being

Algorithms that respond to cues of emotion open avenues for measuring well-being. With this capability, the responsibility arises to incorporate measures of well-being into analyses of the cost and benefits of empathic technology. We offer recommendations for doing so, informed by the science of happiness.

 emotional well-being (and not the contrary).

Guiding Principles
Given that well-being is key to the ethical deployment of empathic AI, it is critical that we define human well-being and identify ways it can be measured. First, a definition: well-being is the experience of living a good life. It is the experience of living a life on balance, characterized by comfort, health, happiness, fulfillment, and a desired level of variety or richness in emotional experience. This brings to fore three key tenets of measuring and optimizing for well-being:

1

No single measure of well-being is perfect, but many are adequate.

No single measure of well-being reigns supreme in the eyes of scientists, philosophers, or poets. Yet it is not impossible to measure well-being. There are many valid metrics to choose from, just as there are many ways to be happy. Developers of empathic AI should strive to optimize for the most contextually appropriate measure. They should also ensure that improvements in one measurement or facet of well-being do not come at the expense of decreases in others. It is therefore essential to use multiple complementary measures, particularly in high-risk applications.

2

When optimizing for well-being, preserve emotional richness.

Well-being is not just about feeling "positive." We are not always better off smiling. We like feeling different emotions in different contexts—horror felt in response to a horror movie is, to many, a sought-after experience. We also enjoy experiencing a variety of positive emotions, from awe to amusement to love. But this should not discourage developers from training AI to increase positive emotions on balance. It should just be trained to do so without reducing the overall variety of emotions we experience and express.

3

Algorithms should be tested for their causal effects on well-being.

Developers should take observational measures of well-being into consideration to ensure algorithms are designed in a manner likely to improve well-being. But they should be aware that observational measures of well-being do not allow for causal inference. Before deploying an algorithm, standard experimental tests—such as A/B tests—should be used to evaluate its causal effects on well-being. Such tests should adhere to applicable standards of research ethics.

With these tenets in mind, here we provide a summary of widely accepted measures of human well-being. Given that well-being is subjective, it is most directly measured using self-report instruments. But there are also many objective proxies of well-being that can be measured reliably and unobtrusively. We summarize a range of widely accepted self-report measures and objective proxies in the tables below.

Self-Report Measures and Objective Proxies
Self-Report Measures

Category
Measures
Best Practices
Tier 1: Day-to-Day Measures

Present emotional experience

Self-reported positive emotions (e.g., admiration, adoration, aesthetic appreciation, amusement, awe, calmness, contentment, ecstasy, excitement, interest, joy, love, pride, romance, satisfaction, or triumph)

Lowness in self-reported negative emotions (e.g., anxiety, boredom, confusion, contempt, disappointment, disgust, distress, doubt, fear, guilt, horror, pain, regret, sadness, shame, or tiredness)

Given tradeoffs between immediate and long-term effects on emotions, measure emotions in the days or weeks following use of the application if possible.

Given that negative emotions are not always undesirable (e.g., horror in response to horror movies), measure emotional experience alongside other self-report measures or objective proxies of well-being.

To obtain reliable measurements, consider ecological momentary assessment (EMA) best practices.

Mental health survey instruments

Individual mental health assessments

Community and relationship mental health assessments

Most relevant to health-related applications.

Where possible, use well-validated instruments to assess mental health, as effect sizes can then be compared across many interventions.

Tier 2: Longer Term Measures

Past emotional experience

Feeling positive emotions in the past day, week, or month

Feeling fewer negative emotions in the past day, week, or month

Given that people can be inaccurate in recalling past experiences, measure alongside present emotional experience.

Life satisfaction

Feeling satisfied when reflecting upon one’s life as a whole

Feeling that life is close to ideal

Having few regrets

Satisfaction with Life Scale

Consider pairing intermittent measures of life satisfaction with more frequent well-being measures that are more variable and responsive to changes, such as present emotional experience.

Altruism

Feeling motivated to help others

Recently helped others

High in assessments of empathy

Low in assessments of narcissism, machiavellianism, and psychopathy

The relevance of altruism depends on the specific benefits and risks of the application. It is particularly relevant for platforms that influence social decision-making.

Tier 3: Less Direct Measures

User Satisfaction

Level of satisfaction with a product or service

Likelihood of recommending a product or service to others

Caveat: User satisfaction is less directly linked to the ethics of the application than other well-being measures (e.g., consider user satisfaction with tobacco products).

Repeated informed consent

Informed consent solicited after a comprehensive summary of potential risks is reiterated to the user, such that consent can be viewed as a judgment that the benefits exceed the potential risks

Informed consent may be solicited regularly to ensure that users understand potential risks.

Ongoing consent is essential in early stages of testing, when data is insufficient to weigh benefits and risks.

Objective Proxies

Category
Measures
Best Practices
Tier 1: Day-to-Day Measures

Expressive behavior

Positive facial expressions, vocal utterances (e.g., laughter), speech prosody, language, or emotion-related body movement appropriate to a given social context

Lowness in negative facial expressions, vocal utterances, speech prosody, language, or emotion-related body movement that are not expected in a given social context (for exceptions, consider expressions of horror in response to a horror movie or sadness at a funeral)

Assessing the appropriateness of expressive behavior to a given social context depends on nuanced measurement. For instance, a triumphant shout may be appropriate on a football field but not in a comedy club (valence alone is often insufficient).

Expressive behavior can be measured more frequently and passively than self-report and is less subject to self-enhancement bias and demand characteristics.

To weigh behaviors appropriately, measure alongside more intermittent self-report measures.

Harmful behaviors

Self-harm

Substance abuse

Antisocial behavior (e.g., physical harm, hate speech, environmental pollution)

Where feasible, reductions in harmful behavior can be used as an objective proxy to accompany self-report measures of well-being.

Tier 2: Longer Term Measures

Physical health indicators

Low rate of illness, injury, and/or mortality

Sleep quality

Healthy levels of inflammatory cytokines, cholesterol, blood pressure, and other objective indicators of physical health

Where feasible, physical health indicators, particularly those closely linked to emotional well-being (e.g., sleep quality, inflammatory cytokines), can be used as objective proxies to accompany self-report measures.

Goal attainment

Academic performance metrics

Career advancement metrics

Metrics of goal attainment are most relevant when they represent users’ direct goals in deploying empathic AI.

Negotiate life outcomes

Incarceration rates

Job loss

Poverty rates

Where feasible, reductions in negative life outcomes can be used as a strong objective proxy to accompany self-report measures of well-being.

Accurate beliefs

Agreement with facts that are backed by near-universal scientific or expert consensus

False beliefs are of most concern for applications that control the spread of information, such as search engines and social media, and can be costly to individual and societal well-being.

Tier 3: Less Direct Measures

Emotion related physiology

Physiological changes associated with emotion in a given context, such as pupil dilation, sweating, and heart rate variability

Physiological indicators of emotion such as autonomic activity are highly context dependent and must be interpreted with care.

Standards of Measurement
The following are recommended standards for ongoing measurement of well-being within an application or condition of an A/B test, by risk level and number of users. We recommend discontinuing any application of empathic AI whose benefits do not substantially outweigh its costs for the well-being of users and other affected parties.

Low Risk

Low-risk area AND used for <30 min. per day on average.

Medium Risk

Medium risk area OR used for 30 min. to 1 hour per day on average.

High Risk

High-risk area OR used for >1 hour per day on average.

None (informed consent is sufficient).
<100 DAILY USERS

Track at least one measure of well-being across a minimum of 20 randomly sampled users on at least a weekly basis.
100-1K DAILY USERS

Track at least one Tier 1 measure of well-being across a minimum of 50 randomly sampled users on at least a weekly basis.
1K-10K DAILY USERS

Track at least two measures of well-being, including a Tier 1 measure, across a minimum of 200 randomly sampled users on at least a weekly basis.
10K-100K DAILY USERS

Track at least two measures of well-being, including a Tier 1 measure, across a minimum of 400 randomly sampled users on a daily basis.
100K-1M DAILY USERS

Track at least two Tier 1 measures of well-being across a minimum of 2,000 randomly sampled users on a daily basis. Provide public access to anonymized results.
1M+ DAILY USERS

Track at least one measure of well-being in every user on at least a weekly basis.
Track at least one Tier 1 measure of well-being across a minimum of 40 randomly sampled users on at least a weekly basis.
Track at least two measures of well-being, including a Tier 1 measure, across a minimum of 100 randomly sampled users on a daily basis.
Track at least three measures of average well-being, including a Tier 1 measure, across a minimum of 300 randomly sampled users on a daily basis.
Track at least three measures of average well-being, including two Tier 1 measures, across a minimum of 1,000 randomly sampled users on a daily basis. Provide public access to anonymized results.
Track at least three measures of average well-being, including two Tier 1 measures, across a minimum of 5,000 randomly sampled users on a daily basis. Provide public access to anonymized results.
Track at least one Tier 1 measure of well-being in every user on at least a weekly basis.
Track at least two measures of well-being, including a Tier 1 measure, in every user on at least a weekly basis.
Track at least three measures of well-being, including a Tier 1 measure, across a minimum of 500 randomly sampled users on a daily basis.
Track at least four measures of well-being, including two Tier 1 measures, across a minimum of 1,000 randomly sampled users on a daily basis. Provide public access to anonymized results.
Track at least four measures of well-being, including two Tier 1 measures and another Tier 1 or 2 measure, across a minimum of 4,000 randomly sampled users on a daily basis. Provide public access to anonymized results.
Track at least four measures of well-being, including two Tier 1 measures and another Tier 1 or 2 measure, across a minimum of 10,000 randomly sampled users on a daily basis. Provide public access to anonymized results.
