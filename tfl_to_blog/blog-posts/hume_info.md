Hume AI is a research lab and technology company. Our mission is to ensure that artificial intelligence is built to serve human goals and emotional well-being.
Vision
Our vision is a world where AI translates scientific insights into new ways to improve human emotional experience. Emotional awareness is the missing ingredient needed to build social media algorithms that optimize for user well-being instead of engagement metrics, digital assistants that respond with more nuance to your present state of mind, technologies that help animators create relatable characters, and much more.
Values
We’re committed to paving the way for prosocial AI while staying true to our values and principles:
Empathy
AI privy to cues of our emotions should serve our emotional well-being.
Beneficence
AI should be deployed only if its benefits substantially outweigh its costs.
Scientific Legitimacy
Applications of AI should be supported by collaborative, rigorous, and inclusive science.
Emotional Primacy
AI should be prevented from treating human emotion as a means to an end.
Inclusivity
The benefits of AI should be shared by people from diverse backgrounds.
Transparency
People affected by AI should have enough data to make decisions about its use.
Consent
AI should be deployed only with the informed consent of the people whom it affects.

The new science of expression
Scientific discoveries pave the way for technology that responds appropriately to our expressions.

Capture human expression in text, audio, video, or images
Human values lie beyond words: in tones of sarcasm, subtle facial movements, cringes of empathic pain, laughter tinged with awkwardness, sighs of relief, and more. We can help you read between the lines.

Exploring the full spectrum of expression
We’ve introduced new datasets and statistical methods to explore the dimensions of meaning that explain the feelings we report in different situations, the patterns of brain activity they evoke, physiological responses like goosebumps, and nuanced expressions in the face, body, and voice.

THE FACE AND BODY
Over 28 nuanced facial expressions
With millions of reactions captured, studied, and mapped, our work reveals that facial expressions are more than three times more diverse and complex than previously assumed. These findings pave the way for vast improvements in technologies that interpret facial expression.

THE VOICE
The power of laughs, cries, and sighs
The voice is revealing itself to be an even more important medium for nonverbal expression than the face. Our computational work reveals dozens of kinds of vocal emotional expression, from sighs and gasps to grunts and growls, opening the door to technologies that better understand and communicate with us to improve our well-being.

THE NEW SCIENCE OF EXPRESSION

A unified platform for expressive communication
Based on scientific research, we offer the world’s most accurate and comprehensive tools for understanding nonverbal behavior

Many models, one API

Language, speech prosody, facial expression, and more

Insights from any input

Analyze text, audio, video, or images with one line of code

Pay as you go

With usage-based pricing, start building immediately

State-of-the-art

Updated in tandem with the most advanced published research

Interactive playground

Discover the power of expressive behavior in seconds

Open ethical guidelines

Read the ethical guidelines that define our supported uses

OUR MODELS
Advancing empathic technology with the most accurate and nuanced models to date
Capture nuances in expressive behavior with newfound precision: subtle facial movements that express love or admiration, cringes of empathic pain, laughter tinged with awkwardness, sighs of relief.

Expressive Language
MODEL
VOICE

Measure 53 emotions reliably expressed by the subtleties of language

EXPLORE

Vocal Call Types
MODEL
VOICE

Explore vocal utterances by inferring probabilities of 67 descriptors, like 'laugh', 'sigh', 'shriek', 'oh', 'ahh', 'mhm', and more

EXPLORE

Valence & Arousal
MODEL
MULTIMODAL

Predict perceived valence and arousal in facial expression, vocal bursts, speech, or language

EXPLORE

FACS 2.0
MODEL
FACE + BODY

An improved, automated facial action coding system (FACS): measure 26 facial action units (AUs) and 29 other features with even less bias than traditional FACS

EXPLORE

Facial Expression
MODEL
FACE + BODY

Differentiate 37 kinds of facial movement that are recognized as conveying distinct meanings, and the many ways they are blended together

EXPLORE

Vocal Expression
MODEL
VOICE

Differentiate 28 kinds of vocal expression recognized as conveying distinct meanings, and the many ways they are blended together

EXPLORE

Speech Prosody
MODEL
VOICE

Discover over 25 patterns of tune, rhythm, and timbre that imbue everyday speech with complex, blended meanings

EXPLORE

Dynamic Reaction
MODEL
FACE + BODY

Measure dynamic patterns in facial expression over time that are correlated with over 20 distinct reported emotions

EXPLORE

Sentiment
MODEL
VOICE

Measure the distribution of possible sentiments expressed in a sentence, negative to positive, neutral or ambiguous

EXPLORE

Millions of human experiences and expressions from diverse people around the world
With hundreds of thousands of fully-consented samples, our datasets are emotionally rich, naturalistic, culturally diverse, and equitable. They are the tools needed to train and evaluate unbiased empathic technologies.

Emotional Speech
DATASET
VOICE

Recordings of sentences being spoken in dozens of emotional intonations around the world, with self-report and demographics

EXPLORE

Vocal Utterances
DATASET
VOICE

Diverse vocal expressions (e.g., laughs, sighs, screams) worldwide, including 28+ call types with distinct self-reported meanings

EXPLORE

Facial Expressions
DATASET
FACE + BODY

Hundreds of thousands of diverse facial expressions worldwide, capturing 37+ expressions with distinct self-reported meanings

EXPLORE

Multimodal Reactions
DATASET
MULTIMODAL

Reactions to thousands of evocative experiences across 4 continents, capturing 27+ distinct patterns of emotional response

EXPLORE

Dyadic Conversations
DATASET
MULTIMODAL

Hundreds of thousands of short emotional conversations between friends or strangers, with detailed self-report ratings of expression

REQUEST
